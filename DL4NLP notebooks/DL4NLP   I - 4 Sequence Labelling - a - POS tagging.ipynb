{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=orange>I - 4 </font>\n",
    "    Sequence Labelling\n",
    "    \n",
    "  </div> \n",
    "\n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 20px; \n",
    "      margin: 10px;\">\n",
    "  a. POS tagging\n",
    "  </div>\n",
    "\n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=orange>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. Sequence to sequence\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "1. Abstractive Summarization\n",
    "\n",
    "2. Question Answering\n",
    "\n",
    "3. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "A top-quality Github repository discussing Sequence Labelling is found [here](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling)<br><br><br>\n",
    "\n",
    "\n",
    "| **Corpus** | **Popular library using the corpus** | **Data availability** | \n",
    "|------|------|------|\n",
    "| [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42) | Stanford NLP | Extract in NLTK as detailed [here](https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537) | \n",
    "| OntoNotes 5 |  |  | \n",
    "| Groningen Meaning Bank | | Full data [here](https://gmb.let.rug.nl/data.php) - Extract found [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_DL4NLP = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(path_to_DL4NLP + '\\\\lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Groningen Meaning Bank extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "df_GMB_extract = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_GMB_extract_add = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner_dataset.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050794, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx           word  pos\n",
       "0           1.0      Thousands  NNS\n",
       "1           1.0             of   IN\n",
       "2           1.0  demonstrators  NNS\n",
       "3           1.0           have  VBP\n",
       "4           1.0        marched  VBN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract.dropna(inplace = True)\n",
    "df_GMB_extract = df_GMB_extract[['sentence_idx', 'word', 'pos']]\n",
    "print(df_GMB_extract.shape)\n",
    "df_GMB_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS\n",
       "0  Sentence: 1      Thousands  NNS\n",
       "1  Sentence: 1             of   IN\n",
       "2  Sentence: 1  demonstrators  NNS\n",
       "3  Sentence: 1           have  VBP\n",
       "4  Sentence: 1        marched  VBN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract_add.fillna(method = 'ffill', inplace = True)\n",
    "df_GMB_extract_add = df_GMB_extract_add[['Sentence #', 'Word', 'POS']]\n",
    "df_GMB_extract_add.replace('FW', 'NN', inplace = True)\n",
    "print(df_GMB_extract_add.shape)\n",
    "df_GMB_extract_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "POSs = df_GMB_extract['pos'].unique().tolist()\n",
    "print(len(POSs))\n",
    "POSs2num = {tag : i for i, tag in enumerate(POSs)}\n",
    "num2POSs = {i : tag for i, tag in enumerate(POSs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = df_GMB_extract.groupby(\"sentence_idx\").apply(lambda s: [[w.lower() for w in s[\"word\"].values.tolist()], \n",
    "                                                                 [POSs2num[t] for t in s[\"pos\"].values.tolist()]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_add = df_GMB_extract_add.groupby(\"Sentence #\").apply(lambda s: [[w.lower() for w in s[\"Word\"].values.tolist()], \n",
    "                                                                       [POSs2num[t] for t in s[\"POS\"].values.tolist()]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_trn, corpus_tst = train_test_split(corpus, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_trn = [s_l[0] for s_l in corpus_trn if len(s_l[0]) <= 75]\n",
    "poslabels_trn = [s_l[1] for s_l in corpus_trn if len(s_l[1]) <= 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28141"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(top-seeded, 10, JJ)\n",
      "(lleyton, 4, NNP)\n",
      "(hewitt, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(australia, 4, NNP)\n",
      "(and, 9, CC)\n",
      "(number-two, 10, JJ)\n",
      "(dominik, 4, NNP)\n",
      "(hrbaty, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(slovakia, 4, NNP)\n",
      "(have, 2, VBP)\n",
      "(advanced, 3, VBN)\n",
      "(to, 5, TO)\n",
      "(the, 7, DT)\n",
      "(second, 10, JJ)\n",
      "(round, 8, NN)\n",
      "(of, 1, IN)\n",
      "(the, 7, DT)\n",
      "(international, 10, JJ)\n",
      "(men, 0, NNS)\n",
      "('s, 18, POS)\n",
      "(hardcourt, 8, NN)\n",
      "(tennis, 8, NN)\n",
      "(championships, 0, NNS)\n",
      "(in, 1, IN)\n",
      "(adelaide, 4, NNP)\n",
      "(,, 21, ,)\n",
      "(australia, 4, NNP)\n",
      "(., 11, .)\n",
      "(top-seeded, 10, JJ)\n",
      "(lleyton, 4, NNP)\n",
      "(hewitt, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(australia, 4, NNP)\n",
      "(and, 9, CC)\n",
      "(number-two, 10, JJ)\n",
      "(dominik, 4, NNP)\n",
      "(hrbaty, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(slovakia, 4, NNP)\n",
      "(have, 2, VBP)\n",
      "(advanced, 3, VBN)\n",
      "(to, 5, TO)\n",
      "(the, 7, DT)\n",
      "(second, 10, JJ)\n",
      "(round, 8, NN)\n",
      "(of, 1, IN)\n",
      "(the, 7, DT)\n",
      "(international, 10, JJ)\n",
      "(men, 0, NNS)\n",
      "('s, 18, POS)\n",
      "(hardcourt, 8, NN)\n",
      "(tennis, 8, NN)\n",
      "(championships, 0, NNS)\n",
      "(in, 1, IN)\n",
      "(adelaide, 4, NNP)\n",
      "(,, 21, ,)\n",
      "(australia, 4, NNP)\n",
      "(., 11, .)\n"
     ]
    }
   ],
   "source": [
    "words = corpus_trn[0][0]\n",
    "tags  = corpus_trn[0][1]\n",
    "for word, tag in zip(words, tags) : print('(' + word + ', ' + str(tag)+ ', ' + str(num2POSs[tag]) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective.\n",
    "\n",
    "#### $\\bullet$ FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec = FastText(size = 100, \n",
    "                    window = 5, \n",
    "                    min_count = 1, \n",
    "                    negative = 20,\n",
    "                    sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24652"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.build_vocab(sentences_trn)\n",
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec.train(sentences_trn,\n",
    "               epochs = 20,\n",
    "               total_examples = word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Part Of Speech Tagging Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SequenceTagger(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 n_class = 2,\n",
    "                 dropout = 0,\n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SequenceTagger, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, n_class)\n",
    "        self.act       = F.softmax\n",
    "        self.n_class   = n_class\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index_in  = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.ignore_index_out = n_class\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index_out, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, n_class)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length,  n_class)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        return [(w, i) for w, i in zip(words, inds)]\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def sentence2indices(words) :\n",
    "            # convert to indices\n",
    "            inds = [self.word2vec.lang.getIndex(w) for w in words]\n",
    "            inds = [i for i in inds if i is not None]\n",
    "            # apply mask\n",
    "            mask = [m for m, i in enumerate(inds) if i != self.word2vec.lang.getIndex('UNK')]\n",
    "            mask = random.sample(mask, k = int(mask_ratio*(len(mask) +1)))\n",
    "            inds = [maskInput(i, m in mask) for m, i in enumerate(inds)]\n",
    "            return inds\n",
    "        \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s[0]), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = [[sentence2indices(s[0]), s[1]] for s in sentences[i:i + batch_size]]\n",
    "            pack.sort(key = lambda p : len(p[0]), reverse = True)\n",
    "            pack0 = [p[0] for p in pack] \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index_in))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1)) # size (batch_size, max_length)\n",
    "            lengths = torch.tensor([len(p[0]) for p in pack])         # size (batch_size)\n",
    "            pack1 = [p[1] for p in pack]                              # size (batch_size, max_length)\n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index_out))\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1)) # size (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "\n",
    "    # compute model perf\n",
    "    def compute_accuracy(self, sentences, batch_size = 32) :\n",
    "        def computeLogProbs(batch) :\n",
    "            torch.cuda.empty_cache()\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeSuccess(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return success, total\n",
    "        \n",
    "        # --- main ----\n",
    "        self.eval()\n",
    "        batches = self.generatePackedSentences(sentences, batch_size)\n",
    "        score, total = 0, 0\n",
    "        for batch, targets in batches :\n",
    "            log_probs = computeLogProbs(batch).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets = targets.to(self.device)                  # dim = (batch_size, input_length)\n",
    "            s, t = computeSuccess(log_probs, targets)\n",
    "            score += s\n",
    "            total += t\n",
    "        return score * 100 / total\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, \n",
    "            iters = None, epochs = None, lr = 0.025, random_state = 42, \n",
    "            print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs  = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets    = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss       = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if compute_accuracy :\n",
    "                accuracy = computeAccuracy(log_probs, targets)\n",
    "            else : \n",
    "                accuracy = 0\n",
    "            error = float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index_out))\n",
    "            return error, accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ POSTagger Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458166"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger = SequenceTagger(device = torch.device('cpu'), # device\n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = Word2VecConnector(word2vec),\n",
    "                            hidden_dim = 125, \n",
    "                            n_layers = 2, \n",
    "                            n_class = 41,\n",
    "                            dropout = 0.15,\n",
    "                            optimizer = optim.AdamW)\n",
    "\n",
    "pos_tagger.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5277"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 42)\n",
    "batches+= pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 4242)\n",
    "batches+= pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 1331)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 14s (- 25m 43s) (50 0%) loss : 2.825  accuracy : 21.6 %\n",
      "0m 26s (- 22m 52s) (100 1%) loss : 1.676  accuracy : 54.0 %\n",
      "0m 37s (- 21m 12s) (150 2%) loss : 1.037  accuracy : 72.3 %\n",
      "0m 47s (- 20m 4s) (200 3%) loss : 0.886  accuracy : 75.2 %\n",
      "0m 58s (- 19m 32s) (250 4%) loss : 0.806  accuracy : 77.6 %\n",
      "1m 8s (- 18m 52s) (300 5%) loss : 0.751  accuracy : 78.6 %\n",
      "1m 19s (- 18m 40s) (350 6%) loss : 0.703  accuracy : 79.9 %\n",
      "1m 29s (- 18m 11s) (400 7%) loss : 0.694  accuracy : 79.6 %\n",
      "1m 38s (- 17m 39s) (450 8%) loss : 0.666  accuracy : 80.7 %\n",
      "1m 46s (- 16m 56s) (500 9%) loss : 0.615  accuracy : 81.7 %\n",
      "1m 56s (- 16m 40s) (550 10%) loss : 0.637  accuracy : 80.9 %\n",
      "2m 7s (- 16m 31s) (600 11%) loss : 0.615  accuracy : 81.8 %\n",
      "2m 17s (- 16m 17s) (650 12%) loss : 0.618  accuracy : 81.5 %\n",
      "2m 26s (- 15m 57s) (700 13%) loss : 0.597  accuracy : 81.9 %\n",
      "2m 37s (- 15m 49s) (750 14%) loss : 0.604  accuracy : 81.7 %\n",
      "2m 48s (- 15m 44s) (800 15%) loss : 0.568  accuracy : 82.4 %\n",
      "2m 57s (- 15m 26s) (850 16%) loss : 0.564  accuracy : 83.2 %\n",
      "3m 8s (- 15m 15s) (900 17%) loss : 0.563  accuracy : 82.7 %\n",
      "3m 16s (- 14m 55s) (950 18%) loss : 0.546  accuracy : 83.3 %\n",
      "3m 26s (- 14m 43s) (1000 18%) loss : 0.556  accuracy : 83.4 %\n",
      "3m 38s (- 14m 38s) (1050 19%) loss : 0.563  accuracy : 83.0 %\n",
      "3m 50s (- 14m 37s) (1100 20%) loss : 0.541  accuracy : 83.4 %\n",
      "4m 3s (- 14m 34s) (1150 21%) loss : 0.531  accuracy : 83.8 %\n",
      "4m 13s (- 14m 22s) (1200 22%) loss : 0.513  accuracy : 84.6 %\n",
      "4m 22s (- 14m 6s) (1250 23%) loss : 0.512  accuracy : 84.6 %\n",
      "4m 33s (- 13m 57s) (1300 24%) loss : 0.504  accuracy : 84.7 %\n",
      "4m 44s (- 13m 47s) (1350 25%) loss : 0.507  accuracy : 84.7 %\n",
      "4m 55s (- 13m 37s) (1400 26%) loss : 0.520  accuracy : 84.2 %\n",
      "5m 3s (- 13m 22s) (1450 27%) loss : 0.500  accuracy : 84.8 %\n",
      "5m 13s (- 13m 10s) (1500 28%) loss : 0.508  accuracy : 84.5 %\n",
      "5m 25s (- 13m 3s) (1550 29%) loss : 0.477  accuracy : 85.3 %\n",
      "5m 39s (- 13m 0s) (1600 30%) loss : 0.494  accuracy : 85.1 %\n",
      "5m 51s (- 12m 52s) (1650 31%) loss : 0.501  accuracy : 84.8 %\n",
      "6m 2s (- 12m 42s) (1700 32%) loss : 0.465  accuracy : 85.6 %\n",
      "6m 16s (- 12m 38s) (1750 33%) loss : 0.485  accuracy : 85.4 %\n",
      "6m 27s (- 12m 28s) (1800 34%) loss : 0.472  accuracy : 85.8 %\n",
      "6m 40s (- 12m 22s) (1850 35%) loss : 0.507  accuracy : 84.4 %\n",
      "6m 54s (- 12m 17s) (1900 36%) loss : 0.458  accuracy : 86.1 %\n",
      "7m 5s (- 12m 6s) (1950 36%) loss : 0.470  accuracy : 85.6 %\n",
      "7m 16s (- 11m 55s) (2000 37%) loss : 0.473  accuracy : 85.5 %\n",
      "7m 28s (- 11m 46s) (2050 38%) loss : 0.469  accuracy : 85.4 %\n",
      "7m 40s (- 11m 36s) (2100 39%) loss : 0.473  accuracy : 85.5 %\n",
      "7m 54s (- 11m 29s) (2150 40%) loss : 0.479  accuracy : 85.5 %\n",
      "8m 7s (- 11m 22s) (2200 41%) loss : 0.474  accuracy : 85.6 %\n",
      "8m 21s (- 11m 14s) (2250 42%) loss : 0.454  accuracy : 86.1 %\n",
      "8m 32s (- 11m 3s) (2300 43%) loss : 0.456  accuracy : 86.1 %\n",
      "8m 44s (- 10m 53s) (2350 44%) loss : 0.455  accuracy : 85.9 %\n",
      "8m 55s (- 10m 42s) (2400 45%) loss : 0.450  accuracy : 86.7 %\n",
      "9m 9s (- 10m 34s) (2450 46%) loss : 0.459  accuracy : 86.0 %\n",
      "9m 24s (- 10m 27s) (2500 47%) loss : 0.465  accuracy : 85.9 %\n",
      "9m 34s (- 10m 14s) (2550 48%) loss : 0.435  accuracy : 86.7 %\n",
      "9m 46s (- 10m 3s) (2600 49%) loss : 0.437  accuracy : 86.5 %\n",
      "9m 58s (- 9m 53s) (2650 50%) loss : 0.465  accuracy : 85.6 %\n",
      "10m 8s (- 9m 41s) (2700 51%) loss : 0.438  accuracy : 86.7 %\n",
      "10m 20s (- 9m 30s) (2750 52%) loss : 0.452  accuracy : 86.3 %\n",
      "10m 33s (- 9m 20s) (2800 53%) loss : 0.444  accuracy : 86.6 %\n",
      "10m 44s (- 9m 8s) (2850 54%) loss : 0.427  accuracy : 86.8 %\n",
      "10m 55s (- 8m 57s) (2900 54%) loss : 0.432  accuracy : 86.9 %\n",
      "11m 5s (- 8m 45s) (2950 55%) loss : 0.426  accuracy : 87.1 %\n",
      "11m 16s (- 8m 33s) (3000 56%) loss : 0.419  accuracy : 87.3 %\n",
      "11m 27s (- 8m 22s) (3050 57%) loss : 0.414  accuracy : 87.4 %\n",
      "11m 37s (- 8m 9s) (3100 58%) loss : 0.414  accuracy : 87.2 %\n",
      "11m 49s (- 7m 59s) (3150 59%) loss : 0.417  accuracy : 87.2 %\n",
      "11m 59s (- 7m 47s) (3200 60%) loss : 0.421  accuracy : 87.2 %\n",
      "12m 11s (- 7m 36s) (3250 61%) loss : 0.426  accuracy : 86.9 %\n",
      "12m 23s (- 7m 25s) (3300 62%) loss : 0.415  accuracy : 87.3 %\n",
      "12m 31s (- 7m 12s) (3350 63%) loss : 0.411  accuracy : 87.3 %\n",
      "12m 40s (- 6m 59s) (3400 64%) loss : 0.401  accuracy : 87.6 %\n",
      "12m 51s (- 6m 48s) (3450 65%) loss : 0.415  accuracy : 87.1 %\n",
      "13m 4s (- 6m 38s) (3500 66%) loss : 0.422  accuracy : 86.9 %\n",
      "13m 15s (- 6m 27s) (3550 67%) loss : 0.418  accuracy : 87.2 %\n",
      "13m 26s (- 6m 15s) (3600 68%) loss : 0.414  accuracy : 87.3 %\n",
      "13m 40s (- 6m 5s) (3650 69%) loss : 0.428  accuracy : 87.0 %\n",
      "13m 53s (- 5m 55s) (3700 70%) loss : 0.408  accuracy : 87.4 %\n",
      "14m 6s (- 5m 44s) (3750 71%) loss : 0.413  accuracy : 87.6 %\n",
      "14m 18s (- 5m 33s) (3800 72%) loss : 0.402  accuracy : 87.6 %\n",
      "14m 29s (- 5m 22s) (3850 72%) loss : 0.407  accuracy : 87.4 %\n",
      "14m 39s (- 5m 10s) (3900 73%) loss : 0.411  accuracy : 87.4 %\n",
      "14m 50s (- 4m 59s) (3950 74%) loss : 0.399  accuracy : 87.6 %\n",
      "15m 11s (- 4m 51s) (4000 75%) loss : 0.427  accuracy : 86.8 %\n",
      "15m 21s (- 4m 39s) (4050 76%) loss : 0.403  accuracy : 87.7 %\n",
      "15m 44s (- 4m 31s) (4100 77%) loss : 0.406  accuracy : 87.4 %\n",
      "15m 56s (- 4m 19s) (4150 78%) loss : 0.392  accuracy : 87.9 %\n",
      "16m 9s (- 4m 8s) (4200 79%) loss : 0.416  accuracy : 87.2 %\n",
      "16m 20s (- 3m 56s) (4250 80%) loss : 0.399  accuracy : 87.6 %\n",
      "16m 30s (- 3m 45s) (4300 81%) loss : 0.401  accuracy : 87.7 %\n",
      "16m 42s (- 3m 33s) (4350 82%) loss : 0.394  accuracy : 88.1 %\n",
      "16m 53s (- 3m 22s) (4400 83%) loss : 0.393  accuracy : 87.7 %\n",
      "17m 7s (- 3m 10s) (4450 84%) loss : 0.403  accuracy : 87.6 %\n",
      "17m 21s (- 2m 59s) (4500 85%) loss : 0.408  accuracy : 87.6 %\n",
      "17m 33s (- 2m 48s) (4550 86%) loss : 0.401  accuracy : 87.6 %\n",
      "17m 46s (- 2m 36s) (4600 87%) loss : 0.398  accuracy : 88.1 %\n",
      "17m 56s (- 2m 25s) (4650 88%) loss : 0.385  accuracy : 88.3 %\n",
      "18m 8s (- 2m 13s) (4700 89%) loss : 0.410  accuracy : 87.3 %\n",
      "18m 19s (- 2m 1s) (4750 90%) loss : 0.400  accuracy : 87.7 %\n",
      "18m 28s (- 1m 50s) (4800 90%) loss : 0.386  accuracy : 87.9 %\n",
      "18m 38s (- 1m 38s) (4850 91%) loss : 0.388  accuracy : 87.9 %\n",
      "18m 45s (- 1m 26s) (4900 92%) loss : 0.364  accuracy : 88.9 %\n",
      "18m 57s (- 1m 15s) (4950 93%) loss : 0.388  accuracy : 87.9 %\n",
      "19m 7s (- 1m 3s) (5000 94%) loss : 0.387  accuracy : 87.9 %\n",
      "19m 16s (- 0m 51s) (5050 95%) loss : 0.377  accuracy : 88.5 %\n",
      "19m 29s (- 0m 40s) (5100 96%) loss : 0.392  accuracy : 87.8 %\n",
      "19m 38s (- 0m 29s) (5150 97%) loss : 0.386  accuracy : 88.0 %\n",
      "19m 48s (- 0m 17s) (5200 98%) loss : 0.391  accuracy : 88.2 %\n",
      "20m 0s (- 0m 6s) (5250 99%) loss : 0.397  accuracy : 87.7 %\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.fit(batches, epochs = 1, lr = 0.001, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 10s (- 18m 11s) (50 0%) loss : 0.355  accuracy : 89.2 %\n",
      "0m 24s (- 20m 58s) (100 1%) loss : 0.375  accuracy : 88.4 %\n",
      "0m 36s (- 20m 32s) (150 2%) loss : 0.368  accuracy : 88.8 %\n",
      "0m 48s (- 20m 27s) (200 3%) loss : 0.369  accuracy : 88.8 %\n",
      "0m 58s (- 19m 38s) (250 4%) loss : 0.372  accuracy : 88.5 %\n",
      "1m 8s (- 18m 50s) (300 5%) loss : 0.353  accuracy : 89.0 %\n",
      "1m 17s (- 18m 16s) (350 6%) loss : 0.353  accuracy : 89.0 %\n",
      "1m 28s (- 18m 0s) (400 7%) loss : 0.363  accuracy : 88.5 %\n",
      "1m 39s (- 17m 48s) (450 8%) loss : 0.359  accuracy : 88.6 %\n",
      "1m 48s (- 17m 15s) (500 9%) loss : 0.362  accuracy : 88.9 %\n",
      "2m 1s (- 17m 26s) (550 10%) loss : 0.377  accuracy : 88.5 %\n",
      "2m 10s (- 16m 59s) (600 11%) loss : 0.349  accuracy : 89.0 %\n",
      "2m 22s (- 16m 52s) (650 12%) loss : 0.376  accuracy : 88.4 %\n",
      "2m 32s (- 16m 40s) (700 13%) loss : 0.358  accuracy : 88.9 %\n",
      "2m 42s (- 16m 21s) (750 14%) loss : 0.355  accuracy : 89.0 %\n",
      "2m 53s (- 16m 13s) (800 15%) loss : 0.358  accuracy : 88.9 %\n",
      "3m 5s (- 16m 3s) (850 16%) loss : 0.350  accuracy : 89.2 %\n",
      "3m 15s (- 15m 50s) (900 17%) loss : 0.345  accuracy : 89.6 %\n",
      "3m 26s (- 15m 40s) (950 18%) loss : 0.367  accuracy : 88.7 %\n",
      "3m 36s (- 15m 25s) (1000 18%) loss : 0.357  accuracy : 89.0 %\n",
      "3m 47s (- 15m 17s) (1050 19%) loss : 0.356  accuracy : 89.0 %\n",
      "3m 58s (- 15m 6s) (1100 20%) loss : 0.370  accuracy : 88.6 %\n",
      "4m 8s (- 14m 53s) (1150 21%) loss : 0.350  accuracy : 89.2 %\n",
      "4m 18s (- 14m 39s) (1200 22%) loss : 0.353  accuracy : 89.0 %\n",
      "4m 29s (- 14m 27s) (1250 23%) loss : 0.353  accuracy : 89.2 %\n",
      "4m 40s (- 14m 17s) (1300 24%) loss : 0.359  accuracy : 89.2 %\n",
      "4m 51s (- 14m 7s) (1350 25%) loss : 0.348  accuracy : 89.2 %\n",
      "5m 1s (- 13m 54s) (1400 26%) loss : 0.359  accuracy : 89.2 %\n",
      "5m 11s (- 13m 43s) (1450 27%) loss : 0.351  accuracy : 89.2 %\n",
      "5m 21s (- 13m 29s) (1500 28%) loss : 0.356  accuracy : 89.1 %\n",
      "5m 31s (- 13m 17s) (1550 29%) loss : 0.357  accuracy : 88.9 %\n",
      "5m 43s (- 13m 9s) (1600 30%) loss : 0.370  accuracy : 88.5 %\n",
      "5m 53s (- 12m 58s) (1650 31%) loss : 0.362  accuracy : 88.8 %\n",
      "6m 4s (- 12m 47s) (1700 32%) loss : 0.368  accuracy : 88.8 %\n",
      "6m 15s (- 12m 37s) (1750 33%) loss : 0.357  accuracy : 88.7 %\n",
      "6m 24s (- 12m 23s) (1800 34%) loss : 0.345  accuracy : 89.1 %\n",
      "6m 34s (- 12m 11s) (1850 35%) loss : 0.357  accuracy : 89.0 %\n",
      "6m 45s (- 12m 0s) (1900 36%) loss : 0.372  accuracy : 88.4 %\n",
      "6m 54s (- 11m 48s) (1950 36%) loss : 0.357  accuracy : 89.0 %\n",
      "7m 5s (- 11m 37s) (2000 37%) loss : 0.359  accuracy : 88.9 %\n",
      "7m 16s (- 11m 27s) (2050 38%) loss : 0.363  accuracy : 88.8 %\n",
      "7m 25s (- 11m 13s) (2100 39%) loss : 0.358  accuracy : 88.7 %\n",
      "7m 35s (- 11m 1s) (2150 40%) loss : 0.351  accuracy : 88.9 %\n",
      "7m 45s (- 10m 51s) (2200 41%) loss : 0.361  accuracy : 88.8 %\n",
      "7m 55s (- 10m 39s) (2250 42%) loss : 0.339  accuracy : 89.5 %\n",
      "8m 4s (- 10m 27s) (2300 43%) loss : 0.359  accuracy : 89.0 %\n",
      "8m 16s (- 10m 18s) (2350 44%) loss : 0.355  accuracy : 89.1 %\n",
      "8m 25s (- 10m 6s) (2400 45%) loss : 0.353  accuracy : 89.0 %\n",
      "8m 34s (- 9m 53s) (2450 46%) loss : 0.350  accuracy : 89.0 %\n",
      "8m 43s (- 9m 41s) (2500 47%) loss : 0.342  accuracy : 89.6 %\n",
      "8m 51s (- 9m 28s) (2550 48%) loss : 0.377  accuracy : 88.6 %\n",
      "9m 0s (- 9m 16s) (2600 49%) loss : 0.355  accuracy : 89.1 %\n",
      "9m 10s (- 9m 5s) (2650 50%) loss : 0.340  accuracy : 89.2 %\n",
      "9m 19s (- 8m 54s) (2700 51%) loss : 0.350  accuracy : 89.2 %\n",
      "9m 29s (- 8m 43s) (2750 52%) loss : 0.351  accuracy : 89.2 %\n",
      "9m 40s (- 8m 33s) (2800 53%) loss : 0.348  accuracy : 89.2 %\n",
      "9m 49s (- 8m 22s) (2850 54%) loss : 0.344  accuracy : 89.4 %\n",
      "10m 0s (- 8m 12s) (2900 54%) loss : 0.350  accuracy : 89.3 %\n",
      "10m 8s (- 8m 0s) (2950 55%) loss : 0.353  accuracy : 89.0 %\n",
      "10m 20s (- 7m 51s) (3000 56%) loss : 0.363  accuracy : 88.6 %\n",
      "10m 30s (- 7m 40s) (3050 57%) loss : 0.348  accuracy : 89.1 %\n",
      "10m 40s (- 7m 29s) (3100 58%) loss : 0.349  accuracy : 89.3 %\n",
      "10m 48s (- 7m 17s) (3150 59%) loss : 0.355  accuracy : 89.0 %\n",
      "10m 57s (- 7m 6s) (3200 60%) loss : 0.352  accuracy : 89.0 %\n",
      "11m 8s (- 6m 56s) (3250 61%) loss : 0.373  accuracy : 88.5 %\n",
      "11m 17s (- 6m 46s) (3300 62%) loss : 0.340  accuracy : 89.4 %\n",
      "11m 26s (- 6m 34s) (3350 63%) loss : 0.357  accuracy : 89.0 %\n",
      "11m 38s (- 6m 25s) (3400 64%) loss : 0.354  accuracy : 89.1 %\n",
      "11m 48s (- 6m 15s) (3450 65%) loss : 0.348  accuracy : 89.2 %\n",
      "11m 58s (- 6m 4s) (3500 66%) loss : 0.345  accuracy : 89.1 %\n",
      "12m 7s (- 5m 53s) (3550 67%) loss : 0.336  accuracy : 89.5 %\n",
      "12m 17s (- 5m 43s) (3600 68%) loss : 0.351  accuracy : 89.4 %\n",
      "12m 29s (- 5m 34s) (3650 69%) loss : 0.351  accuracy : 89.0 %\n",
      "12m 38s (- 5m 23s) (3700 70%) loss : 0.349  accuracy : 89.2 %\n",
      "12m 46s (- 5m 11s) (3750 71%) loss : 0.337  accuracy : 89.6 %\n",
      "12m 56s (- 5m 1s) (3800 72%) loss : 0.350  accuracy : 89.3 %\n",
      "13m 6s (- 4m 51s) (3850 72%) loss : 0.346  accuracy : 89.3 %\n",
      "13m 17s (- 4m 41s) (3900 73%) loss : 0.350  accuracy : 89.2 %\n",
      "13m 26s (- 4m 30s) (3950 74%) loss : 0.332  accuracy : 90.0 %\n",
      "13m 38s (- 4m 21s) (4000 75%) loss : 0.358  accuracy : 89.0 %\n",
      "13m 48s (- 4m 11s) (4050 76%) loss : 0.348  accuracy : 89.2 %\n",
      "13m 59s (- 4m 0s) (4100 77%) loss : 0.349  accuracy : 89.1 %\n",
      "14m 8s (- 3m 50s) (4150 78%) loss : 0.345  accuracy : 89.4 %\n",
      "14m 17s (- 3m 39s) (4200 79%) loss : 0.353  accuracy : 89.3 %\n",
      "14m 28s (- 3m 29s) (4250 80%) loss : 0.350  accuracy : 89.2 %\n",
      "14m 36s (- 3m 19s) (4300 81%) loss : 0.332  accuracy : 89.7 %\n",
      "14m 45s (- 3m 8s) (4350 82%) loss : 0.361  accuracy : 88.7 %\n",
      "14m 58s (- 2m 59s) (4400 83%) loss : 0.358  accuracy : 88.7 %\n",
      "15m 8s (- 2m 48s) (4450 84%) loss : 0.349  accuracy : 89.2 %\n",
      "15m 24s (- 2m 39s) (4500 85%) loss : 0.331  accuracy : 89.6 %\n",
      "15m 38s (- 2m 29s) (4550 86%) loss : 0.347  accuracy : 89.2 %\n",
      "15m 49s (- 2m 19s) (4600 87%) loss : 0.337  accuracy : 89.6 %\n",
      "15m 59s (- 2m 9s) (4650 88%) loss : 0.348  accuracy : 89.4 %\n",
      "16m 11s (- 1m 59s) (4700 89%) loss : 0.350  accuracy : 89.3 %\n",
      "16m 19s (- 1m 48s) (4750 90%) loss : 0.340  accuracy : 89.7 %\n",
      "16m 29s (- 1m 38s) (4800 90%) loss : 0.344  accuracy : 89.4 %\n",
      "16m 39s (- 1m 27s) (4850 91%) loss : 0.348  accuracy : 89.2 %\n",
      "16m 50s (- 1m 17s) (4900 92%) loss : 0.336  accuracy : 89.5 %\n",
      "17m 0s (- 1m 7s) (4950 93%) loss : 0.331  accuracy : 89.7 %\n",
      "17m 12s (- 0m 57s) (5000 94%) loss : 0.352  accuracy : 89.0 %\n",
      "17m 22s (- 0m 46s) (5050 95%) loss : 0.357  accuracy : 88.9 %\n",
      "17m 32s (- 0m 36s) (5100 96%) loss : 0.347  accuracy : 89.1 %\n",
      "17m 39s (- 0m 26s) (5150 97%) loss : 0.344  accuracy : 89.2 %\n",
      "17m 51s (- 0m 15s) (5200 98%) loss : 0.361  accuracy : 88.8 %\n",
      "18m 2s (- 0m 5s) (5250 99%) loss : 0.354  accuracy : 89.1 %\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.fit(batches, epochs = 1, lr = 0.00025, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "#torch.save(pos_tagger.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_pos_tagger.pth')\n",
    "\n",
    "# load\n",
    "#pos_tagger.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_pos_tagger.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ POSTagger Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.84015023260446"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.compute_accuracy(corpus_tst, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.20570774622702"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.compute_accuracy(corpus_add, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mexico 's andres valencia told reporters late tuesday his talks with national liberation army ( eln ) leader francisco galan focused on ways to reduce differences between the rebels and the government in order to set up a possible meeting between the two sides in mexico . mexico 's andres valencia told reporters late tuesday his talks with national liberation army ( eln ) leader francisco galan focused on ways to reduce differences between the rebels and the government in order to set up a possible meeting between the two sides in mexico .\n",
      "\n",
      "\n",
      "[('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 10), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11), ('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 10), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11)]\n",
      "\n",
      "\n",
      "[('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11), ('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.eval()\n",
    "sentence = ' '.join(corpus_tst[11][0]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "print(pos_tagger(sentence))\n",
    "print('\\n')\n",
    "print([(w, i) for w, i in zip(corpus_tst[11][0], corpus_tst[11][1])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
