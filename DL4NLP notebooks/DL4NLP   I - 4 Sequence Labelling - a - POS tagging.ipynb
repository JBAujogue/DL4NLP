{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=orange>I - 4 </font>\n",
    "    Sequence Labelling\n",
    "    \n",
    "  </div> \n",
    "\n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 20px; \n",
    "      margin: 10px;\">\n",
    "  a. POS tagging\n",
    "  </div>\n",
    "\n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=orange>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. Sequence to sequence\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "1. Abstractive Summarization\n",
    "\n",
    "2. Question Answering\n",
    "\n",
    "3. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "A top-quality Github repository discussing Sequence Labelling is found [here](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling)<br><br><br>\n",
    "\n",
    "\n",
    "| **Corpus** | **Popular library using the corpus** | **Data availability** | \n",
    "|------|------|------|\n",
    "| [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42) | Stanford NLP | Extract in NLTK as detailed [here](https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537) | \n",
    "| OntoNotes 5 |  |  | \n",
    "| Groningen Meaning Bank | | Full data [here](https://gmb.let.rug.nl/data.php) - Extract found [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.5.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_DL4NLP = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(path_to_DL4NLP + '\\\\lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet$ Groningen Meaning Bank extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "df_GMB_extract = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_GMB_extract_add = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner_dataset.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050794, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx           word  pos\n",
       "0           1.0      Thousands  NNS\n",
       "1           1.0             of   IN\n",
       "2           1.0  demonstrators  NNS\n",
       "3           1.0           have  VBP\n",
       "4           1.0        marched  VBN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract.dropna(inplace = True)\n",
    "df_GMB_extract = df_GMB_extract[['sentence_idx', 'word', 'pos']]\n",
    "print(df_GMB_extract.shape)\n",
    "df_GMB_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS\n",
       "0  Sentence: 1      Thousands  NNS\n",
       "1  Sentence: 1             of   IN\n",
       "2  Sentence: 1  demonstrators  NNS\n",
       "3  Sentence: 1           have  VBP\n",
       "4  Sentence: 1        marched  VBN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract_add.fillna(method = 'ffill', inplace = True)\n",
    "df_GMB_extract_add = df_GMB_extract_add[['Sentence #', 'Word', 'POS']]\n",
    "df_GMB_extract_add.replace('FW', 'NN', inplace = True)\n",
    "print(df_GMB_extract_add.shape)\n",
    "df_GMB_extract_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "POSs = df_GMB_extract['pos'].unique().tolist()\n",
    "print(len(POSs))\n",
    "POSs2num = {tag : i for i, tag in enumerate(POSs)}\n",
    "num2POSs = {i : tag for i, tag in enumerate(POSs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = df_GMB_extract.groupby(\"sentence_idx\").apply(lambda s: [[w.lower() for w in s[\"word\"].values.tolist()], \n",
    "                                                                 [POSs2num[t] for t in s[\"pos\"].values.tolist()]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_add = df_GMB_extract_add.groupby(\"Sentence #\").apply(lambda s: [[w.lower() for w in s[\"Word\"].values.tolist()], \n",
    "                                                                       [POSs2num[t] for t in s[\"POS\"].values.tolist()]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_trn, corpus_tst = train_test_split(corpus, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_trn = [s_l[0] for s_l in corpus_trn if len(s_l[0]) <= 75]\n",
    "poslabels_trn = [s_l[1] for s_l in corpus_trn if len(s_l[1]) <= 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28141"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(top-seeded, 10, JJ)\n",
      "(lleyton, 4, NNP)\n",
      "(hewitt, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(australia, 4, NNP)\n",
      "(and, 9, CC)\n",
      "(number-two, 10, JJ)\n",
      "(dominik, 4, NNP)\n",
      "(hrbaty, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(slovakia, 4, NNP)\n",
      "(have, 2, VBP)\n",
      "(advanced, 3, VBN)\n",
      "(to, 5, TO)\n",
      "(the, 7, DT)\n",
      "(second, 10, JJ)\n",
      "(round, 8, NN)\n",
      "(of, 1, IN)\n",
      "(the, 7, DT)\n",
      "(international, 10, JJ)\n",
      "(men, 0, NNS)\n",
      "('s, 18, POS)\n",
      "(hardcourt, 8, NN)\n",
      "(tennis, 8, NN)\n",
      "(championships, 0, NNS)\n",
      "(in, 1, IN)\n",
      "(adelaide, 4, NNP)\n",
      "(,, 21, ,)\n",
      "(australia, 4, NNP)\n",
      "(., 11, .)\n",
      "(top-seeded, 10, JJ)\n",
      "(lleyton, 4, NNP)\n",
      "(hewitt, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(australia, 4, NNP)\n",
      "(and, 9, CC)\n",
      "(number-two, 10, JJ)\n",
      "(dominik, 4, NNP)\n",
      "(hrbaty, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(slovakia, 4, NNP)\n",
      "(have, 2, VBP)\n",
      "(advanced, 3, VBN)\n",
      "(to, 5, TO)\n",
      "(the, 7, DT)\n",
      "(second, 10, JJ)\n",
      "(round, 8, NN)\n",
      "(of, 1, IN)\n",
      "(the, 7, DT)\n",
      "(international, 10, JJ)\n",
      "(men, 0, NNS)\n",
      "('s, 18, POS)\n",
      "(hardcourt, 8, NN)\n",
      "(tennis, 8, NN)\n",
      "(championships, 0, NNS)\n",
      "(in, 1, IN)\n",
      "(adelaide, 4, NNP)\n",
      "(,, 21, ,)\n",
      "(australia, 4, NNP)\n",
      "(., 11, .)\n"
     ]
    }
   ],
   "source": [
    "words = corpus_trn[0][0]\n",
    "tags  = corpus_trn[0][1]\n",
    "for word, tag in zip(words, tags) : print('(' + word + ', ' + str(tag)+ ', ' + str(num2POSs[tag]) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective.\n",
    "\n",
    "#### $\\bullet$ FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec = FastText(size = 100, \n",
    "                    window = 5, \n",
    "                    min_count = 1, \n",
    "                    negative = 20,\n",
    "                    sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24652"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.build_vocab(sentences_trn)\n",
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I4a_fasttext.model\")\n",
    "word2vec = FastText.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec.train(sentences_trn,\n",
    "               epochs = 20,\n",
    "               total_examples = word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I4a_fasttext.model\")\n",
    "#word2vec.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder\n",
    "from libDL4NLP.misc    import Highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Part Of Speech Tagging Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SequenceTagger(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layer = 1, \n",
    "                 n_class = 2,\n",
    "                 dropout = 0,\n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(\n",
    "            emb_dim = self.word2vec.out_dim, \n",
    "            hid_dim = hidden_dim, \n",
    "            n_layer = n_layer, \n",
    "            dropout = dropout, \n",
    "            bidirectional = True)\n",
    "        self.out       = nn.Sequential(\n",
    "            Highway(self.context.out_dim, dropout), \n",
    "            nn.Linear(self.context.out_dim, n_class))\n",
    "        self.act       = F.softmax\n",
    "        self.n_class   = n_class\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index_in  = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.ignore_index_out = n_class\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index_out, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, n_class)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length,  n_class)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        return [(w, i) for w, i in zip(words, inds)]\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def sentence2indices(words) :\n",
    "            # convert to indices\n",
    "            inds = [self.word2vec.lang.getIndex(w) for w in words]\n",
    "            inds = [i for i in inds if i is not None]\n",
    "            # apply mask\n",
    "            mask = [m for m, i in enumerate(inds) if i != self.word2vec.lang.getIndex('UNK')]\n",
    "            mask = random.sample(mask, k = int(mask_ratio*(len(mask) +1)))\n",
    "            inds = [maskInput(i, m in mask) for m, i in enumerate(inds)]\n",
    "            return inds\n",
    "        \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s[0]), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = [[sentence2indices(s[0]), s[1]] for s in sentences[i:i + batch_size]]\n",
    "            pack.sort(key = lambda p : len(p[0]), reverse = True)\n",
    "            pack0 = [p[0] for p in pack] \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index_in))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1)) # size (batch_size, max_length)\n",
    "            lengths = torch.tensor([len(p[0]) for p in pack])         # size (batch_size)\n",
    "            pack1 = [p[1] for p in pack]                              # size (batch_size, max_length)\n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index_out))\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1)) # size (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "\n",
    "    # compute model perf\n",
    "    def compute_accuracy(self, sentences, batch_size = 32) :\n",
    "        def computeLogProbs(batch) :\n",
    "            torch.cuda.empty_cache()\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeSuccess(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return success, total\n",
    "        \n",
    "        # --- main ----\n",
    "        self.eval()\n",
    "        batches = self.generatePackedSentences(sentences, batch_size)\n",
    "        score, total = 0, 0\n",
    "        for batch, targets in batches :\n",
    "            log_probs = computeLogProbs(batch).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets = targets.to(self.device)                  # dim = (batch_size, input_length)\n",
    "            s, t = computeSuccess(log_probs, targets)\n",
    "            score += s\n",
    "            total += t\n",
    "        return score * 100 / total\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, \n",
    "            iters = None, epochs = None, lr = 0.025, random_state = 42, \n",
    "            print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs  = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets    = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss       = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if compute_accuracy :\n",
    "                accuracy = computeAccuracy(log_probs, targets)\n",
    "            else : \n",
    "                accuracy = 0\n",
    "            error = float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index_out))\n",
    "            return error, accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ POSTagger Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685091"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger = SequenceTagger(device = device, #torch.device('cpu'), # \n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = Word2VecConnector(word2vec),\n",
    "                            hidden_dim = 150, \n",
    "                            n_layer = 2, \n",
    "                            n_class = 41,\n",
    "                            dropout = 0.15,\n",
    "                            optimizer = optim.AdamW)\n",
    "\n",
    "pos_tagger.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5277"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 42)\n",
    "batches+= pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 4242)\n",
    "batches+= pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 1331)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 16s (- 29m 6s) (50 0%) loss : 2.813  accuracy : 21.5 %\n",
      "0m 31s (- 26m 58s) (100 1%) loss : 1.657  accuracy : 52.9 %\n",
      "0m 44s (- 25m 30s) (150 2%) loss : 1.066  accuracy : 70.8 %\n",
      "0m 58s (- 24m 42s) (200 3%) loss : 0.898  accuracy : 75.2 %\n",
      "1m 11s (- 24m 1s) (250 4%) loss : 0.803  accuracy : 77.4 %\n",
      "1m 25s (- 23m 42s) (300 5%) loss : 0.751  accuracy : 78.3 %\n",
      "1m 39s (- 23m 27s) (350 6%) loss : 0.697  accuracy : 79.5 %\n",
      "1m 53s (- 23m 2s) (400 7%) loss : 0.681  accuracy : 80.1 %\n",
      "2m 6s (- 22m 39s) (450 8%) loss : 0.663  accuracy : 80.5 %\n",
      "2m 18s (- 22m 5s) (500 9%) loss : 0.617  accuracy : 81.8 %\n",
      "2m 32s (- 21m 50s) (550 10%) loss : 0.631  accuracy : 81.3 %\n",
      "2m 45s (- 21m 33s) (600 11%) loss : 0.602  accuracy : 82.0 %\n",
      "2m 59s (- 21m 20s) (650 12%) loss : 0.604  accuracy : 81.6 %\n",
      "3m 12s (- 21m 1s) (700 13%) loss : 0.590  accuracy : 82.2 %\n",
      "3m 26s (- 20m 46s) (750 14%) loss : 0.591  accuracy : 82.2 %\n",
      "3m 40s (- 20m 36s) (800 15%) loss : 0.555  accuracy : 83.2 %\n",
      "3m 53s (- 20m 15s) (850 16%) loss : 0.551  accuracy : 83.5 %\n",
      "4m 6s (- 20m 0s) (900 17%) loss : 0.544  accuracy : 83.4 %\n",
      "4m 19s (- 19m 40s) (950 18%) loss : 0.542  accuracy : 83.5 %\n",
      "4m 32s (- 19m 26s) (1000 18%) loss : 0.546  accuracy : 83.5 %\n",
      "4m 46s (- 19m 14s) (1050 19%) loss : 0.548  accuracy : 83.1 %\n",
      "5m 2s (- 19m 6s) (1100 20%) loss : 0.536  accuracy : 83.8 %\n",
      "5m 16s (- 18m 57s) (1150 21%) loss : 0.525  accuracy : 83.9 %\n",
      "5m 30s (- 18m 43s) (1200 22%) loss : 0.499  accuracy : 85.1 %\n",
      "5m 43s (- 18m 27s) (1250 23%) loss : 0.505  accuracy : 84.5 %\n",
      "5m 57s (- 18m 13s) (1300 24%) loss : 0.496  accuracy : 84.9 %\n",
      "6m 10s (- 17m 59s) (1350 25%) loss : 0.495  accuracy : 84.7 %\n",
      "6m 25s (- 17m 48s) (1400 26%) loss : 0.506  accuracy : 84.8 %\n",
      "6m 38s (- 17m 32s) (1450 27%) loss : 0.494  accuracy : 85.1 %\n",
      "6m 53s (- 17m 19s) (1500 28%) loss : 0.498  accuracy : 85.2 %\n",
      "7m 7s (- 17m 8s) (1550 29%) loss : 0.466  accuracy : 85.7 %\n",
      "7m 22s (- 16m 57s) (1600 30%) loss : 0.482  accuracy : 85.2 %\n",
      "7m 36s (- 16m 44s) (1650 31%) loss : 0.493  accuracy : 85.1 %\n",
      "7m 50s (- 16m 29s) (1700 32%) loss : 0.463  accuracy : 85.9 %\n",
      "8m 5s (- 16m 17s) (1750 33%) loss : 0.483  accuracy : 85.1 %\n",
      "8m 17s (- 16m 0s) (1800 34%) loss : 0.467  accuracy : 86.1 %\n",
      "8m 31s (- 15m 48s) (1850 35%) loss : 0.487  accuracy : 84.9 %\n",
      "8m 46s (- 15m 36s) (1900 36%) loss : 0.451  accuracy : 86.2 %\n",
      "9m 0s (- 15m 22s) (1950 36%) loss : 0.455  accuracy : 86.1 %\n",
      "9m 13s (- 15m 6s) (2000 37%) loss : 0.456  accuracy : 86.0 %\n",
      "9m 26s (- 14m 52s) (2050 38%) loss : 0.455  accuracy : 86.1 %\n",
      "9m 41s (- 14m 39s) (2100 39%) loss : 0.460  accuracy : 86.0 %\n",
      "9m 56s (- 14m 27s) (2150 40%) loss : 0.470  accuracy : 85.8 %\n",
      "10m 11s (- 14m 15s) (2200 41%) loss : 0.467  accuracy : 85.7 %\n",
      "10m 25s (- 14m 1s) (2250 42%) loss : 0.446  accuracy : 86.4 %\n",
      "10m 39s (- 13m 47s) (2300 43%) loss : 0.447  accuracy : 86.3 %\n",
      "10m 53s (- 13m 33s) (2350 44%) loss : 0.451  accuracy : 86.2 %\n",
      "11m 5s (- 13m 18s) (2400 45%) loss : 0.440  accuracy : 86.9 %\n",
      "11m 22s (- 13m 7s) (2450 46%) loss : 0.446  accuracy : 86.4 %\n",
      "11m 37s (- 12m 54s) (2500 47%) loss : 0.444  accuracy : 86.3 %\n",
      "11m 50s (- 12m 39s) (2550 48%) loss : 0.427  accuracy : 86.9 %\n",
      "12m 4s (- 12m 26s) (2600 49%) loss : 0.432  accuracy : 86.8 %\n",
      "12m 18s (- 12m 12s) (2650 50%) loss : 0.457  accuracy : 86.1 %\n",
      "12m 31s (- 11m 57s) (2700 51%) loss : 0.433  accuracy : 86.8 %\n",
      "12m 45s (- 11m 43s) (2750 52%) loss : 0.446  accuracy : 86.5 %\n",
      "13m 1s (- 11m 31s) (2800 53%) loss : 0.434  accuracy : 86.9 %\n",
      "13m 15s (- 11m 17s) (2850 54%) loss : 0.424  accuracy : 87.1 %\n",
      "13m 29s (- 11m 3s) (2900 54%) loss : 0.421  accuracy : 87.1 %\n",
      "13m 43s (- 10m 49s) (2950 55%) loss : 0.423  accuracy : 87.0 %\n",
      "13m 57s (- 10m 35s) (3000 56%) loss : 0.411  accuracy : 87.4 %\n",
      "14m 11s (- 10m 21s) (3050 57%) loss : 0.407  accuracy : 87.4 %\n",
      "14m 25s (- 10m 7s) (3100 58%) loss : 0.404  accuracy : 87.6 %\n",
      "14m 39s (- 9m 53s) (3150 59%) loss : 0.412  accuracy : 87.5 %\n",
      "14m 51s (- 9m 38s) (3200 60%) loss : 0.414  accuracy : 87.4 %\n",
      "15m 6s (- 9m 25s) (3250 61%) loss : 0.413  accuracy : 87.5 %\n",
      "15m 20s (- 9m 11s) (3300 62%) loss : 0.407  accuracy : 87.6 %\n",
      "15m 32s (- 8m 56s) (3350 63%) loss : 0.397  accuracy : 87.8 %\n",
      "15m 44s (- 8m 41s) (3400 64%) loss : 0.397  accuracy : 88.0 %\n",
      "15m 58s (- 8m 27s) (3450 65%) loss : 0.397  accuracy : 87.7 %\n",
      "16m 14s (- 8m 14s) (3500 66%) loss : 0.417  accuracy : 87.0 %\n",
      "16m 28s (- 8m 0s) (3550 67%) loss : 0.402  accuracy : 87.6 %\n",
      "16m 41s (- 7m 46s) (3600 68%) loss : 0.397  accuracy : 87.7 %\n",
      "16m 58s (- 7m 33s) (3650 69%) loss : 0.415  accuracy : 87.5 %\n",
      "17m 13s (- 7m 20s) (3700 70%) loss : 0.395  accuracy : 88.0 %\n",
      "17m 28s (- 7m 6s) (3750 71%) loss : 0.407  accuracy : 87.5 %\n",
      "17m 43s (- 6m 53s) (3800 72%) loss : 0.391  accuracy : 87.9 %\n",
      "17m 58s (- 6m 39s) (3850 72%) loss : 0.394  accuracy : 87.8 %\n",
      "18m 11s (- 6m 25s) (3900 73%) loss : 0.394  accuracy : 87.8 %\n",
      "18m 25s (- 6m 11s) (3950 74%) loss : 0.387  accuracy : 87.8 %\n",
      "18m 41s (- 5m 57s) (4000 75%) loss : 0.413  accuracy : 87.2 %\n",
      "18m 54s (- 5m 43s) (4050 76%) loss : 0.398  accuracy : 88.0 %\n",
      "19m 10s (- 5m 30s) (4100 77%) loss : 0.392  accuracy : 87.9 %\n",
      "19m 23s (- 5m 16s) (4150 78%) loss : 0.387  accuracy : 87.9 %\n",
      "19m 38s (- 5m 2s) (4200 79%) loss : 0.397  accuracy : 87.6 %\n",
      "19m 51s (- 4m 48s) (4250 80%) loss : 0.392  accuracy : 87.9 %\n",
      "20m 4s (- 4m 33s) (4300 81%) loss : 0.386  accuracy : 88.1 %\n",
      "20m 17s (- 4m 19s) (4350 82%) loss : 0.386  accuracy : 88.3 %\n",
      "20m 31s (- 4m 5s) (4400 83%) loss : 0.382  accuracy : 88.2 %\n",
      "20m 46s (- 3m 51s) (4450 84%) loss : 0.389  accuracy : 88.1 %\n",
      "21m 0s (- 3m 37s) (4500 85%) loss : 0.395  accuracy : 88.1 %\n",
      "21m 13s (- 3m 23s) (4550 86%) loss : 0.396  accuracy : 87.8 %\n",
      "21m 28s (- 3m 9s) (4600 87%) loss : 0.385  accuracy : 88.3 %\n",
      "21m 42s (- 2m 55s) (4650 88%) loss : 0.369  accuracy : 88.6 %\n",
      "21m 56s (- 2m 41s) (4700 89%) loss : 0.391  accuracy : 87.7 %\n",
      "22m 10s (- 2m 27s) (4750 90%) loss : 0.383  accuracy : 88.2 %\n",
      "22m 23s (- 2m 13s) (4800 90%) loss : 0.378  accuracy : 88.3 %\n",
      "22m 35s (- 1m 59s) (4850 91%) loss : 0.379  accuracy : 88.5 %\n",
      "22m 47s (- 1m 45s) (4900 92%) loss : 0.350  accuracy : 89.4 %\n",
      "23m 1s (- 1m 31s) (4950 93%) loss : 0.381  accuracy : 88.4 %\n",
      "23m 14s (- 1m 17s) (5000 94%) loss : 0.375  accuracy : 88.4 %\n",
      "23m 26s (- 1m 3s) (5050 95%) loss : 0.362  accuracy : 88.8 %\n",
      "23m 40s (- 0m 49s) (5100 96%) loss : 0.386  accuracy : 88.1 %\n",
      "23m 52s (- 0m 35s) (5150 97%) loss : 0.381  accuracy : 88.4 %\n",
      "24m 6s (- 0m 21s) (5200 98%) loss : 0.386  accuracy : 88.3 %\n",
      "24m 20s (- 0m 7s) (5250 99%) loss : 0.388  accuracy : 88.0 %\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.fit(batches, epochs = 1, lr = 0.001, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 13s (- 24m 17s) (50 0%) loss : 0.367  accuracy : 88.6 %\n",
      "0m 27s (- 23m 40s) (100 1%) loss : 0.353  accuracy : 89.0 %\n",
      "0m 42s (- 24m 20s) (150 2%) loss : 0.349  accuracy : 89.1 %\n",
      "0m 57s (- 24m 14s) (200 3%) loss : 0.361  accuracy : 88.9 %\n",
      "1m 10s (- 23m 46s) (250 4%) loss : 0.347  accuracy : 89.1 %\n",
      "1m 24s (- 23m 15s) (300 5%) loss : 0.361  accuracy : 88.9 %\n",
      "1m 39s (- 23m 13s) (350 6%) loss : 0.376  accuracy : 88.6 %\n",
      "1m 53s (- 23m 7s) (400 7%) loss : 0.357  accuracy : 88.8 %\n",
      "2m 6s (- 22m 31s) (450 8%) loss : 0.367  accuracy : 88.6 %\n",
      "2m 18s (- 22m 4s) (500 9%) loss : 0.349  accuracy : 89.5 %\n",
      "2m 33s (- 22m 0s) (550 10%) loss : 0.348  accuracy : 89.4 %\n",
      "2m 47s (- 21m 44s) (600 11%) loss : 0.331  accuracy : 89.8 %\n",
      "3m 2s (- 21m 39s) (650 12%) loss : 0.341  accuracy : 89.3 %\n",
      "3m 16s (- 21m 25s) (700 13%) loss : 0.362  accuracy : 89.0 %\n",
      "3m 30s (- 21m 9s) (750 14%) loss : 0.346  accuracy : 89.1 %\n",
      "3m 43s (- 20m 50s) (800 15%) loss : 0.352  accuracy : 89.2 %\n",
      "3m 57s (- 20m 37s) (850 16%) loss : 0.334  accuracy : 89.6 %\n",
      "4m 13s (- 20m 32s) (900 17%) loss : 0.352  accuracy : 89.2 %\n",
      "4m 27s (- 20m 17s) (950 18%) loss : 0.345  accuracy : 89.3 %\n",
      "4m 41s (- 20m 5s) (1000 18%) loss : 0.353  accuracy : 89.2 %\n",
      "4m 57s (- 19m 55s) (1050 19%) loss : 0.344  accuracy : 89.3 %\n",
      "5m 11s (- 19m 41s) (1100 20%) loss : 0.341  accuracy : 89.5 %\n",
      "5m 26s (- 19m 32s) (1150 21%) loss : 0.343  accuracy : 89.3 %\n",
      "5m 39s (- 19m 13s) (1200 22%) loss : 0.343  accuracy : 89.3 %\n",
      "5m 54s (- 19m 2s) (1250 23%) loss : 0.352  accuracy : 89.2 %\n",
      "6m 8s (- 18m 47s) (1300 24%) loss : 0.354  accuracy : 89.0 %\n",
      "6m 21s (- 18m 29s) (1350 25%) loss : 0.347  accuracy : 89.2 %\n",
      "6m 36s (- 18m 18s) (1400 26%) loss : 0.361  accuracy : 88.9 %\n",
      "6m 52s (- 18m 8s) (1450 27%) loss : 0.360  accuracy : 88.9 %\n",
      "7m 5s (- 17m 52s) (1500 28%) loss : 0.341  accuracy : 89.8 %\n",
      "7m 20s (- 17m 39s) (1550 29%) loss : 0.366  accuracy : 88.6 %\n",
      "7m 37s (- 17m 30s) (1600 30%) loss : 0.346  accuracy : 89.3 %\n",
      "7m 49s (- 17m 12s) (1650 31%) loss : 0.330  accuracy : 89.6 %\n",
      "8m 3s (- 16m 56s) (1700 32%) loss : 0.357  accuracy : 89.0 %\n",
      "8m 16s (- 16m 40s) (1750 33%) loss : 0.336  accuracy : 89.7 %\n",
      "8m 30s (- 16m 25s) (1800 34%) loss : 0.365  accuracy : 88.6 %\n",
      "8m 44s (- 16m 11s) (1850 35%) loss : 0.341  accuracy : 89.7 %\n",
      "8m 59s (- 15m 59s) (1900 36%) loss : 0.340  accuracy : 89.5 %\n",
      "9m 11s (- 15m 41s) (1950 36%) loss : 0.319  accuracy : 90.0 %\n",
      "9m 25s (- 15m 26s) (2000 37%) loss : 0.347  accuracy : 89.5 %\n",
      "9m 40s (- 15m 13s) (2050 38%) loss : 0.338  accuracy : 89.7 %\n",
      "9m 52s (- 14m 56s) (2100 39%) loss : 0.341  accuracy : 89.5 %\n",
      "10m 8s (- 14m 44s) (2150 40%) loss : 0.333  accuracy : 89.7 %\n",
      "10m 24s (- 14m 33s) (2200 41%) loss : 0.343  accuracy : 89.3 %\n",
      "10m 38s (- 14m 19s) (2250 42%) loss : 0.333  accuracy : 89.7 %\n",
      "10m 51s (- 14m 3s) (2300 43%) loss : 0.326  accuracy : 89.8 %\n",
      "11m 4s (- 13m 48s) (2350 44%) loss : 0.338  accuracy : 89.6 %\n",
      "11m 17s (- 13m 32s) (2400 45%) loss : 0.337  accuracy : 89.7 %\n",
      "11m 32s (- 13m 18s) (2450 46%) loss : 0.343  accuracy : 89.5 %\n",
      "11m 46s (- 13m 4s) (2500 47%) loss : 0.339  accuracy : 89.6 %\n",
      "11m 59s (- 12m 49s) (2550 48%) loss : 0.333  accuracy : 89.7 %\n",
      "12m 13s (- 12m 34s) (2600 49%) loss : 0.348  accuracy : 89.2 %\n",
      "12m 27s (- 12m 21s) (2650 50%) loss : 0.334  accuracy : 89.7 %\n",
      "12m 39s (- 12m 5s) (2700 51%) loss : 0.346  accuracy : 89.6 %\n",
      "12m 53s (- 11m 50s) (2750 52%) loss : 0.343  accuracy : 89.6 %\n",
      "13m 7s (- 11m 36s) (2800 53%) loss : 0.335  accuracy : 89.6 %\n",
      "13m 21s (- 11m 22s) (2850 54%) loss : 0.347  accuracy : 89.4 %\n",
      "13m 35s (- 11m 8s) (2900 54%) loss : 0.328  accuracy : 89.8 %\n",
      "13m 51s (- 10m 55s) (2950 55%) loss : 0.345  accuracy : 89.5 %\n",
      "14m 4s (- 10m 41s) (3000 56%) loss : 0.345  accuracy : 89.4 %\n",
      "14m 18s (- 10m 26s) (3050 57%) loss : 0.336  accuracy : 89.4 %\n",
      "14m 32s (- 10m 12s) (3100 58%) loss : 0.336  accuracy : 89.6 %\n",
      "14m 46s (- 9m 58s) (3150 59%) loss : 0.338  accuracy : 89.9 %\n",
      "15m 1s (- 9m 45s) (3200 60%) loss : 0.331  accuracy : 90.1 %\n",
      "15m 15s (- 9m 30s) (3250 61%) loss : 0.339  accuracy : 89.2 %\n",
      "15m 30s (- 9m 17s) (3300 62%) loss : 0.337  accuracy : 89.6 %\n",
      "15m 43s (- 9m 2s) (3350 63%) loss : 0.327  accuracy : 89.9 %\n",
      "15m 59s (- 8m 49s) (3400 64%) loss : 0.328  accuracy : 89.7 %\n",
      "16m 13s (- 8m 35s) (3450 65%) loss : 0.336  accuracy : 89.8 %\n",
      "16m 26s (- 8m 21s) (3500 66%) loss : 0.333  accuracy : 89.8 %\n",
      "16m 41s (- 8m 7s) (3550 67%) loss : 0.327  accuracy : 89.8 %\n",
      "16m 56s (- 7m 53s) (3600 68%) loss : 0.335  accuracy : 89.7 %\n",
      "17m 10s (- 7m 39s) (3650 69%) loss : 0.326  accuracy : 90.1 %\n",
      "17m 25s (- 7m 25s) (3700 70%) loss : 0.330  accuracy : 89.7 %\n",
      "17m 39s (- 7m 11s) (3750 71%) loss : 0.339  accuracy : 89.6 %\n",
      "17m 54s (- 6m 57s) (3800 72%) loss : 0.336  accuracy : 89.6 %\n",
      "18m 7s (- 6m 43s) (3850 72%) loss : 0.349  accuracy : 89.3 %\n",
      "18m 21s (- 6m 28s) (3900 73%) loss : 0.332  accuracy : 89.7 %\n",
      "18m 34s (- 6m 14s) (3950 74%) loss : 0.336  accuracy : 89.4 %\n",
      "18m 48s (- 6m 0s) (4000 75%) loss : 0.326  accuracy : 90.1 %\n",
      "19m 2s (- 5m 46s) (4050 76%) loss : 0.328  accuracy : 90.0 %\n",
      "19m 15s (- 5m 31s) (4100 77%) loss : 0.319  accuracy : 90.0 %\n",
      "19m 28s (- 5m 17s) (4150 78%) loss : 0.331  accuracy : 89.8 %\n",
      "19m 42s (- 5m 3s) (4200 79%) loss : 0.326  accuracy : 90.0 %\n",
      "19m 57s (- 4m 49s) (4250 80%) loss : 0.332  accuracy : 89.7 %\n",
      "20m 9s (- 4m 34s) (4300 81%) loss : 0.340  accuracy : 89.6 %\n",
      "20m 21s (- 4m 20s) (4350 82%) loss : 0.320  accuracy : 90.3 %\n",
      "20m 34s (- 4m 6s) (4400 83%) loss : 0.326  accuracy : 89.9 %\n",
      "20m 48s (- 3m 52s) (4450 84%) loss : 0.328  accuracy : 89.8 %\n",
      "21m 3s (- 3m 38s) (4500 85%) loss : 0.331  accuracy : 89.6 %\n",
      "21m 17s (- 3m 24s) (4550 86%) loss : 0.334  accuracy : 89.7 %\n",
      "21m 32s (- 3m 10s) (4600 87%) loss : 0.333  accuracy : 90.0 %\n",
      "21m 45s (- 2m 56s) (4650 88%) loss : 0.324  accuracy : 90.1 %\n",
      "21m 59s (- 2m 41s) (4700 89%) loss : 0.325  accuracy : 89.7 %\n",
      "22m 12s (- 2m 27s) (4750 90%) loss : 0.332  accuracy : 89.6 %\n",
      "22m 24s (- 2m 13s) (4800 90%) loss : 0.314  accuracy : 90.1 %\n",
      "22m 39s (- 1m 59s) (4850 91%) loss : 0.345  accuracy : 89.4 %\n",
      "22m 52s (- 1m 45s) (4900 92%) loss : 0.315  accuracy : 90.3 %\n",
      "23m 4s (- 1m 31s) (4950 93%) loss : 0.315  accuracy : 90.0 %\n",
      "23m 16s (- 1m 17s) (5000 94%) loss : 0.331  accuracy : 89.9 %\n",
      "23m 31s (- 1m 3s) (5050 95%) loss : 0.332  accuracy : 89.7 %\n",
      "23m 46s (- 0m 49s) (5100 96%) loss : 0.335  accuracy : 89.6 %\n",
      "24m 2s (- 0m 35s) (5150 97%) loss : 0.330  accuracy : 89.6 %\n",
      "24m 15s (- 0m 21s) (5200 98%) loss : 0.336  accuracy : 89.7 %\n",
      "24m 31s (- 0m 7s) (5250 99%) loss : 0.347  accuracy : 89.4 %\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.fit(batches, epochs = 1, lr = 0.00025, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "#torch.save(pos_tagger.state_dict(), path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I4a_pos_tagger.pth')\n",
    "\n",
    "# load\n",
    "#pos_tagger.load_state_dict(torch.load(path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I4a_pos_tagger.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ POSTagger Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.06682695271562"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.compute_accuracy(corpus_tst, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.46634718546599"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.compute_accuracy(corpus_add, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mexico 's andres valencia told reporters late tuesday his talks with national liberation army ( eln ) leader francisco galan focused on ways to reduce differences between the rebels and the government in order to set up a possible meeting between the two sides in mexico . mexico 's andres valencia told reporters late tuesday his talks with national liberation army ( eln ) leader francisco galan focused on ways to reduce differences between the rebels and the government in order to set up a possible meeting between the two sides in mexico .\n",
      "\n",
      "\n",
      "[('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11), ('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11)]\n",
      "\n",
      "\n",
      "[('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11), ('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.eval()\n",
    "sentence = ' '.join(corpus_tst[11][0]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "print(pos_tagger(sentence))\n",
    "print('\\n')\n",
    "print([(w, i) for w, i in zip(corpus_tst[11][0], corpus_tst[11][1])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
