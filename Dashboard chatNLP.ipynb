{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Dashboard\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  Jean-baptiste Aujogue\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Models](#models)\n",
    "\n",
    "### Part I\n",
    "\n",
    "1. [Word Embedding](#embedding)\n",
    "\n",
    "|  | CBOW | Skipgram|\n",
    "|------|------|------|\n",
    "| Word-level Embedding | [1.1](#CBOW) | [1.2](#Skipgram) |\n",
    "\n",
    "2. [Sentence Classification](#sentence_classification)\n",
    "    \n",
    "3. [Language Model](#languageModel)\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoder\n",
    "\n",
    "6. Translator\n",
    "\n",
    "7. Text classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive summarization\n",
    "\n",
    "9. Question answering\n",
    "\n",
    "10. [Chatbot](#chatbot)\n",
    "\n",
    "\n",
    "|  | Sans mémoire | Avec mémoire à règles | Avec mémoire agnostique|\n",
    "|------|------|------|\n",
    "| **Sélectif** | [10.1.1](#ChatbotsSelectifsSansMemoire) | [10.2.1](#ChatbotsSelectifsAvecMemoireRegles) | [10.3.1](#ChatbotsSelectifsAvecMemoireAgnostique) |\n",
    "| **Génératif** | [10.1.2](#ChatbotsGeneratifsSansMemoire) | [10.2.2](#ChatbotsGeneratifsAvecMemoireRegles) | [10.3.2](#ChatbotsGeneratifsAvecMemoireAgnostique) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# [Modules](#modules)\n",
    "\n",
    "2.1 [Encodeurs de mots](#encodeursDeMots)\n",
    "- 2.1.1 Encodeur Récurrent\n",
    "        \n",
    "2.2 [Modules d'attention simple](#attentionSimple)\n",
    "- 2.2.1 Attention additive\n",
    "- 2.2.2 Attention additive multi-tête\n",
    "- 2.2.3 Attention additive multi-hopée\n",
    "        \n",
    "2.3 [Modules d'attention hiérarchique](#attentionHierarchique)\n",
    "    \n",
    "2.4 [Décodeurs](#decodeurs)\n",
    "- 2.4.1 Décodeur sélectif\n",
    "- 2.4.2 Décodeur génératif\n",
    "- 2.4.3 Décodeur génératif à attention\n",
    "- 2.4.4 Décodeur génératif à modèle linguistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Miscellaneous](#misc)\n",
    "\n",
    "\n",
    "M.1 [Filtre Anti-bruit](#FiltreAntiBruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Utils](#utils)\n",
    "\n",
    "\n",
    "4.1 [Language](#lang)\n",
    "\n",
    "4.2 [Attention weights visualization](#attn_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bas de page](#basDePage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du répertoire principal contenant la librairie, dans le quel on se déplace ensuite et où on génère un fichier README.txt avec une brève présentation de cette librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier chatNLP existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir chatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jb\\Desktop\\NLP\\chatNLP\\chatNLP\n"
     ]
    }
   ],
   "source": [
    "%cd chatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting README.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile README.txt\n",
    "\n",
    "\n",
    "Inspiration pour la construction de la librairie :\n",
    "    \n",
    "https://github.com/pytorch/fairseq\n",
    "https://github.com/allenai/allennlp\n",
    "https://www.dabeaz.com/modulepackage/ModulePackage.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation du répertoire courant en librairie Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "\n",
    "#import libNLP.modules\n",
    "#import libNLP.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Models\n",
    "***\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Génération du sous-répertoire _libNLP.models_ contenant l'ensemble des modèles de Deep Learning développés dans cette librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier models existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/__init__.py\n",
    "\n",
    "\n",
    "from .Word_Embedding import Word2Vec, Word2VecConnector, Word2VecShell\n",
    "from .Sentence_Classifier import SentenceClassifier\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'Word2Vec',\n",
    "    'Word2VecConnector',\n",
    "    'Word2VecShell',\n",
    "    \n",
    "    'SentenceClassifier',\n",
    "    \n",
    "    'LanguageModel',\n",
    "    \n",
    "    'Chatbot',\n",
    "    'CreateBot',\n",
    "    'BotTrainer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"embedding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Embedding\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CBOW\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Continuous Bag Of Word (CBOW)\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient}, et consiste à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite de façon à pouvoir prédire chaque mot à partir de son contexte. La construction de cette table $T$ passe par la création d'un réseau de neurones, qui sert de modèle pour l'estimation de la probabilité de prédiction d'un mot $w_t$ d'après son contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce qu'un mot $w_t$ maximise la vraisemblance de la probabilité $P(. \\, | \\, c)$ fournie par le modèle. \n",
    "\n",
    "Le réseau de neurones de décrit de la façon suivante :\n",
    "\n",
    "![cbow](figs/CBOW.png)\n",
    "\n",
    "Un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ est vectorisé via une table $T$ fournissant un ensemble de vecteurs denses (typiquement de dimension comprise entre 50 et 300) $T(w_{t-N}), \\, ... \\, , T(w_{t-1})$, $T(w_{t+1}), \\, ... \\, , T(w_{t+N})$. Chaque vecteur est ensuite transformé via une transformation affine, dont les vecteurs résultants sont superposés en un unique vecteur\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\sum _{i = - N}^N M_i T(w_{t+i}) + b_i\n",
    "\\end{align*}\n",
    "\n",
    "Le vecteur $v_c$ est de dimension typiquement égale à la dimension de la vectorisation de mots. Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que le mot $w_{t}$ soit transformé en un vecteur $T'(w_{t})$ par cette table, et soit proposé en position $t$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_{t} \\, | \\, c\\,) = \\frac{\\exp\\left( T'(w_{t}) \\cdot v_c \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) \\cdot v_c \n",
    "\\right) }\n",
    "\\end{align*}\n",
    "\n",
    "Ici $\\cdot$ désigne le produit scalaire entre vecteurs. L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer un mot à partir du contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Skipgram\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Skip-Gram\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient} comme version mirroir au Continuous Bag Of Words, et consiste là encore à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite non pas de façon prédire un mot central $w$ à partir d'un contexte $c $ comme pour CBOW, mais plutôt de prédire le contexte $c $ à partir du mot central $w$. La construction de cette table $T$ passe par la création d'un réseau de neurones servant de modèle pour l'estimation de la probabilité de prédiction d'un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ à partir d'un mot central $w_t$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce que le contexte  $ c $ maximise la vraisemblance de la probabilité $P( . \\, | \\, w_t)$ fournie par le modèle.\n",
    "\n",
    "\n",
    "Une implémentation de ce modèle est la suivante : \n",
    "\n",
    "\n",
    "![cbow](figs/skipgram.png)\n",
    "\n",
    "\n",
    "Un mot courant $w_t$ est vectorisé par une table $T$ fournissant un vecteur dense (typiquement de dimension comprise entre 50 et 300) $T(w_t)$. Ce vecteur est alors transformé en un ensemble de $2N$ vecteurs\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma (M_{i} T(w_t) + b_{i}) \\qquad \\qquad i =-N,\\, ...\\, , -1, 1, \\, ...\\, , N\n",
    "\\end{align*}\n",
    "\n",
    "où $N$ désigne la taille de la fenêtre retenue, d'une dimension typiquement égale à la dimension de la vectorisation de mots, et $\\sigma$ une fonction non linéaire (typiquement la _Rectified Linear Unit_ $\\sigma (x) = max (0, x)$). Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que chaque mot $w_{t+i}$, transformé en un vecteur $T'(w_{t+i})$ par cette table, soit proposé en position $t+i$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P( w_{t+i} | \\, w_t) = \\frac{\\exp\\left(  T'(w_{t+i}) ^\\perp \\sigma \\left( M_i T(w_t) + b_{i}\\right) \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) ^\\perp \\sigma \\left( M_i T(w_t) + b_i\\right) \\right) }\n",
    "\\end{align*}\n",
    "\n",
    "On modélise alors la probabilité qu'un ensemble de mots $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ soit le contexte d'un mot $w_t$ par le produit\n",
    "\n",
    "\\begin{align*}\n",
    " P( c\\, | \\, w_t) = \\prod _{i = -N}^N P( w_{t+i}\\, | \\, w_t)\n",
    "\\end{align*}\n",
    "\n",
    "Ce modèle de probabilité du contexte d'un mot est naif au sens où les mots de contextes sont considérés comme indépendants deux à deux dès lors que le mot central est connu. Cette approximation rend cependant le calcul d'optimisation beaucoup plus court.\n",
    "\n",
    "\n",
    "\n",
    "L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer l'intégralité du contexte à partir de ce seul mot. La vectorisation Skip-Gram est typiquement plus performante que CBOW, car la table $T$ subit plus de contrainte dans son optimisation, et puisque le vecteur d'un mot est obtenu de façon à pouvoir prédire l'utilisation réelle du mot, ici donnée par son contexte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Word_Embedding.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Word_Embedding.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from chatNLP.utils import Lang\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "#                       Word Embedding model                        #\n",
    "#-------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module) :\n",
    "    def __init__(self, lang, T = 100):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.lang = lang\n",
    "        if type(T) == int :\n",
    "            self.embedding = nn.Embedding(lang.n_words, T)  \n",
    "        else :\n",
    "            self.embedding = nn.Embedding(T.shape[0], T.shape[1])\n",
    "            self.embedding.weight = nn.Parameter(torch.FloatTensor(T))\n",
    "            \n",
    "        self.output_dim = self.lookupTable().shape[1]\n",
    "        self.sims = None\n",
    "        \n",
    "    def lookupTable(self) :\n",
    "        return self.embedding.weight.cpu().detach().numpy()\n",
    "        \n",
    "    def computeSimilarities(self) :\n",
    "        T = normalize(self.lookupTable(), norm = 'l2', axis = 1)\n",
    "        self.sims = np.matmul(T, T.transpose())\n",
    "        return\n",
    "\n",
    "    def most_similar(self, word, bound = 10) :\n",
    "        if word not in self.lang.word2index : return\n",
    "        if self.sims is None : self.computeSimilarities()\n",
    "        index = self.lang.word2index[word]\n",
    "        coefs = self.sims[index]\n",
    "        indices = coefs.argsort()[-bound -1 :-1]\n",
    "        output = [(self.lang.index2word[i], coefs[i]) for i in reversed(indices)]\n",
    "        return output\n",
    "    \n",
    "    def wv(self, word) :\n",
    "        return self.lookupTable()[self.lang.getIndex(word)]\n",
    "    \n",
    "    def addWord(self, word, vector = None) :\n",
    "        self.lang.addWord(word)\n",
    "        T = self.lookupTable()\n",
    "        v = np.random.rand(1, T.shape[1]) if vector is None else vector\n",
    "        updated_T = np.concatenate((T, v), axis = 0)\n",
    "        self.embedding = nn.Embedding(updated_T.shape[0], updated_T.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(updated_T))\n",
    "        return\n",
    "    \n",
    "    def freeze(self) :\n",
    "        for param in self.embedding.parameters() : param.requires_grad = False\n",
    "        return self\n",
    "    \n",
    "    def unfreeze(self) :\n",
    "        for param in self.embedding.parameters() : param.requires_grad = True\n",
    "        return self\n",
    "    \n",
    "    def forward(self, words, device = None) :\n",
    "        '''Transforms a list of n words into a torch.FloatTensor of size (1, n, emb_dim)'''\n",
    "        indices  = [self.lang.getIndex(w) for w in words]\n",
    "        indices  = [[i for i in indices if i is not None]]\n",
    "        variable = Variable(torch.LongTensor(indices)) # size = (1, n)\n",
    "        if device is not None : variable = variable.to(device)\n",
    "        tensor   = self.embedding(variable)            # size = (1, n, emb_dim)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "\n",
    "class Word2VecConnector(nn.Module) :\n",
    "    '''A Pytorch module wrapping a FastText word2vec model'''\n",
    "    def __init__(self, word2vec) :\n",
    "        super(Word2VecConnector, self).__init__()\n",
    "        self.word2vec = word2vec\n",
    "        self.twin = Word2Vec(lang = Lang([list(word2vec.wv.index2word)], base_tokens = []), T = word2vec.wv.vectors)\n",
    "        self.twin.addWord('PADDING_WORD')\n",
    "        self.twin.addWord('UNK')\n",
    "        self.twin = self.twin.freeze()\n",
    "        \n",
    "        self.lang       = self.twin.lang\n",
    "        self.embedding  = self.twin.embedding\n",
    "        self.output_dim = self.twin.output_dim\n",
    "        \n",
    "    def lookupTable(self) :\n",
    "        return self.word2vec.wv.vectors\n",
    "        \n",
    "    def forward(self, words, device = None) :\n",
    "        '''Transforms a sequence of n words into a Torch FloatTensor of size (1, n, emb_dim)'''\n",
    "        try :\n",
    "            embeddings = Variable(torch.Tensor(self.word2vec[words])).unsqueeze(0)\n",
    "            if device is not None : embeddings = embeddings.to(device)\n",
    "        except :\n",
    "            embeddings = self.twin(words, device)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "#                         training shell                            #\n",
    "#-------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "class Word2VecShell(nn.Module):\n",
    "    '''Word2Vec model :\n",
    "        - sg = 0 yields CBOW training procedure\n",
    "        - sg = 1 yields Skip-Gram training procedure\n",
    "    '''\n",
    "    def __init__(self, word2vec, device, sg = 0, context_size = 5, hidden_dim = 150, \n",
    "                 criterion = nn.NLLLoss(size_average = False), optimizer = optim.SGD):\n",
    "        super(Word2VecShell, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # core of Word2Vec\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "        # training layers\n",
    "        self.input_n_words  = (2 * context_size if sg == 0 else 1)\n",
    "        self.output_n_words = (1 if sg == 0 else 2 * context_size)\n",
    "        self.linear_1  = nn.Linear(self.input_n_words * word2vec.embedding.weight.size(1), self.output_n_words * hidden_dim)\n",
    "        self.linear_2  = nn.Linear(hidden_dim, lang.n_words)\n",
    "        \n",
    "        # training tools\n",
    "        self.sg = sg\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        '''Transforms a batch of Ngrams of size (batch_size, input_n_words)\n",
    "           Into log probabilities of size (batch_size, lang.n_words, output_n_words)\n",
    "           '''\n",
    "        batch = batch.to(self.device)                 # size = (batch_size, self.input_n_words)\n",
    "        embed = self.word2vec.embedding(batch)        # size = (batch_size, self.input_n_words, embedding_dim)\n",
    "        embed = embed.view((batch.size(0), -1))       # size = (batch_size, self.input_n_words * embedding_dim)\n",
    "        out = self.linear_1(embed)                    # size = (batch_size, self.output_n_words * hidden_dim) \n",
    "        out = out.view((batch.size(0),self.output_n_words, -1))\n",
    "        out = F.relu(out)                             # size = (batch_size, self.output_n_words, hidden_dim)                                         \n",
    "        out = self.linear_2(out)                      # size = (batch_size, self.output_n_words, lang.n_words)\n",
    "        out = torch.transpose(out, 1, 2)              # size = (batch_size, lang.n_words, self.output_n_words)\n",
    "        log_probs = F.log_softmax(out, dim = 1)       # size = (batch_size, lang.n_words, self.output_n_words)\n",
    "        return log_probs\n",
    "    \n",
    "    def generatePackedNgrams(self, corpus, context_size = 5, batch_size = 32, seed = 42) :\n",
    "        # generate Ngrams\n",
    "        data = []\n",
    "        for text in corpus :\n",
    "            text = [w for w in text if w in self.word2vec.lang.word2index]\n",
    "            text = ['SOS' for i in range(context_size)] + text + ['EOS' for i in range(context_size)]\n",
    "            for i in range(context_size, len(text) - context_size):\n",
    "                context = text[i-context_size : i] + text[i+1 : i+context_size+1]\n",
    "                word = text[i]\n",
    "                data.append([word, context])\n",
    "        # pack Ngrams into mini_batches\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            pack0 = [el[0] for el in data[i:i + batch_size]]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w)] for w in pack0]\n",
    "            pack0 = Variable(torch.LongTensor(pack0)) # size = (batch_size, 1)\n",
    "            pack1 = [el[1] for el in data[i:i + batch_size]]\n",
    "            pack1 = [[self.word2vec.lang.getIndex(w) for w in context] for context in pack1]\n",
    "            pack1 = Variable(torch.LongTensor(pack1)) # size = (batch_size, 2*context_size)   \n",
    "            if   self.sg == 1 : packed_data.append([pack0, pack1])\n",
    "            elif self.sg == 0 : packed_data.append([pack1, pack0])\n",
    "            else :\n",
    "                print('A problem occured')\n",
    "                pass\n",
    "        return packed_data\n",
    "    \n",
    "    def train(self, ngrams, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = False):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loop\n",
    "        s\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            accuracy = 0\n",
    "            for i in range(targets.size(0)) :\n",
    "                for j in range(targets.size(1)) :\n",
    "                    topv, topi = log_probs[i, :, j].data.topk(1) \n",
    "                    ni = topi[0][0]\n",
    "                    if ni == targets[i, j].data[0] : accuracy += 1\n",
    "            return (accuracy * 100) / (targets.size(0) * targets.size(1))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(couple, optimizer, compute_accuracy = False):\n",
    "            \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = self(couple[0])           # size = (batch_size, agent.output_n_words, agent.lang.n_words)\n",
    "            targets   = couple[1].to(self.device) # size = (batch_size, agent.output_n_words)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / (targets.size(0) * targets.size(1))), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_loss_words = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                couple = random.choice(ngrams)\n",
    "                loss, loss_words = trainLoop(couple, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_loss_words += loss_words      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_loss_words = printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(ngrams) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(ngrams)\n",
    "                for couple in ngrams :\n",
    "                    loss, loss_words = trainLoop(couple, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_loss_words += loss_words \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_loss_words = printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sentence_classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentence Classification\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Sentence_Classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Sentence_Classifier.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from chatNLP.modules import RecurrentEncoder, SelfAttention\n",
    "from chatNLP.utils   import heatmap, annotate_heatmap\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "#                       Sentence Classifier                         #\n",
    "#-------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 n_attn_heads = 1, \n",
    "                 n_class = 2, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.bin_mode  = (n_class == 'binary')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.attention = MultiHeadSelfAttention(self.context.output_dim, n_head = n_attn_heads, dropout = dropout)\n",
    "        self.out       = nn.Linear(self.attention.output_dim, (1 if self.bin_mode else n_class))\n",
    "        self.act       = F.sigmoid if self.bin_mode else F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        if self.bin_mode : self.criterion = nn.BCEWithLogitsLoss(size_average = False)\n",
    "        else             : self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def showAttention(self, words, attn) :\n",
    "        for i in range(attn.size(1)) :\n",
    "            fig, ax  = plt.subplots()\n",
    "            im       = heatmap(np.array(attn[:, i, :].data.cpu().numpy()),  [' '], words, ax=ax, cmap=\"YlGn\", cbarlabel=\"harvest [t/year]\")\n",
    "            texts    = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "        return\n",
    "        \n",
    "    def forward(self, sentence, show_attention = False) :\n",
    "        '''classifies a sentence as string'''\n",
    "        words         = self.tokenizer(sentence)\n",
    "        embeddings    = self.word2vec(words, self.device)\n",
    "        hiddens, _    = self.context(embeddings) \n",
    "        attended, atn = self.attention(hiddens)\n",
    "        if self.bin_mode : prediction = self.act(self.out(attended).view(-1)).data.topk(1)[0].item()\n",
    "        else             : prediction = self.act(self.out(attended.squeeze(1)), dim = 1).data.topk(1)[1].item()\n",
    "        if show_attention : self.showAttention(words, atn)\n",
    "        return prediction\n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32) :\n",
    "        sentences.sort(key = lambda s: len(self.tokenizer(s[0])), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = [self.tokenizer(s[0]) for s in sentences[i:i + batch_size]]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in words] for words in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            lengths = torch.tensor([len(p) for p in pack0])               # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))     # size = (batch_size, max_length)\n",
    "            pack1 = [[el[1]] for el in sentences[i:i + batch_size]]\n",
    "            if self.bin_mode : pack1 = Variable(torch.FloatTensor(pack1)) # size = (batch_size) \n",
    "            else             : pack1 = Variable(torch.LongTensor(pack1))  # size = (batch_size) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def compute_accuracy(self, sentences) :\n",
    "        batches = self.generatePackedSentences(sentences, batch_size = 32)\n",
    "        score = 0\n",
    "        for batch, target in batches :\n",
    "            embeddings  = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens, _  = self.context(embeddings, lengths = batch[1].to(self.device))\n",
    "            attended, _ = self.attention(hiddens)\n",
    "            if self.bin_mode : \n",
    "                vects  = self.out(attended).view(-1)\n",
    "                target = target.to(self.device).view(-1)\n",
    "                score += sum(torch.abs(target - self.act(vects)) < 0.5).item()\n",
    "            else : \n",
    "                log_probs = F.log_softmax(self.out(attended.squeeze(1)))\n",
    "                target    = target.to(self.device).view(-1)\n",
    "                score    += sum([target[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(target.size(0))])\n",
    "        return score * 100 / len(sentences)\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings  = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens, _  = self.context(embeddings, lengths = batch[1].to(self.device))\n",
    "            attended, _, penal = self.attention(hiddens, penal = True)\n",
    "            if self.bin_mode : return self.out(attended).view(-1), penal\n",
    "            else             : return F.log_softmax(self.out(attended.squeeze(1))), penal\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            if self.bin_mode : return sum(torch.abs(targets - self.act(log_probs)) < 0.5).item() * 100 / targets.size(0)\n",
    "            else             : return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "            \n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs, penal = computeLogProbs(batch[0])\n",
    "            targets = batch[1].to(self.device).view(-1)\n",
    "            loss    = self.criterion(log_probs, targets)\n",
    "            if penal is not None and penal.item() > 10 : loss = loss + penal\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"languageModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language models\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Language_Model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Language_Model.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "#                        Language model                             #\n",
    "#-------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = False)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1, :, :]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, depth_range = (2, 10)) :\n",
    "        sentences = [s[i: i+j] \\\n",
    "                     for s in sentences \\\n",
    "                     for i in range(len(s)-depth_range[0]) \\\n",
    "                     for j in range(depth_range[0], min(depth_range[1], len(s)-i)+1) \\\n",
    "                    ]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack1 = Variable(torch.LongTensor([s[-1] for s in pack0]))\n",
    "            pack0 = [s[:-1] for s in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hidden[-1, :, :]), dim = 1)   # dim = (batch_size, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets   = batch[1].to(self.device).view(-1)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chatbot\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Chatbots\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ChatbotsGeneratifsAvecMemoireAgnostique\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Chatbots génératifs à mémoire agnostique\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![Chatbot génératif à mémoire agnostique](figs/Chatbot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Chatbot.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from .Language_Model import LanguageModel\n",
    "\n",
    "\n",
    "from chatNLP.modules import (RecurrentWordsEncoder, \n",
    "                            \n",
    "                            AdditiveAttention,\n",
    "                            MultiHeadAttention,\n",
    "                            MultiHopedAttention,\n",
    "                            RecurrentHierarchicalAttention, \n",
    "                            \n",
    "                            WordsDecoder,\n",
    "                            AttnWordsDecoder,\n",
    "                            LMWordsDecoder)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "#                         Chatbot model                             #\n",
    "#-------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class Chatbot(nn.Module):\n",
    "    \"\"\"Conversationnal agent with bi-GRU Encoder, taking as parameters at training time :\n",
    "    \n",
    "            -a complete dialogue of the form (with each content as string)\n",
    "    \n",
    "                    [['question 1', 'answer 1'],\n",
    "                     ['question 2', 'answer 2'],\n",
    "                             ..........\n",
    "                     ['current question', 'current answer']]\n",
    "     \n",
    "            -the current answer for teacher forcing, or None\n",
    "    \n",
    "    and at test time :\n",
    "    \n",
    "            -the current question as string\n",
    "    \n",
    "    Returns :\n",
    "     \n",
    "            -word indices of the generated answer, according to output language of the model\n",
    "            -attention weights of first attention layer, or None is no attention\n",
    "            -attention weights of second attention layer, or None is no attention\n",
    "    \"\"\"\n",
    "    def __init__(self, device, lang, encoder, attention, decoder, autoencoder = None):\n",
    "        super(Chatbot, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.lang = lang \n",
    "        self.device = device\n",
    "        self.n_level = attention.n_level if attention is not None else 1\n",
    "        self.memory_dim = encoder.output_dim\n",
    "        self.memory_length = 0\n",
    "        # modules        \n",
    "        self.encoder = encoder\n",
    "        self.attention = attention\n",
    "        self.decoder = decoder\n",
    "        self.autoencoder = autoencoder\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ---------------------- Technical methods -----------------------------\n",
    "    def loadSubModule(self, encoder = None, attention = None, decoder = None) :\n",
    "        if encoder is not None   : self.encoder = encoder\n",
    "        if attention is not None : self.attention = attention\n",
    "        if decoder is not None   : self.decoder = decoder\n",
    "        return\n",
    "    \n",
    "    def freezeSubModule(self, encoder = False, attention = False, decoder = False) :\n",
    "        for param in self.encoder.parameters()  : param.requires_grad = not encoder\n",
    "        for param in self.attention.parameters(): param.requires_grad = not attention\n",
    "        for param in self.decoder.parameters()  : param.requires_grad = not decoder\n",
    "        return\n",
    "    \n",
    "    def nbParametres(self) :\n",
    "        count = 0\n",
    "        for p in self.parameters():\n",
    "            if p.requires_grad == True : count += p.data.nelement()\n",
    "        return count\n",
    "    \n",
    "    def flatten(self, description):\n",
    "        '''Baisse le nombre de niveaux de 1 dans la description'''\n",
    "        flatten = []\n",
    "        for line in description :\n",
    "            if type(line) == list : flatten += line  \n",
    "            else                  : flatten.append(line)\n",
    "        return [[int(word) for word in sentence.data.view(-1)] for sentence in flatten]\n",
    "\n",
    "    \n",
    "    \n",
    "    # ------------------------ Text processing methods ---------------------------------\n",
    "    def variableFromSentence(self, sentence):\n",
    "        def normalizeString(sentence) :\n",
    "            '''Remove rare symbols from a string'''\n",
    "            def unicodeToAscii(s):\n",
    "                \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "                return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "            sentence = unicodeToAscii(sentence.lower().strip())\n",
    "            sentence = re.sub(r\"[^a-zA-Z0-9?&\\%\\-\\_]+\", r\" \", sentence) \n",
    "            return sentence\n",
    "        sentence = normalizeString(sentence).split(' ') # a raw string transformed into a list of clean words\n",
    "        indexes = []\n",
    "        #unknowns = 0\n",
    "        for word in sentence:\n",
    "            if word not in self.lang.word2index.keys() :\n",
    "                if 'UNK' in self.lang.word2index.keys() : indexes.append(self.lang.word2index['UNK'])\n",
    "            else :\n",
    "                indexes.append(self.lang.word2index[word])\n",
    "        #indexes.append(self.lang.word2index['EOS']) \n",
    "        indexes.append(1) # EOS_token\n",
    "        result = Variable(torch.LongTensor([[i] for i in indexes]))\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------------ Visualisation methods ---------------------------------\n",
    "    def flattenDialogue(self, dialogue):\n",
    "        flatten = []\n",
    "        for paire in dialogue : flatten += paire\n",
    "        return [[int(word) for word in sentence.data.view(-1)] for sentence in flatten]\n",
    "    \n",
    "    def flattenWeights(self, weights) :\n",
    "        '''Baisse le nombre de niveaux de 1 dans les poids d'attention'''\n",
    "        flatten = []\n",
    "        for weight_layer in weights : flatten.append(torch.cat(tuple(weight_layer.values()), dim = 2))\n",
    "        return flatten\n",
    "    \n",
    "    def formatWeights(self, dialogue, attn1_weights, attn2_weights) :\n",
    "        if self.n_level == 2 : attn1_weights = self.flattenWeights(attn1_weights)\n",
    "        hops = self.attention.hops\n",
    "        l, L = len(dialogue), max([len(line) for line in dialogue])\n",
    "        Table = np.zeros((l, 1, L))\n",
    "        Liste = np.zeros((l, 1)) if attn2_weights is not None else None\n",
    "        count = 0\n",
    "        count_line = 0\n",
    "        for i, line in enumerate(dialogue) :\n",
    "            present = False\n",
    "            for j, word in enumerate(line) :\n",
    "                if word in self.lang.index2word.keys():\n",
    "                    present = True\n",
    "                    Table[i, 0, j] = sum([attn1_weights[k][0, 0, count].data for k in range(hops)])\n",
    "                    count += 1\n",
    "            if present and Liste is not None :\n",
    "                Liste[i] = sum([attn2_weights[k][count_line].data for k in range(hops)])\n",
    "                count_line += 1\n",
    "        return Table, Liste\n",
    "    \n",
    "    def showWeights(self, dialogue, attn1_weights, attn2_weights, maxi):\n",
    "        table, liste = self.formatWeights(dialogue[:-2], attn1_weights, attn2_weights)\n",
    "        l = table.shape[0]\n",
    "        L = table.shape[2]\n",
    "        fig = plt.figure(figsize = (l, L))\n",
    "        for i, line in enumerate(dialogue[:-2]):\n",
    "            ligne = [self.lang.index2word[int(word)] for word in line]\n",
    "            ax = fig.add_subplot(l, 1, i+1)\n",
    "            vals = table[i]\n",
    "            text = [' '] + ligne + [' ' for k in range(L-len(ligne))] if L>len(ligne) else [' '] + ligne\n",
    "            if liste is not None :\n",
    "                vals = np.concatenate((np.zeros((1, 1)) , vals), axis = 1)  \n",
    "                vals = np.concatenate((np.reshape(liste[i], (1, 1)) , vals), axis = 1)\n",
    "                turn = 'User' if i % 2 == 0 else 'Bot'\n",
    "                text = [turn] + [' '] + text\n",
    "            cax = ax.matshow(vals, vmin=0, vmax=maxi, cmap='YlOrBr')\n",
    "            ax.set_xticklabels(text, ha='left')\n",
    "            ax.set_yticklabels(' ')\n",
    "            ax.tick_params(axis=u'both', which=u'both',length=0, labelrotation = 30, labelright  = True)\n",
    "            ax.grid(b = False, which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            plt.subplots_adjust(hspace=0, wspace = 0.1)\n",
    "        plt.show()\n",
    "    \n",
    "    def showAttention(self, dialogue, n_col = 1, maxi = None):\n",
    "        answer, decoder_outputs, attn1_w, attn2_w, _ = self.answerTrain(dialogue)\n",
    "        dialogue = self.flattenDialogue(dialogue)\n",
    "        if len(dialogue) > 1 : self.showWeights(dialogue, attn1_w, attn2_w, maxi)\n",
    "        print('User : ', ' '.join([self.lang.index2word[int(word)] for word in dialogue[-2][:-1]]))\n",
    "        print('target : ', ' '.join([self.lang.index2word[int(word)] for word in dialogue[-1][:-1]]))\n",
    "        print('predic : ', ' '.join([self.lang.index2word[int(word)] for word in answer]))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------- Process methods ------------------------\n",
    "    def initMemory(self):\n",
    "        \"\"\"Initialize memory slots\"\"\"\n",
    "        self.memory = {}\n",
    "        self.memory_queries = {}\n",
    "        self.query_hidden = self.encoder.initHidden()\n",
    "        self.memory_length = 0\n",
    "        \n",
    "    def updateMemory(self, last_words, query_hidden):\n",
    "        \"\"\"Update memory with a list of word vectors 'last_words' and the last query vector 'last_query'\"\"\"\n",
    "        self.memory[self.memory_length] = last_words\n",
    "        self.memory_queries[self.memory_length] = query_hidden\n",
    "        self.query_hidden = query_hidden\n",
    "        self.memory_length += 1\n",
    "        \n",
    "    def readSentence(self, utterance):\n",
    "        \"\"\"Perform reading of an utterance, returning created word vectors\n",
    "           and last hidden states of teh encoder bi-GRU\n",
    "        \"\"\"\n",
    "        utterance = utterance.to(self.device)\n",
    "        last_words, query_hidden = self.encoder(utterance, self.query_hidden)\n",
    "        return last_words, query_hidden\n",
    "        \n",
    "    def readDialogue(self, dialogue):\n",
    "        \"\"\"Loop of readUtterance over a whole dialogue\n",
    "        \"\"\"\n",
    "        for i in range(len(dialogue)) :\n",
    "            if type(dialogue[i]) == list :\n",
    "                for utterance in dialogue[i]:\n",
    "                    last_words, query_hidden = self.readSentence(utterance)\n",
    "                    self.updateMemory(last_words, query_hidden)\n",
    "            else :\n",
    "                utterance = dialogue[i]\n",
    "                last_words, query_hidden = self.readSentence(utterance)\n",
    "                self.updateMemory(last_words, query_hidden)\n",
    "   \n",
    "    def tracking(self, query_vector = None):\n",
    "        \"\"\"Détermine un vecteur d'attention sur les éléments du registre de l'agent,\n",
    "        sachant un vecteur 'very_last_hidden', et l'accole à ce vecteur \"\"\"\n",
    "        decision_vector, attn1_weights, attn2_weights = self.attention(words_memory = self.memory, \n",
    "                                                                       query = query_vector)\n",
    "        return decision_vector, attn1_weights, attn2_weights\n",
    "\n",
    "    def generateAnswer(self,last_words, query_vector, decision_vector, target_answer = None) :\n",
    "        \"\"\"Génère une réponse à partir d'un état caché initialisant le décodeur,\n",
    "        en utilisant une réponse cible pour un mode 'teacher forcing-like' si celle-ci est fournie \"\"\"\n",
    "        answer, decoder_outputs = self.decoder(last_words, query_vector, decision_vector, target_answer)\n",
    "        return answer, decoder_outputs\n",
    "    \n",
    "    def generateQuery(self,last_words, query_vector, decision_vector, target_answer = None) :\n",
    "        \"\"\"Génère une réponse à partir d'un état caché initialisant le décodeur,\n",
    "        en utilisant une réponse cible pour un mode 'teacher forcing-like' si celle-ci est fournie \"\"\"\n",
    "        if self.autoencoder is not None : \n",
    "            query, autoencoder_outputs = self.autoencoder(last_words, query_vector, decision_vector, target_answer)\n",
    "            return query, autoencoder_outputs\n",
    "        else :\n",
    "            return None, None\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ------------ 1st working mode : training mode ------------\n",
    "    def answerTrain(self, input, target_answer = None):\n",
    "        \"\"\"Parameters are a complete dialogue, containing the current query,\n",
    "           \n",
    "           - either of the form :\n",
    "\n",
    "                    [['query 1', 'answer 1'],\n",
    "                     ['query 2', 'answer 2'],\n",
    "                             ..........\n",
    "                     ['current query', 'current answer']]\n",
    "                     \n",
    "           - or :\n",
    "           \n",
    "                    ['query 1',\n",
    "                     'query 2',\n",
    "                       ....\n",
    "                     'current query'] \n",
    "\n",
    "           The model learns to generate the current answer. \n",
    "           Teacher forcing can be enabled by passing the ground answer though the 'target_answer' option.\n",
    "        \"\"\"\n",
    "        # 1) initiates memory instance\n",
    "        self.initMemory()\n",
    "        \n",
    "        # 2) reads historical part of dialogue (if applicable),\n",
    "        # word vectors and last hidden states of encoder bi-GRU are stored in memory\n",
    "        dialogue = input[:-1]\n",
    "        self.readDialogue(dialogue)\n",
    "        \n",
    "        # 3) reads current utterance,\n",
    "        # returns word vectors of query and query vector\n",
    "        query = input[-1]\n",
    "        query = query[0] if type(query) == list else query\n",
    "        last_words, query_hidden = self.readSentence(query)\n",
    "        query_hidden = query_hidden.view(1, 1, -1)\n",
    "        \n",
    "        # 4) performs tracking\n",
    "        # returns decision vector\n",
    "        if self.attention is not None :\n",
    "            decision_vector, attn1_weights, attn2_weights = self.tracking(query_hidden)\n",
    "        else :\n",
    "            decision_vector = query_hidden\n",
    "            attn1_attention_weights = None\n",
    "            attn2_attention_weights = None\n",
    "            \n",
    "        # 5) response generation\n",
    "        # returns list of indices\n",
    "        answer, decoder_outputs = self.generateAnswer(last_words, query_hidden, decision_vector, target_answer)\n",
    "        pred_query, autoencoder_outputs = self.generateQuery(last_words, query_hidden, decision_vector, target_answer)    \n",
    "        # 6) returns answer\n",
    "        return answer, decoder_outputs, attn1_weights, attn2_weights, autoencoder_outputs\n",
    "\n",
    "        \n",
    "        \n",
    "    # ------------ 2nd working mode : test mode ------------\n",
    "    def forward(self, input):\n",
    "        \"\"\"Parameters are a single current query as string, and the model attempts to generate the current answer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1) initiates memory and hidden states of encoder bi-GRU if conversation starts\n",
    "        if self.memory_length == 0 : self.initMemory()\n",
    "            \n",
    "        # 2) reads current utterance,\n",
    "        # returns word vectors of query and query vector\n",
    "        sentence = self.variableFromSentence(input)\n",
    "        if sentence is None :\n",
    "            return \"Excusez-moi je n'ai pas compris\", None, None\n",
    "        else :\n",
    "            last_words, query_hidden = self.readSentence(sentence)\n",
    "            q_hidden = query_hidden.view(1, 1, -1)\n",
    "\n",
    "            # 3) performs tracking\n",
    "            # returns decision vector\n",
    "            if self.attention is not None :\n",
    "                decision_vector, attn1_weights, attn2_weights = self.tracking(q_hidden)\n",
    "            else :\n",
    "                decision_vector = q_hidden\n",
    "                attn1_attention_weights = None\n",
    "                attn2_attention_weights = None\n",
    "\n",
    "            # 4) response generation\n",
    "            # returns list of indices\n",
    "            answer, decoder_outputs = self.generateAnswer(last_words, q_hidden, decision_vector)\n",
    "            \n",
    "            # 5) updates memory with current query and answer\n",
    "            self.updateMemory(last_words, query_hidden)\n",
    "            answer_var = Variable(torch.LongTensor([[i] for i in answer]))\n",
    "            last_words, query_hidden = self.readSentence(answer_var)\n",
    "            self.updateMemory(last_words, query_hidden)\n",
    "\n",
    "            # 6) returns answer\n",
    "            answer = ' '.join([self.lang.index2word[int(word)] for word in answer])\n",
    "            return answer, attn1_weights, attn2_weights\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#-------------------------------------------------------------------#\n",
    "#                         Instanciator                              #\n",
    "#-------------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    \n",
    "def CreateBot(lang,                     ###\n",
    "              embedding,                  # --- Encoder options\n",
    "              hidden_dim,                 #\n",
    "              n_layers,                 ###\n",
    "\n",
    "              sentence_hidden_dim,      ###\n",
    "              hops,                       #\n",
    "              share,                      # --- Hierarchical encoder options\n",
    "              transf,                     #\n",
    "              dropout,                  ###\n",
    "              \n",
    "              attn_decoder_n_layers,    ###\n",
    "              language_model_n_layers,    #\n",
    "              tf_ratio,                   # --- decoder options\n",
    "              bound,                      #\n",
    "              autoencoding,             ###\n",
    "              \n",
    "              device\n",
    "             ):\n",
    "    '''Create an agent with specified dimensions and specificities'''\n",
    "    # 1) ----- encoding -----\n",
    "    EOS_token = lang.word2index['EOS'] if 'EOS' in lang.word2index.keys() else 1\n",
    "    if type(embedding) == torch.nn.modules.sparse.Embedding : \n",
    "        for param in embedding.parameters() : param.requires_grad = False\n",
    "    elif type(embedding) == int : \n",
    "        embedding = nn.Embedding(lang.n_words, embedding) \n",
    "    else : \n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding), freeze=True)\n",
    "\n",
    "    encoder = RecurrentWordsEncoder(device, \n",
    "                                    embedding, \n",
    "                                    hidden_dim, \n",
    "                                    n_layers, \n",
    "                                    dropout) # embedding, hidden_dim, n_layers = 1, dropout = 0\n",
    "    # 2) ----- attention -----\n",
    "    word_hidden_dim = encoder.output_dim\n",
    "    attention = RecurrentHierarchicalAttention(device,\n",
    "                                               word_hidden_dim,\n",
    "                                               sentence_hidden_dim, \n",
    "                                               query_dim = word_hidden_dim,\n",
    "                                               n_heads = 1,\n",
    "                                               n_layers = n_layers,\n",
    "                                               hops = hops,\n",
    "                                               share = share,\n",
    "                                               transf = transf,\n",
    "                                               dropout = dropout)\n",
    "    # 3) ----- decoding -----\n",
    "    tracking_dim = attention.output_dim\n",
    "    autoencoder = None\n",
    "    if language_model_n_layers > 0 :\n",
    "        language_model = LanguageModel(device, \n",
    "                                       lang,\n",
    "                                       embedding = embedding, \n",
    "                                       hidden_dim = hidden_dim,\n",
    "                                       n_layers = language_model_n_layers, \n",
    "                                       dropout = dropout)\n",
    "        decoder = LMWordsDecoder(device,\n",
    "                                 language_model,                                   \n",
    "                                 word_hidden_dim,\n",
    "                                 tracking_dim,\n",
    "                                 dropout = dropout,\n",
    "                                 tf_ratio = tf_ratio,\n",
    "                                 bound = bound)\n",
    "        if autoencoding :\n",
    "            autoencoder = LMWordsDecoder(device,\n",
    "                                         language_model,                                   \n",
    "                                         word_hidden_dim,\n",
    "                                         tracking_dim,\n",
    "                                         dropout = dropout,\n",
    "                                         tf_ratio = tf_ratio,\n",
    "                                         bound = bound)\n",
    "        \n",
    "    elif attn_decoder_n_layers >= 0 :\n",
    "        decoder = AttnWordsDecoder(device,\n",
    "                                   embedding,\n",
    "                                   word_hidden_dim,\n",
    "                                   tracking_dim,\n",
    "                                   dropout = dropout,\n",
    "                                   n_layers = attn_decoder_n_layers,\n",
    "                                   tf_ratio = tf_ratio,\n",
    "                                   bound = bound)\n",
    "        if autoencoding :\n",
    "            autoencoder = AttnWordsDecoder(device,\n",
    "                                           embedding,\n",
    "                                           word_hidden_dim,\n",
    "                                           tracking_dim,\n",
    "                                           dropout = dropout,\n",
    "                                           n_layers = attn_decoder_n_layers,\n",
    "                                           tf_ratio = tf_ratio,\n",
    "                                           bound = bound)\n",
    "    else :\n",
    "        decoder = WordsDecoder(device,\n",
    "                               embedding,                                   \n",
    "                               word_hidden_dim,\n",
    "                               tracking_dim,\n",
    "                               dropout = dropout,\n",
    "                               tf_ratio = tf_ratio,\n",
    "                               EOS_token = EOS_token,\n",
    "                               bound = bound)       \n",
    "        if autoencoding :\n",
    "            autoencoder = WordsDecoder(device,\n",
    "                                       embedding,                                   \n",
    "                                       word_hidden_dim,\n",
    "                                       tracking_dim,\n",
    "                                       dropout = dropout,\n",
    "                                       tf_ratio = tf_ratio,\n",
    "                                       EOS_token = EOS_token,\n",
    "                                       bound = bound) \n",
    "    # 4) ----- model -----\n",
    "    chatbot = Chatbot(device, lang, encoder, attention, decoder, autoencoder)\n",
    "    chatbot = chatbot.to(device)\n",
    "    return chatbot\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------#\n",
    "#                             Trainer                               #\n",
    "#-------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "class BotTrainer(object):\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 criterion = nn.NLLLoss(), \n",
    "                 optimizer = optim.SGD, \n",
    "                 clipping = 10, \n",
    "                 print_every=100):\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.clip = clipping\n",
    "        self.print_every = print_every# timer\n",
    "        \n",
    "        \n",
    "    def asMinutes(self, s):\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "    def timeSince(self, since, percent):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        es = s / (percent)\n",
    "        rs = es - s\n",
    "        return '%s (- %s)' % (self.asMinutes(s), self.asMinutes(rs))\n",
    "        \n",
    "        \n",
    "    def distance(self, agent_outputs, target_answer) :\n",
    "        \"\"\" Compute cumulated error between predicted output and ground answer.\"\"\"\n",
    "        loss = 0\n",
    "        loss_diff_mots = 0\n",
    "        agent_outputs_length = len(agent_outputs)\n",
    "        target_length = len(target_answer)\n",
    "        Max = max(agent_outputs_length, target_length)\n",
    "        Min = min(agent_outputs_length, target_length)   \n",
    "        for i in range(Min):\n",
    "            agent_output = agent_outputs[i]\n",
    "            target_word = target_answer[i]\n",
    "            #print(agent_output.size(), target_answer.size())\n",
    "            factor = (1 + Max - Min) if i == Min -1 else 1\n",
    "            loss += factor * self.criterion(agent_output, target_word)\n",
    "            topv, topi = agent_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            if ni != target_word.data[0]:\n",
    "                loss_diff_mots += 1\n",
    "        loss_diff_mots += Max - Min\n",
    "        return loss, loss_diff_mots\n",
    "        \n",
    "        \n",
    "    def trainLoop(self, agent, dialogue, target_answer, optimizer):\n",
    "        \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        query = dialogue[-1][0].to(self.device)\n",
    "        target_answer = target_answer.to(self.device)\n",
    "        answer, agent_outputs, attn1_w, attn2_w, _ = agent.answerTrain(dialogue, target_answer) \n",
    "        loss, loss_diff_mots = self.distance(agent_outputs, target_answer)\n",
    "        loss.backward()\n",
    "        _ = torch.nn.utils.clip_grad_norm_(agent.parameters(), self.clip)\n",
    "        optimizer.step()\n",
    "        return loss.data[0] / len(target_answer), loss_diff_mots\n",
    "        \n",
    "        \n",
    "    def train(self, agent, dialogues, n_iters = 10000, learning_rate=0.01, dic = None, per_dialogue = False, return_errors = False):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops.\"\"\"\n",
    "        if type(dialogues[0][0]) == list :\n",
    "            debut = 0\n",
    "            double = True\n",
    "        else :\n",
    "            debut = 1\n",
    "            double = False\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in agent.parameters() if param.requires_grad == True], lr=learning_rate)\n",
    "        print_loss_total = 0  \n",
    "        print_loss_diff_mots_total = 0\n",
    "        if return_errors : errors = {}\n",
    "        iter = 1\n",
    "        while iter < n_iters :\n",
    "            if dic is not None :\n",
    "                j = int(random.choice(list(dic.keys())))\n",
    "                training_dialogue = dialogues[j]\n",
    "                i = random.choice(dic[j])\n",
    "                partie_dialogue = training_dialogue[:i+1-debut]\n",
    "                target_answer = training_dialogue[i][1] if double else training_dialogue[i]\n",
    "                loss, loss_diff_mots = self.trainLoop(agent, partie_dialogue, target_answer, optimizer)\n",
    "                if return_errors and loss_diff_mots > 0 :\n",
    "                    if j not in list(errors.keys()) : errors[j] = []\n",
    "                    if i not in errors[j] : errors[j].append(i)\n",
    "                # quantité d'erreurs sur la réponse i\n",
    "                print_loss_total += loss\n",
    "                print_loss_diff_mots_total += loss_diff_mots \n",
    "                iter += 1\n",
    "                if iter % self.print_every == 0:\n",
    "                    print_loss_avg = print_loss_total / self.print_every\n",
    "                    print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                    print_loss_total = 0\n",
    "                    print_loss_diff_mots_total = 0\n",
    "                    print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, iter / n_iters), iter, iter / n_iters * 100, \n",
    "                                                                  print_loss_avg, print_loss_diff_mots_avg))\n",
    "            elif per_dialogue :\n",
    "                j = int(random.choice(range(len(dialogues))))\n",
    "                training_dialogue = dialogues[j]\n",
    "                indices = list(range(debut, len(training_dialogue)))\n",
    "                random.shuffle(indices)\n",
    "                for i in indices :\n",
    "                    partie_dialogue = training_dialogue[:i+1]\n",
    "                    target_answer = training_dialogue[i][1] if double else training_dialogue[i]\n",
    "                    loss, loss_diff_mots = self.trainLoop(agent, partie_dialogue, target_answer, optimizer)\n",
    "                    if return_errors and loss_diff_mots > 0 :\n",
    "                        if j not in list(errors.keys()) : errors[j] = []\n",
    "                        if i not in errors[j] : errors[j].append(i)\n",
    "                    # quantité d'erreurs sur la réponse i\n",
    "                    print_loss_total += loss\n",
    "                    print_loss_diff_mots_total += loss_diff_mots\n",
    "                    iter += 1\n",
    "                    if iter % self.print_every == 0:\n",
    "                        print_loss_avg = print_loss_total / self.print_every\n",
    "                        print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                        print_loss_total = 0\n",
    "                        print_loss_diff_mots_total = 0\n",
    "                        print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, iter / n_iters), iter, iter / n_iters * 100, \n",
    "                                                                      print_loss_avg, print_loss_diff_mots_avg))\n",
    "            else :\n",
    "                j = int(random.choice(range(len(dialogues))))\n",
    "                training_dialogue = dialogues[j]\n",
    "                i = random.choice(range(debut, len(training_dialogue)))\n",
    "                partie_dialogue = training_dialogue[:i+1]\n",
    "                target_answer = training_dialogue[i][1] if double else training_dialogue[i]\n",
    "                loss, loss_diff_mots = self.trainLoop(agent, partie_dialogue, target_answer, optimizer)\n",
    "                if return_errors and loss_diff_mots > 0 :\n",
    "                    if j not in list(errors.keys()) : errors[j] = []\n",
    "                    if i not in errors[j] : errors[j].append(i)\n",
    "                # quantité d'erreurs sur la réponse i\n",
    "                print_loss_total += loss\n",
    "                print_loss_diff_mots_total += loss_diff_mots\n",
    "                iter += 1\n",
    "                if iter % self.print_every == 0:\n",
    "                    print_loss_avg = print_loss_total / self.print_every\n",
    "                    print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                    print_loss_total = 0\n",
    "                    print_loss_diff_mots_total = 0\n",
    "                    print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, iter / n_iters), iter, iter / n_iters * 100, \n",
    "                                                                  print_loss_avg, print_loss_diff_mots_avg))\n",
    "\n",
    "\n",
    "        if return_errors : return errors\n",
    "                \n",
    "                \n",
    "    def ErrorCount(self, agent, dialogues):\n",
    "        bound = 10\n",
    "        ERRORS = [0 for i in range(bound +1)]\n",
    "        repartitionError = {}\n",
    "        for i in range(bound +1) :\n",
    "            repartitionError[i] = []\n",
    "        liste = []\n",
    "        for k, input_dialogue in enumerate(dialogues):\n",
    "            for l in range(len(input_dialogue)):\n",
    "                if len(input_dialogue[l][1])>0 :\n",
    "                    dialogue = input_dialogue[:l+1]\n",
    "                    #target_answer = variableFromSentence(agent.output_lang, input_dialogue[l][1])\n",
    "                    target_answer = input_dialogue[l][1]\n",
    "                    target_answer = target_answer.to(self.device)\n",
    "                    answer, agent_outputs, attn1_w, attn2_w, _ = agent.answerTrain(dialogue)\n",
    "                    loss, loss_diff_mots = self.distance(agent_outputs, target_answer)\n",
    "                    if loss_diff_mots > bound :\n",
    "                        ERRORS = ERRORS + [0 for i in range(loss_diff_mots - bound)]\n",
    "                        for i in range(bound +1, loss_diff_mots +1) :\n",
    "                            repartitionError[i] = []\n",
    "                        bound  = loss_diff_mots\n",
    "                    ERRORS[loss_diff_mots] += 1\n",
    "                    if loss_diff_mots > 0 :\n",
    "                        liste.append([k, l, loss_diff_mots])\n",
    "        for triple in liste:\n",
    "            repartitionError[triple[2]].append(triple[:2])\n",
    "        print(\"The repartition of errors :\", ERRORS)\n",
    "        return repartitionError\n",
    "\n",
    "\n",
    "    def DialoguesWithErrors(self, agent, dialogues) :\n",
    "        '''Returns a dictionnary, with indices of dialogues and index of line in dialogue\n",
    "           where a mistake was made.\n",
    "        '''\n",
    "        start = time.time()\n",
    "        Sortie = {}\n",
    "        L = len(dialogues)\n",
    "        for i, dialogue in enumerate(dialogues) :\n",
    "            errs = []\n",
    "            for j in range(len(dialogue)) :\n",
    "                target_answer = dialogue[j][1]\n",
    "                target_answer = target_answer.to(self.device)\n",
    "                answer, agent_outputs, attn1_w, attn2_w, _ = agent.answerTrain(dialogue[:j+1], target_answer)\n",
    "                loss, loss_diff_mots = self.distance(agent_outputs, target_answer)\n",
    "                if loss_diff_mots > 0 :\n",
    "                    errs.append(j)\n",
    "            if errs != []:\n",
    "                Sortie[i] = errs\n",
    "            if (i+1) % self.print_every == 0:\n",
    "                print('%s (%d %d%%)' % (self.timeSince(start, (i+1) / L),\n",
    "                                             (i+1), (i+1) / L * 100))\n",
    "        return Sortie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Modules\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier modules existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/__init__.py\n",
    "\n",
    "from .Encoder_Words_Recurrent import RecurrentEncoder, RecurrentWordsEncoder\n",
    "\n",
    "from .Attention_Additive import SelfAttention, AdditiveAttention\n",
    "from .Attention_MultiHead import MultiHeadSelfAttention, MultiHeadAttention\n",
    "from .Attention_MultiHoped import MultiHopedAttention\n",
    "from .Attention_Hierarchical_Recurrent import RecurrentHierarchicalAttention\n",
    "\n",
    "from .Decoder_Classes import ClassDecoder\n",
    "from .Decoder_Words import WordsDecoder\n",
    "from .Decoder_Words_Attn import AttnWordsDecoder\n",
    "from .Decoder_Words_LM import LMWordsDecoder\n",
    "\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'RecurrentEncoder',\n",
    "    'RecurrentWordsEncoder',\n",
    "    \n",
    "    'SelfAttention',\n",
    "    'AdditiveAttention',\n",
    "    'MultiHeadSelfAttention',\n",
    "    'MultiHeadAttention',\n",
    "    'MultiHopedAttention',\n",
    "    'RecurrentHierarchicalAttention',\n",
    "    \n",
    "    'ClassDecoder',\n",
    "    'WordsDecoder',\n",
    "    'AttnWordsDecoder',\n",
    "    'LMWordsDecoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encodeursDeMots\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encodeurs de mots\n",
    "\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.1 Encodeur récurrent\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module **RecurrentWordsEncoder** encode une séquence de mots $w_1, ..., w_T$ en une séquence de vecteurs $h_1, ..., h_T$ en appliquant un plongement suivi d'une couche GRU bi-directionnelle. On peut représenter son fonctionnement par la figure suivante :\n",
    "\n",
    "\n",
    "![WordEncoder](figs/WordEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Encoder_Words_Recurrent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Encoder_Words_Recurrent.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class RecurrentEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers = 1, dropout = 0, bidirectional = False): \n",
    "        super(RecurrentEncoder, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        # layers\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.bigru = nn.GRU(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout = (0 if n_layers == 1 else dropout), \n",
    "                            bidirectional = bidirectional,\n",
    "                            batch_first = True)\n",
    "\n",
    "    def forward(self, embeddings, lengths = None, hidden = None) :\n",
    "        '''Transforms a batch of size (batch_size, input_length, embedding_dim) into \n",
    "        \n",
    "              - outputs of size (batch_size, input_length, 2 * embedding_dim)\n",
    "              - hidden  of size (batch_size, 2 * n_layers, embedding_dim)\n",
    "        '''\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        if lengths is not None : embeddings = torch.nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first = True)\n",
    "        outputs, hidden = self.bigru(embeddings, hidden) # dim = (batch_size, input_length, output_dim)\n",
    "        if lengths is not None : outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first = True)\n",
    "        outputs = self.dropout(outputs)                  # dim = (batch_size, input_length, output_dim)\n",
    "        hidden  = self.dropout(hidden)                   # dim = (batch_size, 2, hidden_dim)\n",
    "        return outputs, hidden\n",
    "\n",
    "    \n",
    "# -- OLD --\n",
    "class RecurrentWordsEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 embedding, \n",
    "                 hidden_dim, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0\n",
    "                ): \n",
    "        super(RecurrentWordsEncoder, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim           # dimension of hidden state of GRUs \n",
    "        self.dropout_p = dropout\n",
    "        self.n_layers = n_layers               # number of stacked GRU layers\n",
    "        self.output_dim = hidden_dim * 2       # dimension of outputed rep. of words and utterance\n",
    "        \n",
    "        # parameters\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            embedding_dim = p.data.size(1)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.bigru = nn.GRU(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), \n",
    "                            bidirectional=True)\n",
    "\n",
    "        \n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(2 * self.n_layers, 1, self.hidden_dim)).to(self.device)\n",
    "\n",
    "    def forward(self, utterance, hidden = None):\n",
    "        if hidden is None : hidden = self.initHidden()\n",
    "        embeddings = self.embedding(utterance)                          # dim = (input_length, 1, embedding_dim)\n",
    "        embeddings = self.dropout(embeddings)                           # dim = (input_length, 1, embedding_dim)\n",
    "        outputs, hidden = self.bigru(embeddings, hidden)\n",
    "        outputs = self.dropout(outputs)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return outputs, hidden                                          # dim = (input_length, 1, hidden_dim * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionSimple\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Modules d'attention simple\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.1 Attention additive\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![AttentionAdditive](figs/Attention_Additive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_Additive.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_Additive.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout = 0): \n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = embedding_dim\n",
    "        \n",
    "        # parameters\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.attn_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.attn_v = nn.Linear(embedding_dim, 1, bias = False)\n",
    "        self.act = F.softmax\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        weights = self.attn_layer(embeddings).tanh()       # size (minibatch_size, input_length, embedding_dim)\n",
    "        weights = self.act(self.attn_v(weights), dim = 1)  # size (minibatch_size, input_length, 1)\n",
    "        weights = torch.transpose(weights, 1, 2)           # size (minibatch_size, 1, input_length)\n",
    "        attn_applied = torch.bmm(weights, embeddings)      # size (minibatch_size, 1, embedding_dim)\n",
    "        attn_applied = self.dropout(attn_applied)\n",
    "        return attn_applied, weights\n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 query_dim, \n",
    "                 targets_dim, \n",
    "                 n_layers = 1\n",
    "                ): \n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.n_level = 1\n",
    "        self.query_dim = query_dim\n",
    "        self.targets_dim = targets_dim\n",
    "        self.output_dim = targets_dim\n",
    "        self.n_layers = n_layers\n",
    "        # parameters\n",
    "        self.attn_layer = nn.Linear(query_dim + targets_dim, targets_dim) if n_layers >= 1 else None\n",
    "        self.attn_layer2 = nn.Linear(targets_dim, targets_dim) if n_layers >= 2 else None\n",
    "        self.attn_v = nn.Linear(targets_dim, 1, bias = False) if n_layers >= 1 else None\n",
    "        self.act = F.softmax\n",
    "        \n",
    "    def forward(self, query = None, targets = None):\n",
    "        '''takes as parameters : \n",
    "                a query tensor conditionning the attention,     size = (1, minibatch_size, query_dim)\n",
    "                a tensor containing attention targets           size = (targets_length, minibatch_size, targets_dim)\n",
    "           returns : \n",
    "                the resulting tensor of the attention process,  size = (1, minibatch_size, targets_dim)\n",
    "                the attention weights,                          size = (1, targets_length)\n",
    "        '''\n",
    "        if targets is not None :\n",
    "            # concat method \n",
    "            if self.n_layers >= 1 :\n",
    "                poids = torch.cat((query.expand(targets.size(0), -1, -1), targets), 2) if query is not None else targets\n",
    "                poids = self.attn_layer(poids).tanh()                 # size (targets_length, minibatch_size, targets_dim)\n",
    "                if self.n_layers >= 2 :\n",
    "                    poids = self.attn_layer2(poids).tanh()            # size (targets_length, minibatch_size, targets_dim)\n",
    "                attn_weights = self.attn_v(poids)                     # size (targets_length, minibatch_size, 1)\n",
    "                attn_weights = torch.transpose(attn_weights, 0,1)     # size (minibatch_size, targets_length, 1)\n",
    "                targets = torch.transpose(targets, 0,1)               # size (minibatch_size, targets_length, targets_dim)\n",
    "            # dot method\n",
    "            else :\n",
    "                targets = torch.transpose(targets, 0,1)               # size (minibatch_size, targets_length, targets_dim)\n",
    "                query = torch.transpose(query, 0, 1)                  # size (minibatch_size, 1, query_dim)\n",
    "                query = torch.transpose(query, 1, 2)                  # size (minibatch_size, query_dim, 1)\n",
    "                attn_weights = torch.bmm(targets, query)              # size (minibatch_size, targets_length, 1)\n",
    "                \n",
    "            attn_weights = self.act(attn_weights, dim = 1)        # size (minibatch_size, targets_length, 1)\n",
    "            attn_weights = torch.transpose(attn_weights, 1,2)     # size (minibatch_size, 1, targets_length)\n",
    "            attn_applied = torch.bmm(attn_weights, targets)       # size (minibatch_size, 1, targets_dim)\n",
    "            attn_applied = torch.transpose(attn_applied, 0,1)     # size (1, minibatch_size, targets_dim)\n",
    "\n",
    "        else :\n",
    "            attn_applied = query\n",
    "            attn_weights = None\n",
    "        return attn_applied, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"attentionAdditiveMultitete\"></a>\n",
    "\n",
    "\n",
    "### 2.2.2 Attention additive multi-tête\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_MultiHead.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_MultiHead.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from . import SelfAttention, AdditiveAttention\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_head = 1, dropout = 0): \n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = n_head * embedding_dim\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        # parameters\n",
    "        self.attn_list = nn.ModuleList([SelfAttention(embedding_dim, dropout) for i in range(n_head)])\n",
    "        \n",
    "    def compute_penalty(self, weights) :\n",
    "        weights_t = torch.transpose(weights, 1, 2)\n",
    "        def_pos = [torch.mm(weights[i], weights_t[i]) for i in range(weights.size(0))] # size (minibatch_size, n_heads, n_heads)\n",
    "        ide = Variable(torch.eye(self.n_head)).to(device)\n",
    "        penal = torch.sum(torch.cat([torch.norm(mmt - ide).view(1) for mmt in def_pos]))\n",
    "        return penal\n",
    "    \n",
    "    def forward(self, embeddings, penal = False):\n",
    "        outputs = [attn(embeddings) for attn in self.attn_list]\n",
    "        applied = torch.cat([out[0] for out in outputs], dim = 2) # size (minibatch_size, 1, n_heads * embedding_dim)\n",
    "        weights = torch.cat([out[1] for out in outputs], dim = 1) # size (minibatch_size, n_heads, input_length)\n",
    "        if penal and self.n_head > 1 :\n",
    "            penal = self.compute_penalty(weights)\n",
    "            return applied, weights, penal\n",
    "        elif penal : \n",
    "            return applied, weights, None\n",
    "        else :\n",
    "            return applied, weights\n",
    "\n",
    "\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Module performing additive attention over a sequence of vectors stored in\n",
    "       a memory block, conditionned by some vector. At instanciation it takes as imput :\n",
    "       \n",
    "                - query_dim : the dimension of the conditionning vector\n",
    "                - targets_dim : the dimension of vectors stored in memory\n",
    "                \n",
    "      Other ideas on Multi head attention on \n",
    "      https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py\n",
    "      https://github.com/tlatkowski/multihead-siamese-nets/blob/master/layers/attention.py\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 n_heads, \n",
    "                 query_dim, \n",
    "                 targets_dim, \n",
    "                 n_layers = 2\n",
    "                ): \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.n_level = 1\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # parameters\n",
    "        self.attn_modules_list = nn.ModuleList([AdditiveAttention(query_dim, targets_dim, n_layers) for i in range(n_heads)])\n",
    "\n",
    "    def forward(self, query = None, targets = None):\n",
    "        '''takes as parameters : \n",
    "                a query tensor conditionning the attention,     size = (1, n_heads, query_dim)\n",
    "                a tensor containing attention targets           size = (targets_length, n_heads, targets_dim)\n",
    "           returns : \n",
    "                the resulting tensor of the attention process,  size = (1, n_heads, targets_dim)\n",
    "                the attention weights,                          size = (n_heads, 1, targets_length)\n",
    "        '''\n",
    "        print(\"multihead attention\")\n",
    "        targets_length = targets.size(0)\n",
    "        targets_dim    = targets.size(2)\n",
    "        attn_applied   = Variable(torch.zeros(1, self.n_heads, targets_dim)).to(self.device)\n",
    "        attn_weights   = torch.zeros(self.n_heads, 1, targets_length).to(self.device)\n",
    "        for i, attn in enumerate(self.attn_modules_list) :\n",
    "            que = query[:, i, :] if query is not None else None\n",
    "            print(que.size())\n",
    "            tar = targets[:, i, :].unsqueeze(1)\n",
    "            print(tar.size())\n",
    "            attn_appl, attn_wghts = attn(que, tar)\n",
    "            print(attn_appl.size())\n",
    "            print(attn_wghts.size())\n",
    "            attn_applied[:, i, :] = attn_appl.squeeze(1)\n",
    "            attn_weights[i, :, :] = attn_wghts.squeeze(0)\n",
    "        return attn_applied, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionAdditiveMultihoped\"></a>\n",
    "\n",
    "\n",
    "### 2.2.3 Attention additive multi-hopée\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_MultiHoped.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_MultiHoped.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class MultiHopedAttention(nn.Module):\n",
    "    '''Module performing additive attention over a sequence of vectors stored in\n",
    "       a memory block, conditionned by some vector. At instanciation it takes as imput :\n",
    "       \n",
    "                - query_dim : the dimension of the conditionning vector\n",
    "                - targets_dim : the dimension of vectors stored in memory\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 targets_dim,\n",
    "                 base_query_dim = 0,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(MultiHopedAttention, self).__init__()\n",
    "\n",
    "        # dimensions\n",
    "        self.targets_dim = targets_dim\n",
    "        self.output_dim = targets_dim\n",
    "        self.hops_query_dim = self.output_dim if hops > 1 else 0\n",
    "        self.query_dim = base_query_dim + self.hops_query_dim\n",
    "        \n",
    "        # structural coefficients\n",
    "        self.device = device\n",
    "        self.n_level = 1\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        self.transf = transf\n",
    "        self.dropout_p = dropout\n",
    "        if dropout > 0 : self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # parameters\n",
    "        self.attn = AdditiveAttention(self.query_dim, self.targets_dim) \n",
    "        self.transf = nn.GRU(self.targets_dim, self.targets_dim) if transf else None\n",
    "        \n",
    "        \n",
    "    def initQuery(self): \n",
    "        if self.hops_query_dim > 0 :\n",
    "            return Variable(torch.zeros(1, 1, self.hops_query_dim)).to(self.device)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def update(self, hops_query, decision_vector):\n",
    "        if self.transf is not None : _ , update = self.transf(decision_vector, hops_query)\n",
    "        else                       :     update = hops_query + decision_vector\n",
    "        return update\n",
    "    \n",
    "    \n",
    "    def forward(self, words_memory, base_query = None):\n",
    "        attn_weights_list = []\n",
    "        hops_query = self.initQuery() if (self.hops > 1 and self.share) else None\n",
    "        \n",
    "        for hop in range(self.hops) :\n",
    "            if base_query is not None and hops_query is not None : query = torch.cat((base_query, hops_query), 2) # size (1, self.n_heads, self.query_dim)\n",
    "            elif base_query is not None                          : query = base_query\n",
    "            elif hops_query is not None                          : query = hops_query\n",
    "            else                                                 : query = None\n",
    "            \n",
    "            decision_vector, attn_weights = self.attn(query, words_memory)\n",
    "            attn_weights_list.append(attn_weights)\n",
    "            hops_query = self.update(hops_query, decision_vector) if (self.hops > 1 and hops_query is not None) else decision_vector\n",
    "  \n",
    "        # output decision vector\n",
    "        return hops_query, attn_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionHierarchique\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Attention hiérarchique\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![HierarchicalAttention](figs/Hierarchical_Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_Hierarchical_Recurrent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_Hierarchical_Recurrent.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from . import AdditiveAttention, MultiHeadAttention\n",
    "\n",
    "\n",
    "class RecurrentHierarchicalAttention(nn.Module):\n",
    "    '''Ce module d'attention est :\n",
    "    \n",
    "    - hiérarchique avec bi-GRU entre chaque niveau d'attention\n",
    "    - multi-tête sur chaque niveau d'attention\n",
    "    - globalement multi-hopé, où il est possible d'effectuer plusieurs passes pour accumuler de l'information\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 word_hidden_dim, \n",
    "                 sentence_hidden_dim,\n",
    "                 query_dim = 0, \n",
    "                 n_heads = 1,\n",
    "                 n_layers = 1,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(RecurrentHierarchicalAttention, self).__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.query_dim = query_dim\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        self.sentence_input_dim = self.word_hidden_dim\n",
    "        self.sentence_hidden_dim = sentence_hidden_dim\n",
    "        self.context_vector_dim = sentence_hidden_dim * 2\n",
    "        self.output_dim = self.query_dim if (transf or self.hops > 0) else self.context_vector_dim\n",
    "        \n",
    "        # structural coefficients\n",
    "        self.device = device\n",
    "        self.n_level = 2\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        self.dropout_p = dropout\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # first attention module\n",
    "        attn1_list = []\n",
    "        if share :\n",
    "            attn1 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.word_hidden_dim) if n_heads > 1 else \\\n",
    "                    AdditiveAttention(self.query_dim, self.word_hidden_dim) \n",
    "            for hop in range(hops) : attn1_list.append(attn1)\n",
    "            self.attn1 = nn.ModuleList(attn1_list)\n",
    "        else :\n",
    "            for hop in range(hops):\n",
    "                attn1 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.word_hidden_dim) if n_heads > 1 else \\\n",
    "                        AdditiveAttention(self.query_dim, self.word_hidden_dim) \n",
    "                attn1_list.append(attn1)\n",
    "            self.attn1 = nn.ModuleList(attn1_list)\n",
    "        \n",
    "        # intermediate encoder module\n",
    "        self.bigru = nn.GRU(self.sentence_input_dim, \n",
    "                            self.sentence_hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        # second attention module\n",
    "        attn2_list = []\n",
    "        if share :\n",
    "            attn2 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.context_vector_dim) if n_heads > 1 else \\\n",
    "                    AdditiveAttention(self.query_dim, self.context_vector_dim) \n",
    "            for hop in range(hops) : attn2_list.append(attn2)\n",
    "            self.attn2 = nn.ModuleList(attn2_list)\n",
    "        else :\n",
    "            for hop in range(hops):\n",
    "                attn2 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.context_vector_dim) if n_heads > 1 else \\\n",
    "                        AdditiveAttention(self.query_dim, self.context_vector_dim) \n",
    "                attn2_list.append(attn2)\n",
    "            self.attn2 = nn.ModuleList(attn2_list)\n",
    "        \n",
    "        # accumulation step\n",
    "        self.transf = nn.Linear(self.context_vector_dim, self.output_dim, bias = False) \\\n",
    "                      if (transf or self.hops > 0) else None\n",
    "\n",
    "\n",
    "    def initQuery(self): \n",
    "        if self.query_dim > 0 :\n",
    "            return Variable(torch.zeros(1, self.n_heads, self.query_dim)).to(self.device)\n",
    "        return None\n",
    "        \n",
    "                \n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(2 * self.n_layers, self.n_heads, self.sentence_hidden_dim)).to(self.device)\n",
    "        \n",
    "        \n",
    "    def singlePass(self, words_memory, query, attn1, attn2): \n",
    "        L = len(words_memory)\n",
    "        attn1_weights = {}\n",
    "        bigru_inputs = Variable(torch.zeros(L, self.n_heads, self.sentence_input_dim)).to(self.device)\n",
    "        # first attention layer\n",
    "        for i in range(L) :\n",
    "            targets = words_memory[i]                              # size (N_i, 1, 2*word_hidden_dim)\n",
    "            targets = targets.repeat(1, self.n_heads, 1)           # size (N_i, n_heads, 2*word_hidden_dim)\n",
    "            attn1_output, attn1_wghts = attn1(query, targets)\n",
    "            attn1_weights[i] = attn1_wghts\n",
    "            bigru_inputs[i] = attn1_output.squeeze(0)              # size (n_heads, 2*word_hidden_dim)\n",
    "        # intermediate biGRU\n",
    "        bigru_hidden = self.initHidden()\n",
    "        attn2_inputs, bigru_hidden = self.bigru(bigru_inputs, bigru_hidden)  # size (L, n_heads, 2*word_hidden_dim)\n",
    "        # second attention layer\n",
    "        attn2_inputs = self.dropout(attn2_inputs)\n",
    "        decision_vector, attn2_weights = attn2(query = query, targets = attn2_inputs)\n",
    "        attn2_weights = attn2_weights.view(-1)\n",
    "        # output decision vector\n",
    "        return decision_vector, attn1_weights, attn2_weights\n",
    "    \n",
    "    \n",
    "    def update(self, query, decision_vector):\n",
    "        update = query + self.transf(decision_vector) if self.transf is not None else query + decision_vector\n",
    "        return update\n",
    "        \n",
    "        \n",
    "    def forward(self, words_memory, query = None):\n",
    "        '''takes as parameters : \n",
    "                a tensor containing words_memory vectors        dim = (words_memory_length, word_hidden_dim)\n",
    "                a tensor containing past queries                dim = (words_memory_length, query_dim)\n",
    "           returns : \n",
    "                the resulting decision vector                   dim = (1, 1, query_dim)\n",
    "                the weights of first attention layer (dict)     \n",
    "                the weights of second attention layer (dict)\n",
    "        '''\n",
    "        attn1_weights_list = []\n",
    "        attn2_weights_list = []\n",
    "        if len(words_memory) > 0 :\n",
    "            if query is not None : query = query.repeat(1, self.n_heads, 1)\n",
    "            elif self.hops > 1   : query = self.initQuery()\n",
    "            for hop in range(self.hops) :\n",
    "                decision_vector, attn1_weights, attn2_weights = self.singlePass(words_memory, \n",
    "                                                                                query, \n",
    "                                                                                self.attn1[hop], \n",
    "                                                                                self.attn2[hop])\n",
    "                attn1_weights_list.append(attn1_weights)\n",
    "                attn2_weights_list.append(attn2_weights)\n",
    "                query = self.update(query, decision_vector)  # size (L, self.n_heads, self.output_dim)\n",
    "\n",
    "        # output decision vector\n",
    "        return query, attn1_weights_list, attn2_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Modules de décodage\n",
    "\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeursSelectifs\"></a>\n",
    "\n",
    "\n",
    "### 2.4.1 Décodeur sélectif\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Classes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Classes.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_dim, n_classes) :\n",
    "        super(ClassDecoder, self).__init__() \n",
    "        self.version = 'class'\n",
    "        self.n_classes = n_classes\n",
    "        self.classes_decoder = nn.Linear(text_dim, n_classes)\n",
    "\n",
    "    def forward(self, text_vector, train_mode = False):\n",
    "        classes_vector = self.classes_decoder(text_vector).view(-1)\n",
    "        if train_mode :\n",
    "            return classes_vector\n",
    "        else :\n",
    "            classes = F.softmax(classes_vector) \n",
    "            topv, topi = classes.data.topk(1)\n",
    "            result = topi[0][0].numpy()\n",
    "            return result   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurGeneratif\"></a>\n",
    "\n",
    "\n",
    "### 2.4.2 Décodeur génératif\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![Decoder](figs/Decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class WordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 embedding, \n",
    "                 hidden_dim, \n",
    "                 tracking_dim, \n",
    "                 dropout = 0.1,\n",
    "                 tf_ratio = 1,\n",
    "                 EOS_token = 1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(WordsDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tracking_dim = tracking_dim\n",
    "        self.tf_ratio = tf_ratio\n",
    "        self.EOS_token = EOS_token\n",
    "        self.bound = bound\n",
    "        # modules\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            lang_size     = p.data.size(0)\n",
    "            embedding_dim = p.data.size(1)\n",
    "        self.gru = nn.GRU(embedding_dim + tracking_dim, tracking_dim)\n",
    "        self.out = nn.Linear(tracking_dim, lang_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def generateWord(self, query_vector, hidden, current_word_index):\n",
    "        # update hidden state\n",
    "        embedded = self.embedding(current_word_index).view(1, 1, -1)\n",
    "        if query_vector is not None : embedded = torch.cat((query_vector, embedded), dim = 2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # generate next word\n",
    "        vector = self.out(hidden).squeeze(0)\n",
    "        log_proba = F.log_softmax(vector, dim = 1)\n",
    "        return log_proba, hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, query_words, query_vector, decision_vector, target_answer = None) :\n",
    "        log_probas = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        ta = target_answer if random.random() < self.tf_ratio else None\n",
    "        current_word_index = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        hidden = self.dropout(decision_vector)\n",
    "        for di in range(self.bound) :\n",
    "            log_proba, hidden = self.generateWord(decision_vector, hidden, current_word_index) \n",
    "            topv, topi = log_proba.data.topk(1)\n",
    "            log_probas.append(log_proba)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            if ni == self.EOS_token : # EOS_token\n",
    "                break\n",
    "            elif ta is not None : # Teacher forcing\n",
    "                answer.append(ni)\n",
    "                if di < ta.size(0) : current_word_index = ta[di].to(self.device)\n",
    "                else               : break\n",
    "            else :\n",
    "                answer.append(ni)\n",
    "                current_word_index = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "        return answer, log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version lisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words_Smooth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words_Smooth.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class SmoothWordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 embedding, \n",
    "                 hidden_dim, \n",
    "                 tracking_dim, \n",
    "                 dropout = 0.1,\n",
    "                 tf_ratio = 1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(SmoothWordsDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tracking_dim = tracking_dim\n",
    "        self.tf_ratio = tf_ratio\n",
    "        self.bound = bound\n",
    "        for p in embedding.parameters() :\n",
    "            lang_size     = p.data.size(0)\n",
    "            embedding_dim = p.data.size(1)\n",
    "        # modules\n",
    "        self.enbedding = nn.Linear((lang_size, embedding_dim), bias = False)\n",
    "        # TODO : put embedding weights into the self.embedding layer\n",
    "        self.gru = nn.GRU(embedding_dim + tracking_dim, tracking_dim)\n",
    "        self.out = nn.Linear(tracking_dim, lang_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def generateWord(self, query_vector, hidden, embedded):\n",
    "        # update hidden state\n",
    "        if query_vector is not None : embedded = torch.cat((query_vector, embedded), dim = 2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # generate next word\n",
    "        vector = self.out(hidden).squeeze(0)\n",
    "        log_proba = F.log_softmax(vector, dim = 1)\n",
    "        return log_proba, hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, query_words, query_vector, decision_vector, target_answer = None) :\n",
    "        log_probas = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        ta = target_answer if random.random() < self.tf_ratio else None\n",
    "        current_word_index = Variable(torch.LongTensor(1, 1, self.lang_size)).to(self.device)\n",
    "        current_word_index.zero_()\n",
    "        current_word_index = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        current_embedded_word = self.embedding(current_word_index).view(1, 1, -1)\n",
    "        hidden = self.dropout(decision_vector)\n",
    "        for di in range(self.bound) :\n",
    "            log_proba, hidden = self.generateWord(decision_vector, hidden, current_word_index) \n",
    "            topv, topi = log_proba.data.topk(1)\n",
    "            log_probas.append(log_proba)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            if ni == 1 : # EOS_token\n",
    "                break\n",
    "            elif ta is not None : # Teacher forcing\n",
    "                answer.append(ni)\n",
    "                if di < ta.size(0) :\n",
    "                    current_word_index = ta[di].to(self.device)\n",
    "                else :\n",
    "                    break\n",
    "            else :\n",
    "                answer.append(ni)\n",
    "                current_word_index = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "                current_embedded_word = self.embedding(current_word_index).view(1, 1, -1)\n",
    "        return answer, log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurGeneratifAttention\"></a>\n",
    "\n",
    "\n",
    "### 2.4.3 Décodeur génératif à attention\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words_Attn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words_Attn.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class AttnWordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 embedding, \n",
    "                 hidden_dim, \n",
    "                 tracking_dim,\n",
    "                 n_layers = 0, \n",
    "                 dropout = 0.1,\n",
    "                 tf_ratio = 1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(AttnWordsDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tracking_dim = tracking_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.tf_ratio = tf_ratio\n",
    "        self.bound = bound\n",
    "        # modules\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            lang_size     = p.data.size()[0]\n",
    "            embedding_dim = p.data.size()[1]\n",
    "        self.gru = nn.GRU(embedding_dim + tracking_dim, hidden_dim)\n",
    "        self.attn = AdditiveAttention(hidden_dim, hidden_dim, n_layers) \n",
    "        self.concat = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, lang_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def generateWord(self, query_words, query_vector, hidden, current_word_index):\n",
    "        # update hidden state\n",
    "        embedded = self.embedding(current_word_index).view(1, 1, -1)\n",
    "        if query_vector is not None : embedded = torch.cat((query_vector, embedded), dim = 2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # generate next word\n",
    "        attn, attn_weights = self.attn(hidden, query_words)\n",
    "        vector = self.concat(torch.cat((hidden, attn), dim = 2)).tanh()\n",
    "        vector = self.out(vector).squeeze(0)\n",
    "        log_proba = F.log_softmax(vector, dim = 1)\n",
    "        return log_proba, hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, query_words, query_vector, decision_vector, target_answer = None) :\n",
    "        log_probas = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        ta = target_answer if random.random() < self.tf_ratio else None\n",
    "        current_word_index = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        hidden = self.dropout(decision_vector)\n",
    "        for di in range(self.bound) :\n",
    "            log_proba, hidden = self.generateWord(query_words, query_vector, hidden, current_word_index)\n",
    "            topv, topi = log_proba.data.topk(1)\n",
    "            log_probas.append(log_proba)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            if ni == 1 : # EOS_token\n",
    "                break\n",
    "            elif ta is not None : # Teacher forcing\n",
    "                answer.append(ni)\n",
    "                if di < ta.size(0) :\n",
    "                    current_word_index = ta[di].to(self.device)\n",
    "                else :\n",
    "                    break\n",
    "            else :\n",
    "                answer.append(ni)\n",
    "                current_word_index = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "        return answer, log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurGeneratifML\"></a>\n",
    "\n",
    "\n",
    "### 2.4.4 Décodeur génératif à modèle linguistique\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words_LM.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words_LM.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class LMWordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 language_model, \n",
    "                 hidden_dim, \n",
    "                 tracking_dim, \n",
    "                 dropout = 0.1,\n",
    "                 tf_ratio = 1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(LMWordsDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tracking_dim = tracking_dim\n",
    "        self.tf_ratio = tf_ratio\n",
    "        self.lm_ratio = 0.25\n",
    "        self.bound = bound\n",
    "        self.pos = 0\n",
    "        self.max_pos = 3\n",
    "        # modules\n",
    "        self.language_model = language_model.to(self.device)\n",
    "        for param in self.language_model.parameters() : param.requires_grad = False \n",
    "        self.embedding = self.language_model.embedding\n",
    "        for p in self.embedding.parameters() :\n",
    "            lang_size     = p.data.size(0)\n",
    "            embedding_dim = p.data.size(1)\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, lang_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def generateWord(self, query_vector, hidden, hidden_lm, current_word_index, current_word_index_lm):\n",
    "        # update hidden state\n",
    "        embedded = self.embedding(current_word_index).view(1, 1, -1)\n",
    "        if query_vector is not None : embedded = torch.cat((query_vector, embedded), dim = 2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # generate next word\n",
    "        vector = self.out(hidden).squeeze(0)\n",
    "        log_proba = F.log_softmax(vector, dim = 1)\n",
    "        # Language Model contribution\n",
    "        log_proba_lm, hidden_lm = self.language_model.generateWord(current_word_index_lm, hidden_lm)\n",
    "        return log_proba + (self.pos/self.max_pos) * self.lm_ratio * log_proba_lm, hidden, hidden_lm\n",
    "    \n",
    "    \n",
    "    def forward(self, query_words, query_vector, decision_vector, target_answer) :\n",
    "        log_probas = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        ta = target_answer if random.random() < self.tf_ratio else None\n",
    "        current_word_index    = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        current_word_index_lm = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        hidden    = self.dropout(decision_vector)\n",
    "        hidden_lm = None\n",
    "        for di in range(self.bound) :\n",
    "            self.pos = min(di, self.max_pos)\n",
    "            log_proba, hidden, hidden_lm = self.generateWord(query_vector, \n",
    "                                                             hidden, \n",
    "                                                             hidden_lm, \n",
    "                                                             current_word_index, \n",
    "                                                             current_word_index_lm)\n",
    "            topv, topi = log_proba.data.topk(1)\n",
    "            log_probas.append(log_proba)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            if ni == 1 : # EOS_token\n",
    "                break\n",
    "            elif ta is not None : # Teacher forcing\n",
    "                answer.append(ni)\n",
    "                if di < ta.size(0) :\n",
    "                    current_word_index    = ta[di].view(-1, 1).to(self.device)\n",
    "                    current_word_index_lm = target_answer[di].view(-1, 1).to(self.device) if di < target_answer.size(0) else \\\n",
    "                                            target_answer[-1].view(-1, 1).to(self.device)\n",
    "                else :\n",
    "                    break\n",
    "            else :\n",
    "                answer.append(ni)\n",
    "                current_word_index    = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "                if target_answer is not None and di < target_answer.size(0): \n",
    "                    current_word_index_lm = target_answer[di].view(-1, 1).to(self.device)\n",
    "                else :\n",
    "                    current_word_index_lm = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "        return answer, log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"misc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Miscellaneous\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier misc existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting misc/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile misc/__init__.py\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'NoiseFilter',\n",
    "    'NoiseFilterWrapper',\n",
    "    'NoiseFilterTrainer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FiltreAntiBruit\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M.1 Filtre Anti-bruit\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting misc/Noise_Filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile misc/Noise_Filter.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class NoiseFilter(nn.Module):\n",
    "\n",
    "    def __init__(self, chatbot, pretrained = True, layers = [50], dropout = 0.15):\n",
    "        super(NoiseFilter, self).__init__()\n",
    "        \n",
    "        # modules        \n",
    "        self.device = chatbot.device\n",
    "        self.chatbot = chatbot\n",
    "        if pretrained : \n",
    "            for param in chatbot.parameters() : param.requires_grad = False\n",
    "        self.decoder = nn.ModuleList([nn.Linear(chatbot.encoder.output_dim, layers[0])] + \n",
    "                                     [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1) if len(layers) > 1] +\n",
    "                                     [nn.Linear(layers[-1], 2)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    # ---------------------- Technical methods -----------------------------\n",
    "    def nbParametres(self) :\n",
    "        count = 0\n",
    "        for p in self.parameters():\n",
    "            if p.requires_grad == True : count += p.data.nelement()\n",
    "        return count\n",
    "        \n",
    "        \n",
    "    # ------------ 2nd working mode : test mode ------------\n",
    "    def forward(self, input):\n",
    "\n",
    "        sentence = self.chatbot.variableFromSentence(input)\n",
    "        if sentence is None :\n",
    "            return 0, None, None\n",
    "        else :\n",
    "            sentence = sentence.to(self.device)\n",
    "            last_words, hidden = self.chatbot.encoder(sentence)\n",
    "            hidden = self.dropout(hidden.view(1,1,-1))\n",
    "            for layer in self.decoder : hidden = self.dropout(F.relu(layer(hidden)))\n",
    "                \n",
    "            log_probas  = F.log_softmax(hidden.view(1, -1), dim = 1)\n",
    "            topv, topi = log_probas.data.topk(1)\n",
    "            predict = topi[0][0]\n",
    "            return predict, log_probas\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "class NoiseFilterWrapper(nn.Module) :\n",
    "    def __init__(self, noise_filter, chatbot) :\n",
    "        \n",
    "        super(NoiseFilterWrapper, self).__init__()\n",
    "        self.noise_filter = noise_filter\n",
    "        self.chatbot = chatbot\n",
    "        self.basic_answer = \"Je n'ai pas compris, merci de reformuler la question\"\n",
    "        \n",
    "    def forward(self, sentence) :\n",
    "        #print(self.noise_filter(sentence)[1].data)\n",
    "        if self.noise_filter(sentence)[0] == 1 : return self.chatbot(sentence)\n",
    "        else                                   : return self.basic_answer, None, None\n",
    "        \n",
    "        \n",
    "\n",
    "class NoiseFilterTrainer(object):\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 criterion = nn.NLLLoss(), \n",
    "                 optimizer = optim.SGD, \n",
    "                 clipping = 10, \n",
    "                 print_every=100):\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.clip = clipping\n",
    "        self.print_every = print_every# timer\n",
    "        \n",
    "        \n",
    "    def asMinutes(self, s):\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "    def timeSince(self, since, percent):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        es = s / (percent)\n",
    "        rs = es - s\n",
    "        return '%s (- %s)' % (self.asMinutes(s), self.asMinutes(rs))\n",
    "        \n",
    "        \n",
    "    def distance(self, probas, target_var) :\n",
    "        \"\"\" Compute cumulated error between predicted output and ground answer.\"\"\"\n",
    "        loss = self.criterion(probas, target_var)\n",
    "        loss_diff = int(ni != target_var.item())\n",
    "        return loss, loss_diff\n",
    "        \n",
    "        \n",
    "    def trainLoop(self, agent, sentence, target, optimizer):\n",
    "        \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        target_var = Variable(torch.LongTensor([target])).to(self.device)\n",
    "        answer, log_probas = agent(sentence) \n",
    "        loss = self.criterion(log_probas, target_var)\n",
    "        loss_diff_mots = int(answer != target_var.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        _ = torch.nn.utils.clip_grad_norm_(agent.parameters(), self.clip)\n",
    "        optimizer.step()\n",
    "        return loss.data[0] , loss_diff_mots\n",
    "        \n",
    "        \n",
    "    def train(self, agent, sentences, n_iters = 10000, learning_rate=0.01):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops.\"\"\"\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in agent.parameters() if param.requires_grad == True], lr=learning_rate)\n",
    "        print_loss_total = 0  \n",
    "        print_loss_diff_mots_total = 0\n",
    "        for iter in range(1, n_iters + 1):\n",
    "            training_sentence = random.choice(sentences)\n",
    "            sentence = training_sentence[0]\n",
    "            target   = training_sentence[1]\n",
    "\n",
    "            loss, loss_diff_mots = self.trainLoop(agent, sentence, target, optimizer)\n",
    "            # quantité d'erreurs sur la réponse i\n",
    "            print_loss_total += loss\n",
    "            print_loss_diff_mots_total += loss_diff_mots       \n",
    "            if iter % self.print_every == 0:\n",
    "                print_loss_avg = print_loss_total / self.print_every\n",
    "                print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                print_loss_total = 0\n",
    "                print_loss_diff_mots_total = 0\n",
    "                print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg, print_loss_diff_mots_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"utils\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Utils\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier utils existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/__init__.py\n",
    "\n",
    "from .Lang import Lang\n",
    "from .Attention_Weight_Visualization import heatmap, annotate_heatmap\n",
    "\n",
    "__all__ = [\n",
    "    'Lang',\n",
    "    'heatmap',\n",
    "    'annotate_heatmap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lang\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Language\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/Lang.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/Lang.py\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, corpus = None, base_tokens = ['UNK'], min_count = None):\n",
    "        self.base_tokens = base_tokens\n",
    "        self.initData(base_tokens)\n",
    "        if    corpus is not None : self.addCorpus(corpus)\n",
    "        if min_count is not None : self.removeRareWords(min_count)\n",
    "\n",
    "        \n",
    "    def initData(self, base_tokens) :\n",
    "        self.word2index = {word : i for i, word in enumerate(base_tokens)}\n",
    "        self.index2word = {i : word for i, word in enumerate(base_tokens)}\n",
    "        self.word2count = {word : 0 for word in base_tokens}\n",
    "        self.n_words = len(base_tokens)\n",
    "        return\n",
    "    \n",
    "    def getIndex(self, word) :\n",
    "        if    word in self.word2index : return self.word2index[word]\n",
    "        elif 'UNK' in self.word2index : return self.word2index['UNK']\n",
    "        return\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        '''Add a word to the language'''\n",
    "        if word not in self.word2index:\n",
    "            if word.strip() != '' :\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        return \n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        '''Add to the language all words of a sentence'''\n",
    "        words = sentence if type(sentence) == list else nltk.word_tokenize(sentence)\n",
    "        for word in words : self.addWord(word)          \n",
    "        return\n",
    "            \n",
    "    def addCorpus(self, corpus):\n",
    "        '''Add to the language all words contained into a corpus'''\n",
    "        for text in corpus : self.addSentence(text)\n",
    "        return \n",
    "                \n",
    "    def removeRareWords(self, min_count):\n",
    "        '''remove words appearing lesser than a min_count threshold'''\n",
    "        kept_word2count = {word: count for word, count in self.word2count.items() if count >= min_count}\n",
    "        self.initData(self.base_tokens)\n",
    "        for word, count in kept_word2count.items(): \n",
    "            self.addWord(word)\n",
    "            self.word2count[word] = kept_word2count[word]\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attn_viz\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Attention weights visualization\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils/Attention_Weight_Visualization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/Attention_Weight_Visualization.py\n",
    "\n",
    "# single attention head over a sequence of words\n",
    "def heatmap(data, row_labels, col_labels, ax = None, cbar_kw = {}, cbarlabel = \"\", **kwargs):\n",
    "    if not ax: ax = plt.gca()\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    return im\n",
    "\n",
    "def annotate_heatmap(im, data = None, valfmt = \"{x:.2f}\", textcolors = [\"black\", \"white\"], threshold = None, **textkw):\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour dans le répertoire courant du tableau de bord :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jb\\Desktop\\NLP\\chatNLP\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "<a id=\"basDePage\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
