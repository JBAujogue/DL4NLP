{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 1 <br><br><br>\n",
    "  Word Embedding\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "1. <font color=red>**Word Embedding**</font>\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global purpose of Word Embedding is to represent a _Token_, a raw string representing a unit of text, as a low dimensional (dense) vector. The way tokens are defined only depends on the method used to split a text into text units : using blank spaces as separators or using classical NLTK or SpaCy's segmentation models leave _words_ as tokens, but other splitting protocols may be used to obtain _WordPiece_ tokens ([Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)). Here we broadly denote by _word_ any such token. <br> \n",
    "\n",
    "Commonly followed approaches for the embedding of words (aka tokens) decompose into three levels of granularity :\n",
    "\n",
    "| Level |  | |\n",
    "|------|------|------|\n",
    "| **Word** | [I.1 Custom model](#word_level_custom) | [I.2 Gensim Model](#gensim) |\n",
    "| **sub-word unit** | [II.1 FastText model](#fastText) |  |\n",
    "| **Character** |  |  |\n",
    "\n",
    "\n",
    "<br>\n",
    "Visualization with TensorBoard : https://www.tensorflow.org/guide/embedding (TODO)\n",
    "\n",
    "# Training objectives\n",
    "\n",
    "#### CBOW training objective\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient}, et consiste à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite de façon à pouvoir prédire chaque mot à partir de son contexte. La construction de cette table $T$ passe par la création d'un réseau de neurones, qui sert de modèle pour l'estimation de la probabilité de prédiction d'un mot $w_t$ d'après son contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce qu'un mot $w_t$ maximise la vraisemblance de la probabilité $P(. \\, | \\, c)$ fournie par le modèle. \n",
    "\n",
    "Le réseau de neurones de décrit de la façon suivante :\n",
    "\n",
    "![cbow](figs/CBOW.png)\n",
    "\n",
    "Un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ est vectorisé via une table $T$ fournissant un ensemble de vecteurs denses (typiquement de dimension comprise entre 50 et 300) $T(w_{t-N}), \\, ... \\, , T(w_{t-1})$, $T(w_{t+1}), \\, ... \\, , T(w_{t+N})$. Chaque vecteur est ensuite transformé via une transformation affine, dont les vecteurs résultants sont superposés en un unique vecteur\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\sum _{i = - N}^N M_i T(w_{t+i}) + b_i\n",
    "\\end{align*}\n",
    "\n",
    "Le vecteur $v_c$ est de dimension typiquement égale à la dimension de la vectorisation de mots. Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que le mot $w_{t}$ soit transformé en un vecteur $T'(w_{t})$ par cette table, et soit proposé en position $t$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_{t} \\, | \\, c\\,) = \\frac{\\exp\\left( T'(w_{t}) \\cdot v_c \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) \\cdot v_c \n",
    "\\right) }\n",
    "\\end{align*}\n",
    "\n",
    "Ici $\\cdot$ désigne le produit scalaire entre vecteurs. L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer un mot à partir du contexte.\n",
    "\n",
    "\n",
    "#### Skip-Gram training objective\n",
    "\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient} comme version mirroir au Continuous Bag Of Words, et consiste là encore à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite non pas de façon prédire un mot central $w$ à partir d'un contexte $c $ comme pour CBOW, mais plutôt de prédire le contexte $c $ à partir du mot central $w$. La construction de cette table $T$ passe par la création d'un réseau de neurones servant de modèle pour l'estimation de la probabilité de prédiction d'un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ à partir d'un mot central $w_t$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce que le contexte  $ c $ maximise la vraisemblance de la probabilité $P( . \\, | \\, w_t)$ fournie par le modèle.\n",
    "\n",
    "\n",
    "Une implémentation de ce modèle est la suivante : \n",
    "\n",
    "\n",
    "![skipgram](figs/Skipgram.png)\n",
    "\n",
    "\n",
    "Un mot courant $w_t$ est vectorisé par une table $T$ fournissant un vecteur dense (typiquement de dimension comprise entre 50 et 300) $T(w_t)$. Ce vecteur est alors transformé en un ensemble de $2N$ vecteurs\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma (M_{i} T(w_t) + b_{i}) \\qquad \\qquad i =-N,\\, ...\\, , -1, 1, \\, ...\\, , N\n",
    "\\end{align*}\n",
    "\n",
    "où $N$ désigne la taille de la fenêtre retenue, d'une dimension typiquement égale à la dimension de la vectorisation de mots, et $\\sigma$ une fonction non linéaire (typiquement la _Rectified Linear Unit_ $\\sigma (x) = max (0, x)$). Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que chaque mot $w_{t+i}$, transformé en un vecteur $T'(w_{t+i})$ par cette table, soit proposé en position $t+i$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P( w_{t+i} | \\, w_t) = \\frac{\\exp\\left(  T'(w_{t+i}) ^\\perp \\sigma \\left( M_i T(w_t) + b_{i}\\right) \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) ^\\perp \\sigma \\left( M_i T(w_t) + b_i\\right) \\right) }\n",
    "\\end{align*}\n",
    "\n",
    "On modélise alors la probabilité qu'un ensemble de mots $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ soit le contexte d'un mot $w_t$ par le produit\n",
    "\n",
    "\\begin{align*}\n",
    " P( c\\, | \\, w_t) = \\prod _{i = -N}^N P( w_{t+i}\\, | \\, w_t)\n",
    "\\end{align*}\n",
    "\n",
    "Ce modèle de probabilité du contexte d'un mot est naif au sens où les mots de contextes sont considérés comme indépendants deux à deux dès lors que le mot central est connu. Cette approximation rend cependant le calcul d'optimisation beaucoup plus court.\n",
    "\n",
    "\n",
    "\n",
    "L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer l'intégralité du contexte à partir de ce seul mot. La vectorisation Skip-Gram est typiquement plus performante que CBOW, car la table $T$ subit plus de contrainte dans son optimisation, et puisque le vecteur d'un mot est obtenu de façon à pouvoir prédire l'utilisation réelle du mot, ici donnée par son contexte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 0.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(path_to_NLP + '\\\\chatNLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> \n",
    "Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importSheet(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = []\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) #[str]\n",
    "                title  = current_tile                # str\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1])     #[str]\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importCorpus(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importSheet(file_name))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = importCorpus(path_to_NLP + '\\\\data\\\\AMM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_level\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Word Embedding\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_level_custom\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Custom Word-level Embedding Model\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "### 1.1.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language\n",
    "\n",
    "Classe de langage prennant en paramètre un corpus de la forme [[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from chatNLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, corpus = None, base_tokens = ['UNK'], min_count = None):\n",
    "        self.base_tokens = base_tokens\n",
    "        self.initData(base_tokens)\n",
    "        if    corpus is not None : self.addCorpus(corpus)\n",
    "        if min_count is not None : self.removeRareWords(min_count)\n",
    "\n",
    "        \n",
    "    def initData(self, base_tokens) :\n",
    "        self.word2index = {word : i for i, word in enumerate(base_tokens)}\n",
    "        self.index2word = {i : word for i, word in enumerate(base_tokens)}\n",
    "        self.word2count = {word : 0 for word in base_tokens}\n",
    "        self.n_words = len(base_tokens)\n",
    "        return\n",
    "    \n",
    "    def getIndex(self, word) :\n",
    "        if    word in self.word2index : return self.word2index[word]\n",
    "        elif 'UNK' in self.word2index : return self.word2index['UNK']\n",
    "        return\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        '''Add a word to the language'''\n",
    "        if word not in self.word2index:\n",
    "            if word.strip() != '' :\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = 1\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        return \n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        '''Add to the language all words of a sentence'''\n",
    "        words = sentence if type(sentence) == list else nltk.word_tokenize(sentence)\n",
    "        for word in words : self.addWord(word)          \n",
    "        return\n",
    "            \n",
    "    def addCorpus(self, corpus):\n",
    "        '''Add to the language all words contained into a corpus'''\n",
    "        for text in corpus : self.addSentence(text)\n",
    "        return \n",
    "                \n",
    "    def removeRareWords(self, min_count):\n",
    "        '''remove words appearing lesser than a min_count threshold'''\n",
    "        kept_word2count = {word: count for word, count in self.word2count.items() if count >= min_count}\n",
    "        self.initData(self.base_tokens)\n",
    "        for word, count in kept_word2count.items(): \n",
    "            self.addWord(word)\n",
    "            self.word2count[word] = kept_word2count[word]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLang(name, lang):\n",
    "    with open(path_to_NLP + '\\\\saves\\\\lang\\\\' + name + '.file', 'wb') as fil :\n",
    "        pickle.dump(lang, fil)\n",
    "    return\n",
    "\n",
    "def importLang(name):\n",
    "    with open(path_to_NLP + '\\\\saves\\\\lang\\\\' + name + '.file', 'rb') as fil :\n",
    "        lang = pickle.load(fil)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots comptés avant : 8088\n",
      "Mots comptés après : 4066\n"
     ]
    }
   ],
   "source": [
    "lang = Lang(corpus, base_tokens = ['SOS', 'EOS', 'UNK'])\n",
    "print(\"Mots comptés avant : {}\".format(lang.n_words))\n",
    "lang.removeRareWords(min_count = 4)\n",
    "print(\"Mots comptés après : {}\".format(lang.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveLang(name = 'DL4NLP_I1', lang = lang)\n",
    "#lang = importLang(name = 'DL4NLP_I1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec un vocabulaire de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "\n",
    "# --------------------- comparison with Glove vocab ------------------------\n",
    "def vocabGlove(name) :\n",
    "    words = []\n",
    "    path = path_to_NLP + '\\\\vectors\\\\' + name \n",
    "    with open(path + '.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def comparaison(lang) :\n",
    "    vocab_lang = list(lang.word2index.keys())\n",
    "    intersect_glove = intersection(vocab_glove, vocab_lang)\n",
    "    reste_glove = np.setdiff1d(vocab_lang, intersect_glove)\n",
    "    printComparaison('glove', vocab_lang, intersect_glove, reste_glove)\n",
    "    return intersect_glove, reste_glove\n",
    "\n",
    "def printComparaison(nom, vocab_lang, intersect, reste) :\n",
    "    print('proportion de mots du langage appartenants à {}  {:.2f} % \\nproportion de mots du langage ny appartenant pas     {:.2f} %'.format(nom, len(intersect)*100/len(vocab_lang),len(reste)*100/len(vocab_lang) ) )\n",
    "\n",
    "\n",
    "# --------------------- detect missing spaces ------------------------\n",
    "def checkWhetherBroken(vocab, clean_vocab) :\n",
    "    exit = {}\n",
    "    for word in vocab :\n",
    "        exit[word] = True if word in clean_vocab else False\n",
    "    return exit\n",
    "\n",
    "def checkMissingSpaces(word, clean_vocab) :\n",
    "    for word2 in clean_vocab :\n",
    "        if word.startswith(word2) :\n",
    "            rest = word.replace(word2, '')\n",
    "            if rest in clean_vocab :\n",
    "                return word2 + ' ' + rest\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_glove = vocabGlove('glove.6B.200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion de mots du langage appartenants à glove  85.76 % \n",
      "proportion de mots du langage ny appartenant pas     14.24 %\n"
     ]
    }
   ],
   "source": [
    "words_glove, reste_glove = comparaison(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec model\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from chatNLP.models.Word_Embedding import Word2Vec as myWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myWord2Vec(nn.Module) :\n",
    "    def __init__(self, lang, T = 100):\n",
    "        super(myWord2Vec, self).__init__()\n",
    "        self.lang = lang\n",
    "        if type(T) == int :\n",
    "            self.embedding = nn.Embedding(lang.n_words, T)  \n",
    "        else :\n",
    "            self.embedding = nn.Embedding(T.shape[0], T.shape[1])\n",
    "            self.embedding.weight = nn.Parameter(torch.FloatTensor(T))\n",
    "            \n",
    "        self.output_dim = self.lookupTable().shape[1]\n",
    "        self.sims = None\n",
    "        \n",
    "    def lookupTable(self) :\n",
    "        return self.embedding.weight.cpu().detach().numpy()\n",
    "        \n",
    "    def computeSimilarities(self) :\n",
    "        T = normalize(self.lookupTable(), norm = 'l2', axis = 1)\n",
    "        self.sims = np.matmul(T, T.transpose())\n",
    "        return\n",
    "\n",
    "    def most_similar(self, word, bound = 10) :\n",
    "        if word not in self.lang.word2index : return\n",
    "        if self.sims is None : self.computeSimilarities()\n",
    "        index = self.lang.word2index[word]\n",
    "        coefs = self.sims[index]\n",
    "        indices = coefs.argsort()[-bound -1 :-1]\n",
    "        output = [(self.lang.index2word[i], coefs[i]) for i in reversed(indices)]\n",
    "        return output\n",
    "    \n",
    "    def wv(self, word) :\n",
    "        return self.lookupTable()[self.lang.getIndex(word)]\n",
    "    \n",
    "    def addWord(self, word, vector = None) :\n",
    "        self.lang.addWord(word)\n",
    "        T = self.lookupTable()\n",
    "        v = np.random.rand(1, T.shape[1]) if vector is None else vector\n",
    "        updated_T = np.concatenate((T, v), axis = 0)\n",
    "        self.embedding = nn.Embedding(updated_T.shape[0], updated_T.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(updated_T))\n",
    "        return\n",
    "    \n",
    "    def freeze(self) :\n",
    "        for param in self.embedding.parameters() : param.requires_grad = False\n",
    "        return self\n",
    "    \n",
    "    def unfreeze(self) :\n",
    "        for param in self.embedding.parameters() : param.requires_grad = True\n",
    "        return self\n",
    "    \n",
    "    def forward(self, words, device = None) :\n",
    "        '''Transforms a list of n words into a torch.FloatTensor of size (1, n, emb_dim)'''\n",
    "        indices  = [self.lang.getIndex(w) for w in words]\n",
    "        indices  = [[i for i in indices if i is not None]]\n",
    "        variable = Variable(torch.LongTensor(indices)) # size = (1, n)\n",
    "        if device is not None : variable = variable.to(device)\n",
    "        tensor   = self.embedding(variable)            # size = (1, n, emb_dim)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Shell\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Shell acting as a wrapper around the Word2Vec model, implementing :\n",
    "\n",
    "- The layers suited for the training objective\n",
    "- The methods for all optimization steps\n",
    "- The methods for generating the data suitable for the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from chatNLP.models.Word_Embedding import Word2VecShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecShell(nn.Module):\n",
    "    '''Word2Vec model :\n",
    "        - sg = 0 yields CBOW training procedure\n",
    "        - sg = 1 yields Skip-Gram training procedure\n",
    "    '''\n",
    "    def __init__(self, word2vec, device, sg = 0, context_size = 5, hidden_dim = 150, \n",
    "                 criterion = nn.NLLLoss(size_average = False), optimizer = optim.SGD):\n",
    "        super(Word2VecShell, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # core of Word2Vec\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "        # training layers\n",
    "        self.input_n_words  = (2 * context_size if sg == 0 else 1)\n",
    "        self.output_n_words = (1 if sg == 0 else 2 * context_size)\n",
    "        self.linear_1  = nn.Linear(self.input_n_words * word2vec.embedding.weight.size(1), self.output_n_words * hidden_dim)\n",
    "        self.linear_2  = nn.Linear(hidden_dim, lang.n_words)\n",
    "        \n",
    "        # training tools\n",
    "        self.sg = sg\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        '''Transforms a batch of Ngrams of size (batch_size, input_n_words)\n",
    "           Into log probabilities of size (batch_size, lang.n_words, output_n_words)\n",
    "           '''\n",
    "        batch = batch.to(self.device)                 # size = (batch_size, self.input_n_words)\n",
    "        embed = self.word2vec.embedding(batch)        # size = (batch_size, self.input_n_words, embedding_dim)\n",
    "        embed = embed.view((batch.size(0), -1))       # size = (batch_size, self.input_n_words * embedding_dim)\n",
    "        out = self.linear_1(embed)                    # size = (batch_size, self.output_n_words * hidden_dim) \n",
    "        out = out.view((batch.size(0),self.output_n_words, -1))\n",
    "        out = F.relu(out)                             # size = (batch_size, self.output_n_words, hidden_dim)                                         \n",
    "        out = self.linear_2(out)                      # size = (batch_size, self.output_n_words, lang.n_words)\n",
    "        out = torch.transpose(out, 1, 2)              # size = (batch_size, lang.n_words, self.output_n_words)\n",
    "        log_probs = F.log_softmax(out, dim = 1)       # size = (batch_size, lang.n_words, self.output_n_words)\n",
    "        return log_probs\n",
    "    \n",
    "    def generatePackedNgrams(self, corpus, context_size = 5, batch_size = 32, seed = 42) :\n",
    "        # generate Ngrams\n",
    "        data = []\n",
    "        for text in corpus :\n",
    "            text = [w for w in text if w in self.word2vec.lang.word2index]\n",
    "            text = ['SOS' for i in range(context_size)] + text + ['EOS' for i in range(context_size)]\n",
    "            for i in range(context_size, len(text) - context_size):\n",
    "                context = text[i-context_size : i] + text[i+1 : i+context_size+1]\n",
    "                word = text[i]\n",
    "                data.append([word, context])\n",
    "        # pack Ngrams into mini_batches\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            pack0 = [el[0] for el in data[i:i + batch_size]]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w)] for w in pack0]\n",
    "            pack0 = Variable(torch.LongTensor(pack0)) # size = (batch_size, 1)\n",
    "            pack1 = [el[1] for el in data[i:i + batch_size]]\n",
    "            pack1 = [[self.word2vec.lang.getIndex(w) for w in context] for context in pack1]\n",
    "            pack1 = Variable(torch.LongTensor(pack1)) # size = (batch_size, 2*context_size)   \n",
    "            if   self.sg == 1 : packed_data.append([pack0, pack1])\n",
    "            elif self.sg == 0 : packed_data.append([pack1, pack0])\n",
    "            else :\n",
    "                print('A problem occured')\n",
    "                pass\n",
    "        return packed_data\n",
    "    \n",
    "    def train(self, ngrams, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = False):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loop\n",
    "        s\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            accuracy = 0\n",
    "            for i in range(targets.size(0)) :\n",
    "                for j in range(targets.size(1)) :\n",
    "                    topv, topi = log_probs[i, :, j].data.topk(1) \n",
    "                    ni = topi[0][0]\n",
    "                    if ni == targets[i, j].data[0] : accuracy += 1\n",
    "            return (accuracy * 100) / (targets.size(0) * targets.size(1))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(couple, optimizer, compute_accuracy = False):\n",
    "            \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = self(couple[0])           # size = (batch_size, agent.output_n_words, agent.lang.n_words)\n",
    "            targets   = couple[1].to(self.device) # size = (batch_size, agent.output_n_words)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / (targets.size(0) * targets.size(1))), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_loss_words = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                couple = random.choice(ngrams)\n",
    "                loss, loss_words = trainLoop(couple, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_loss_words += loss_words      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_loss_words = printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(ngrams) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(ngrams)\n",
    "                for couple in ngrams :\n",
    "                    loss, loss_words = trainLoop(couple, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_loss_words += loss_words \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_loss_words = printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Training with CBOW objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(corpus, base_tokens = ['SOS', 'EOS', 'UNK'], min_count = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow.word2vec = word2vec : True\n"
     ]
    }
   ],
   "source": [
    "word2vec = myWord2Vec(lang, T = 75)\n",
    "cbow = Word2VecShell(word2vec, device, sg = 0, context_size = 5, hidden_dim = 150)\n",
    "print('cbow.word2vec = word2vec :', cbow.word2vec == word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngrams = cbow.generatePackedNgrams(corpus, context_size = 5, batch_size = 32, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "The training methods allows to display accuracy over predicted target words. However, since the underlying computation is quite time consuming, we display accuracy only at the begining of training, and a few times periodically along the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cbow.train(Ngrams, iters = 100, lr = 0.005, print_every = 100, compute_accuracy = True)\n",
    "\n",
    "for alpha in [0.005, 0.001, 0.0005, 0.00025, 0.0001] : \n",
    "    cbow.train(Ngrams, epochs = 3,  lr = alpha, print_every = 100)\n",
    "    cbow.train(Ngrams, iters = 100, lr = alpha, print_every = 100, compute_accuracy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pasteurized', 0.37487525),\n",
       " ('establish', 0.35843933),\n",
       " ('sodium', 0.35818478),\n",
       " ('responders', 0.35638148),\n",
       " ('indicative', 0.33886254),\n",
       " ('protective', 0.3352614),\n",
       " ('order', 0.32748583),\n",
       " ('hydrochloride', 0.31852683),\n",
       " ('positively', 0.3115115),\n",
       " ('greatest', 0.3066505)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(word = 'formaldehyde', bound = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load<br>\n",
    "\n",
    "The lightweight word2vec model can be saved for further use, or alternatively the full shell wrapping the word2vec model can be saved for subsequent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(word2vec, path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I1_cbow.pt')\n",
    "\n",
    "# load\n",
    "#word2vec = torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I1_cbow.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Training with SkipGram objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(corpus, base_tokens = ['SOS', 'EOS', 'UNK'], min_count = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram.word2vec = word2vec : True\n"
     ]
    }
   ],
   "source": [
    "word2vec = myWord2Vec(lang, T = 75)\n",
    "skipgram = Word2VecShell(word2vec, device, sg = 1, context_size = 5, hidden_dim = 150)\n",
    "print('skipgram.word2vec = word2vec :', skipgram.word2vec == word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ngrams = skipgram.generatePackedNgrams(corpus, context_size = 5, batch_size = 32, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skipgram.train(Ngrams, iters = 100, lr = 0.005, print_every = 100, compute_accuracy = True)\n",
    "\n",
    "for alpha in [0.005, 0.001, 0.0005] : \n",
    "    skipgram.train(Ngrams, epochs = 3,  lr = alpha, print_every = 100)\n",
    "    skipgram.train(Ngrams, iters = 100, lr = alpha, print_every = 100, compute_accuracy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fdnc1947', 0.37422585),\n",
       " ('vendor', 0.3686211),\n",
       " ('nitrate', 0.3527695),\n",
       " ('pairwise', 0.336586),\n",
       " ('phase', 0.33086002),\n",
       " ('hydride', 0.32060236),\n",
       " ('mineralizate', 0.31994864),\n",
       " ('kinetic', 0.31498104),\n",
       " ('filled', 0.3145452),\n",
       " ('daily', 0.30329823)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(word = 'final', bound = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load<br>\n",
    "\n",
    "The lightweight word2vec model can be saved for further use, or alternatively the full shell wrapping the word2vec model can be saved for subsequent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(word2vec, path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I1_skipgram.pt')\n",
    "\n",
    "# load\n",
    "#word2vec = torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I1_skipgram.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gensim\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Gensim Word2Vec\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Link : https://radimrehurek.com/gensim/models/word2vec.html<br>\n",
    "Tutorials :\n",
    "\n",
    "- https://cambridgespark.com/4046-2/\n",
    "- https://rare-technologies.com/word2vec-tutorial/\n",
    "- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "### 1.2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Training with CBOW objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_gensim = Word2Vec(corpus, \n",
    "                       size = 75, \n",
    "                       window = 5, \n",
    "                       min_count = 4, \n",
    "                       negative = 15, \n",
    "                       iter = 50,\n",
    "                       sg = 0,\n",
    "                       workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thiomersal', 0.6767041683197021),\n",
       " ('polysorbate', 0.6719549298286438),\n",
       " ('ovalbumin', 0.6334631443023682),\n",
       " ('moisture', 0.6150280833244324),\n",
       " ('2phenoxyethanol', 0.6061439514160156),\n",
       " ('phenoxyethanol', 0.6048438549041748),\n",
       " ('phosphorus', 0.5749061107635498),\n",
       " ('aluminium', 0.553789496421814),\n",
       " ('phenol', 0.5379729270935059),\n",
       " ('sucrose', 0.5111640691757202)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(cbow_gensim)\n",
    "cbow_gensim.wv.most_similar('formaldehyde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load\n",
    "\n",
    "The Gensim model can easily be saved & loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_cbow_gensim.model\")\n",
    "#cbow_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_cbow_gensim.model\")\n",
    "#cbow_gensim = Word2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it is direct to build a lightweight word2vec model out of a trained gensim model and then save & load it as done in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = myWord2Vec(lang = Lang(corpus = [list(cbow_gensim.wv.index2word)], base_tokens = []), T = cbow_gensim.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polysorbate', 0.6989662),\n",
       " ('ovalbumin', 0.67078525),\n",
       " ('thiomersal', 0.6553794),\n",
       " ('phenol', 0.6534851),\n",
       " ('phosphorus', 0.6203261),\n",
       " ('phenoxyethanol', 0.61794806),\n",
       " ('moisture', 0.5869265),\n",
       " ('sucrose', 0.5719405),\n",
       " ('2phenoxyethanol', 0.5603489),\n",
       " ('aluminium', 0.535767)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('formaldehyde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Training with SkipGram objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_gensim = Word2Vec(corpus, \n",
    "                           size = 75, \n",
    "                           window = 5, \n",
    "                           min_count = 4, \n",
    "                           negative = 15, \n",
    "                           iter = 50,\n",
    "                           sg = 1,\n",
    "                           workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thiomersal', 0.6575016975402832),\n",
       " ('ovalbumin', 0.6415812969207764),\n",
       " ('phenoxyethanol', 0.6408674716949463),\n",
       " ('phenol', 0.6294451951980591),\n",
       " ('hcho', 0.62266606092453),\n",
       " ('residual', 0.587759256362915),\n",
       " ('free', 0.5830410718917847),\n",
       " ('aluminium', 0.5764816999435425),\n",
       " ('polysorbate', 0.5677136182785034),\n",
       " ('triton', 0.5672757625579834)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(skipgram_gensim)\n",
    "skipgram_gensim.wv.most_similar('formaldehyde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_skipgram_gensim.model\")\n",
    "#skipgram_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_skipgram_gensim.model\")\n",
    "#skipgram_gensim = Word2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sub_word_level\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2 Word Embedding via sub-word units\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fastText\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 FastText's Word Embedding via character n-grams\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "We consider the Gensim implementation of FastText, based on the CBOW training objective.<br>\n",
    "Tutorial : [Gensim FastText](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb)<br>\n",
    "Link to the original paper : [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf).\n",
    "\n",
    "### 2.1.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Training with CBOW objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim = FT_gensim(size = 75, \n",
    "                                 window = 5, \n",
    "                                 min_count = 4, \n",
    "                                 negative = 15,\n",
    "                                 sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim.train(sentences = corpus, \n",
    "                           epochs = 50,\n",
    "                           total_examples = cbow_fastText_gensim.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glutaraldehyde', 0.7824963331222534),\n",
       " ('thiomersal', 0.6987553834915161),\n",
       " ('phenoxyethanol', 0.66783607006073),\n",
       " ('ovalbumin', 0.6669738292694092),\n",
       " ('polysorbate', 0.6604278087615967),\n",
       " ('2phenoxyethanol', 0.6562307476997375),\n",
       " ('formal', 0.6519432067871094),\n",
       " ('acetaldehyde', 0.6417372226715088),\n",
       " ('phenoxy', 0.6388593316078186),\n",
       " ('phenol', 0.6205344200134277)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_fastText_gensim.wv.most_similar('formaldehyde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_cbow_fasttext.model\")\n",
    "#cbow_fastText_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_cbow_fasttext.model\")\n",
    "#cbow_fastText_gensim = FT_gensim.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it is direct to build a lightweight word2vec model out of a trained gensim model and then save & load it as done in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = myWord2Vec(lang = Lang(corpus = [list(cbow_fastText_gensim.wv.index2word)], base_tokens = []), T = cbow_fastText_gensim.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the main advantage FastText offers is the possibility to get an embedding vector out of **any word**, and in fact any string thanks to the character-ngrams embedding trick :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim['HelloWorld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, it can be interesting to load the look-up word vectors table into a lightweight word2vec module, as it allows to further optimize this table for any specific downstream task performed by a larger PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Training with SkipGram objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_gensim = FT_gensim(size = 75, \n",
    "                           window = 5, \n",
    "                           min_count = 4, \n",
    "                           negative = 15,\n",
    "                           sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_gensim.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_gensim.train(sentences = corpus, \n",
    "                      epochs = 50,\n",
    "                      total_examples = fastText_gensim.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phenoxyethanol', 0.7352535724639893),\n",
       " ('ovalbumin', 0.7251032590866089),\n",
       " ('2phenoxyethanol', 0.710415244102478),\n",
       " ('hcho', 0.6905250549316406),\n",
       " ('polysorbate', 0.684985876083374),\n",
       " ('residual', 0.639625608921051),\n",
       " ('thiomersal', 0.6269700527191162),\n",
       " ('acetyl', 0.6253007650375366),\n",
       " ('phosphorus', 0.6174876093864441),\n",
       " ('phenol', 0.6120678186416626)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastText_gensim.wv.most_similar('formaldehyde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_fasttext.model\")\n",
    "#fastText_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_fasttext.model\")\n",
    "#fastText_gensim = FT_gensim.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastText_gensim[['13', 'to']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
