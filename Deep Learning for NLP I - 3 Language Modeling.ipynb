{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 3 <br><br><br>\n",
    "  Language Modeling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. <font color=red>**Language Modeling**</font>\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global structure of the [language model](#language_model) is the pipeline of two modules, followed by a final classification layer :\n",
    "\n",
    "\n",
    "\n",
    "| | Module |  | |\n",
    "|------|------|------|------|\n",
    "| 1 | **Word Embedding** | [I.1 Custom model](#word_level_custom) | [I.2 Gensim Model](#gensim) | [I.3 FastText model](#fastText) |\n",
    "| 2 | **Contextualization** | [II.1 bidirectionnal GRU](#bi_gru) | [II.2 Transformer](#transformer) |\n",
    "| 3 | **Attention** | [III.1 Attention](#attention) | [III.2 Multi-head Attention](#attention) |\n",
    "\n",
    "\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**.\n",
    "\n",
    "Exemples d'implémentation en PyTorch :\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "\n",
    "\n",
    "Différentes architectures sont décrites dans la litérature :\n",
    "\n",
    "- Regularizing and Optimizing LSTM Language Models - https://arxiv.org/pdf/1708.02182.pdf\n",
    "\n",
    "Un modèle linguistique est intérressant en soi, mais peut aussi servir pour le pré-entrainement de couches basses d'un modèle plus complexe :\n",
    "\n",
    "- Deep contextualized word representations - https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Improving Language Understanding by Generative Pre-Training - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "- Language Models are Unsupervised Multitask Learners - https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.3.1\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\libDL4NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importSheet(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importCorpus(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importSheet(file_name))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importCorpus(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "## 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = FastText(size = 75, \n",
    "                             window = 5, \n",
    "                             min_count = 1, \n",
    "                             negative = 20,\n",
    "                             sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8086"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fastText_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.train(sentences = corpus, \n",
    "                        epochs = 50,\n",
    "                        total_examples = fastText_word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(fastText_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"language_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = False)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1, :, :]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, depth_range = (2, 10)) :\n",
    "        sentences = [s[i: i+j] \\\n",
    "                     for s in sentences \\\n",
    "                     for i in range(len(s)-depth_range[0]) \\\n",
    "                     for j in range(depth_range[0], min(depth_range[1], len(s)-i)+1) \\\n",
    "                    ]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack1 = Variable(torch.LongTensor([s[-1] for s in pack0]))\n",
    "            pack0 = [s[:-1] for s in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hidden[-1, :, :]), dim = 1)   # dim = (batch_size, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets   = batch[1].to(self.device).view(-1)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462138"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = LanguageModel(device,\n",
    "                               tokenizer = lambda s : s.split(' '),\n",
    "                               word2vec = word2vec,\n",
    "                               hidden_dim = 50, \n",
    "                               n_layers = 3, \n",
    "                               dropout = 0.1,\n",
    "                               optimizer = optim.SGD)\n",
    "\n",
    "language_model.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (word2vec): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(8088, 75)\n",
       "    )\n",
       "    (embedding): Embedding(8088, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (bigru): GRU(75, 50, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  )\n",
       "  (out): Linear(in_features=50, out_features=8088, bias=True)\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121513"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = language_model.generatePackedSentences(corpus, batch_size = 64, depth_range = (5, 20))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 22s (- 14m 42s) (500 2%) loss : 6.562  accuracy : 5.9 %\n",
      "0m 44s (- 14m 13s) (1000 5%) loss : 6.021  accuracy : 10.5 %\n",
      "1m 6s (- 13m 36s) (1500 7%) loss : 5.750  accuracy : 12.6 %\n",
      "1m 27s (- 13m 5s) (2000 10%) loss : 5.578  accuracy : 14.2 %\n",
      "1m 48s (- 12m 40s) (2500 12%) loss : 5.286  accuracy : 16.1 %\n",
      "2m 8s (- 12m 10s) (3000 15%) loss : 5.201  accuracy : 17.3 %\n",
      "2m 33s (- 12m 2s) (3500 17%) loss : 5.077  accuracy : 18.3 %\n",
      "2m 58s (- 11m 55s) (4000 20%) loss : 4.978  accuracy : 18.7 %\n",
      "3m 21s (- 11m 32s) (4500 22%) loss : 4.879  accuracy : 19.9 %\n",
      "3m 44s (- 11m 12s) (5000 25%) loss : 4.825  accuracy : 20.4 %\n",
      "4m 23s (- 11m 35s) (5500 27%) loss : 4.713  accuracy : 20.8 %\n",
      "5m 12s (- 12m 10s) (6000 30%) loss : 4.707  accuracy : 21.2 %\n",
      "6m 3s (- 12m 34s) (6500 32%) loss : 4.691  accuracy : 21.4 %\n",
      "6m 39s (- 12m 22s) (7000 35%) loss : 4.541  accuracy : 22.8 %\n",
      "7m 0s (- 11m 41s) (7500 37%) loss : 4.555  accuracy : 22.9 %\n",
      "7m 21s (- 11m 1s) (8000 40%) loss : 4.587  accuracy : 23.0 %\n",
      "7m 42s (- 10m 26s) (8500 42%) loss : 4.534  accuracy : 23.2 %\n",
      "8m 5s (- 9m 53s) (9000 45%) loss : 4.466  accuracy : 24.0 %\n",
      "8m 29s (- 9m 23s) (9500 47%) loss : 4.435  accuracy : 24.0 %\n",
      "8m 53s (- 8m 53s) (10000 50%) loss : 4.402  accuracy : 24.8 %\n",
      "9m 14s (- 8m 22s) (10500 52%) loss : 4.441  accuracy : 24.1 %\n",
      "9m 38s (- 7m 53s) (11000 55%) loss : 4.357  accuracy : 25.0 %\n",
      "9m 59s (- 7m 23s) (11500 57%) loss : 4.347  accuracy : 24.9 %\n",
      "10m 20s (- 6m 53s) (12000 60%) loss : 4.298  accuracy : 25.4 %\n",
      "10m 41s (- 6m 24s) (12500 62%) loss : 4.296  accuracy : 25.2 %\n",
      "11m 1s (- 5m 56s) (13000 65%) loss : 4.374  accuracy : 24.4 %\n",
      "11m 21s (- 5m 28s) (13500 67%) loss : 4.329  accuracy : 25.2 %\n",
      "11m 41s (- 5m 0s) (14000 70%) loss : 4.338  accuracy : 25.1 %\n",
      "12m 2s (- 4m 33s) (14500 72%) loss : 4.345  accuracy : 25.1 %\n",
      "12m 22s (- 4m 7s) (15000 75%) loss : 4.306  accuracy : 25.2 %\n",
      "12m 42s (- 3m 41s) (15500 77%) loss : 4.269  accuracy : 25.8 %\n",
      "13m 2s (- 3m 15s) (16000 80%) loss : 4.214  accuracy : 26.8 %\n",
      "13m 23s (- 2m 50s) (16500 82%) loss : 4.246  accuracy : 25.6 %\n",
      "13m 43s (- 2m 25s) (17000 85%) loss : 4.192  accuracy : 26.8 %\n",
      "14m 4s (- 2m 0s) (17500 87%) loss : 4.288  accuracy : 25.3 %\n",
      "14m 24s (- 1m 36s) (18000 90%) loss : 4.260  accuracy : 25.7 %\n",
      "14m 44s (- 1m 11s) (18500 92%) loss : 4.265  accuracy : 25.7 %\n",
      "15m 5s (- 0m 47s) (19000 95%) loss : 4.249  accuracy : 25.6 %\n",
      "15m 25s (- 0m 23s) (19500 97%) loss : 4.219  accuracy : 26.4 %\n",
      "15m 45s (- 0m 0s) (20000 100%) loss : 4.212  accuracy : 25.9 %\n",
      "0m 20s (- 13m 33s) (500 2%) loss : 4.128  accuracy : 27.6 %\n",
      "0m 41s (- 13m 12s) (1000 5%) loss : 4.164  accuracy : 27.4 %\n",
      "1m 1s (- 12m 43s) (1500 7%) loss : 4.081  accuracy : 27.9 %\n",
      "1m 22s (- 12m 20s) (2000 10%) loss : 4.045  accuracy : 28.7 %\n",
      "1m 42s (- 12m 0s) (2500 12%) loss : 4.033  accuracy : 28.5 %\n",
      "2m 4s (- 11m 47s) (3000 15%) loss : 3.927  accuracy : 29.7 %\n",
      "2m 27s (- 11m 34s) (3500 17%) loss : 4.069  accuracy : 28.6 %\n",
      "2m 48s (- 11m 15s) (4000 20%) loss : 4.051  accuracy : 29.1 %\n",
      "3m 9s (- 10m 52s) (4500 22%) loss : 4.035  accuracy : 28.7 %\n",
      "3m 31s (- 10m 34s) (5000 25%) loss : 3.972  accuracy : 29.1 %\n",
      "3m 51s (- 10m 11s) (5500 27%) loss : 4.023  accuracy : 29.1 %\n",
      "4m 12s (- 9m 48s) (6000 30%) loss : 3.953  accuracy : 29.7 %\n",
      "4m 32s (- 9m 26s) (6500 32%) loss : 3.957  accuracy : 29.4 %\n",
      "4m 53s (- 9m 4s) (7000 35%) loss : 3.957  accuracy : 30.0 %\n",
      "5m 13s (- 8m 43s) (7500 37%) loss : 4.063  accuracy : 28.0 %\n",
      "5m 34s (- 8m 21s) (8000 40%) loss : 3.943  accuracy : 29.6 %\n",
      "5m 54s (- 8m 0s) (8500 42%) loss : 4.001  accuracy : 29.3 %\n",
      "6m 15s (- 7m 38s) (9000 45%) loss : 4.015  accuracy : 28.8 %\n",
      "6m 35s (- 7m 17s) (9500 47%) loss : 3.981  accuracy : 29.2 %\n",
      "6m 56s (- 6m 56s) (10000 50%) loss : 4.024  accuracy : 29.0 %\n",
      "7m 16s (- 6m 35s) (10500 52%) loss : 4.000  accuracy : 28.9 %\n",
      "7m 36s (- 6m 13s) (11000 55%) loss : 3.950  accuracy : 29.7 %\n",
      "7m 57s (- 5m 52s) (11500 57%) loss : 3.956  accuracy : 29.3 %\n",
      "8m 19s (- 5m 33s) (12000 60%) loss : 3.890  accuracy : 30.1 %\n",
      "8m 40s (- 5m 12s) (12500 62%) loss : 3.870  accuracy : 30.3 %\n",
      "9m 1s (- 4m 51s) (13000 65%) loss : 3.990  accuracy : 29.3 %\n",
      "9m 22s (- 4m 30s) (13500 67%) loss : 3.995  accuracy : 28.8 %\n",
      "9m 45s (- 4m 10s) (14000 70%) loss : 3.983  accuracy : 29.0 %\n",
      "10m 5s (- 3m 49s) (14500 72%) loss : 3.953  accuracy : 29.5 %\n",
      "10m 26s (- 3m 28s) (15000 75%) loss : 3.997  accuracy : 28.8 %\n",
      "10m 50s (- 3m 8s) (15500 77%) loss : 3.977  accuracy : 29.3 %\n",
      "11m 10s (- 2m 47s) (16000 80%) loss : 3.978  accuracy : 29.1 %\n",
      "11m 31s (- 2m 26s) (16500 82%) loss : 3.989  accuracy : 29.3 %\n",
      "11m 53s (- 2m 5s) (17000 85%) loss : 3.927  accuracy : 30.1 %\n",
      "12m 14s (- 1m 44s) (17500 87%) loss : 3.926  accuracy : 29.7 %\n",
      "12m 35s (- 1m 23s) (18000 90%) loss : 3.856  accuracy : 30.4 %\n",
      "12m 56s (- 1m 2s) (18500 92%) loss : 3.986  accuracy : 29.6 %\n",
      "13m 17s (- 0m 41s) (19000 95%) loss : 3.886  accuracy : 30.0 %\n",
      "13m 39s (- 0m 21s) (19500 97%) loss : 3.916  accuracy : 30.0 %\n",
      "14m 0s (- 0m 0s) (20000 100%) loss : 3.896  accuracy : 30.1 %\n",
      "0m 20s (- 13m 30s) (500 2%) loss : 3.935  accuracy : 29.7 %\n",
      "0m 42s (- 13m 32s) (1000 5%) loss : 3.955  accuracy : 29.4 %\n",
      "1m 4s (- 13m 11s) (1500 7%) loss : 3.903  accuracy : 30.5 %\n",
      "1m 25s (- 12m 45s) (2000 10%) loss : 3.878  accuracy : 30.6 %\n",
      "1m 45s (- 12m 21s) (2500 12%) loss : 3.888  accuracy : 30.4 %\n",
      "2m 8s (- 12m 6s) (3000 15%) loss : 3.897  accuracy : 30.3 %\n",
      "2m 31s (- 11m 52s) (3500 17%) loss : 3.839  accuracy : 30.8 %\n",
      "2m 52s (- 11m 30s) (4000 20%) loss : 3.911  accuracy : 29.7 %\n",
      "3m 13s (- 11m 7s) (4500 22%) loss : 3.958  accuracy : 29.2 %\n",
      "3m 35s (- 10m 45s) (5000 25%) loss : 3.925  accuracy : 29.9 %\n",
      "3m 57s (- 10m 26s) (5500 27%) loss : 3.859  accuracy : 31.0 %\n",
      "4m 21s (- 10m 9s) (6000 30%) loss : 3.903  accuracy : 30.6 %\n",
      "4m 41s (- 9m 45s) (6500 32%) loss : 3.873  accuracy : 31.0 %\n",
      "5m 2s (- 9m 22s) (7000 35%) loss : 3.891  accuracy : 30.3 %\n",
      "5m 24s (- 9m 0s) (7500 37%) loss : 3.865  accuracy : 30.7 %\n",
      "5m 45s (- 8m 38s) (8000 40%) loss : 3.915  accuracy : 30.1 %\n",
      "6m 7s (- 8m 16s) (8500 42%) loss : 3.907  accuracy : 30.4 %\n",
      "6m 28s (- 7m 54s) (9000 45%) loss : 3.901  accuracy : 30.3 %\n",
      "6m 49s (- 7m 32s) (9500 47%) loss : 3.910  accuracy : 30.1 %\n",
      "7m 11s (- 7m 11s) (10000 50%) loss : 3.860  accuracy : 31.0 %\n",
      "7m 33s (- 6m 50s) (10500 52%) loss : 3.894  accuracy : 30.7 %\n",
      "7m 54s (- 6m 27s) (11000 55%) loss : 3.838  accuracy : 31.7 %\n",
      "8m 14s (- 6m 5s) (11500 57%) loss : 3.805  accuracy : 31.6 %\n",
      "8m 36s (- 5m 44s) (12000 60%) loss : 3.869  accuracy : 30.8 %\n",
      "8m 58s (- 5m 22s) (12500 62%) loss : 3.985  accuracy : 29.7 %\n",
      "9m 18s (- 5m 0s) (13000 65%) loss : 3.855  accuracy : 30.7 %\n",
      "9m 38s (- 4m 38s) (13500 67%) loss : 3.918  accuracy : 30.0 %\n",
      "9m 59s (- 4m 16s) (14000 70%) loss : 3.889  accuracy : 30.3 %\n",
      "10m 20s (- 3m 55s) (14500 72%) loss : 3.867  accuracy : 30.7 %\n",
      "10m 41s (- 3m 33s) (15000 75%) loss : 3.882  accuracy : 30.6 %\n",
      "11m 2s (- 3m 12s) (15500 77%) loss : 3.900  accuracy : 30.4 %\n",
      "11m 23s (- 2m 50s) (16000 80%) loss : 3.949  accuracy : 29.3 %\n",
      "11m 43s (- 2m 29s) (16500 82%) loss : 3.819  accuracy : 31.1 %\n",
      "12m 5s (- 2m 8s) (17000 85%) loss : 3.895  accuracy : 30.6 %\n",
      "12m 26s (- 1m 46s) (17500 87%) loss : 3.849  accuracy : 30.9 %\n",
      "12m 48s (- 1m 25s) (18000 90%) loss : 3.895  accuracy : 30.6 %\n",
      "13m 9s (- 1m 4s) (18500 92%) loss : 3.800  accuracy : 31.4 %\n",
      "13m 30s (- 0m 42s) (19000 95%) loss : 3.836  accuracy : 30.6 %\n",
      "13m 51s (- 0m 21s) (19500 97%) loss : 3.867  accuracy : 30.6 %\n",
      "14m 12s (- 0m 0s) (20000 100%) loss : 3.872  accuracy : 31.0 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches, iters = 20000, lr = 0.01, print_every = 500)\n",
    "language_model.fit(batches, iters = 20000, lr = 0.0025, print_every = 500)\n",
    "language_model.fit(batches, iters = 20000, lr = 0.0005, print_every = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(language_model.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I3_language_model.pth')\n",
    "\n",
    "# load\n",
    "#language_model.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I3_language_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". introduction . this section provides validation information for the analytical procedures used to release the tdap ipv final bulk product and filled product ( unlabeled ) . the in vitro analytical procedures listed in table are performed in compliance with \u001b[48;2;255;229;217m ph eur monograph no current edition . \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# fastText gensim, n_layers = 3, dh = 150\n",
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
