{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 3 <br><br><br>\n",
    "  Language Modeling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "    _Applications :_\n",
    "    \n",
    "    - Extractive Summarization\n",
    "    - Sentiment Analysis\n",
    "    - Text segmentation\n",
    "\n",
    "\n",
    "3. <font color=red>**Language Modeling**</font>\n",
    "\n",
    "4. Sentence tagging\n",
    "\n",
    "    _Applications :_\n",
    "    \n",
    "    - Part-of-speech Tagging\n",
    "    - Named Entity Recognition\n",
    "    - Automatic Value Extraction\n",
    "    \n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global structure of the [language model](#language_model) is the pipeline of two modules, followed by a final classification layer :\n",
    "\n",
    "\n",
    "\n",
    "| | Module |  | |\n",
    "|------|------|------|------|\n",
    "| 1 | **Word Embedding** | [I.1 Custom model](#word_level_custom) | [I.2 Gensim Model](#gensim) | [I.3 FastText model](#fastText) |\n",
    "| 2 | **Contextualization** | [II.1 bidirectionnal GRU](#bi_gru) | [II.2 Transformer](#transformer) |\n",
    "| 3 | **Attention** | [III.1 Attention](#attention) | [III.2 Multi-head Attention](#attention) |\n",
    "\n",
    "\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**.\n",
    "\n",
    "Exemples d'implémentation en PyTorch :\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "\n",
    "\n",
    "Différentes architectures sont décrites dans la litérature :\n",
    "\n",
    "- Regularizing and Optimizing LSTM Language Models - https://arxiv.org/pdf/1708.02182.pdf\n",
    "\n",
    "Un modèle linguistique est intérressant en soi, mais peut aussi servir pour le pré-entrainement de couches basses d'un modèle plus complexe :\n",
    "\n",
    "- Deep contextualized word representations - https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Improving Language Understanding by Generative Pre-Training - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "- Language Models are Unsupervised Multitask Learners - https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 0.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\chatNLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus et donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text, label]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importSheet(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importCorpus(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importSheet(file_name))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importCorpus(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "## 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from chatNLP.models.Word_Embedding import Word2VecConnector\n",
    "from chatNLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = FastText(size = 75, \n",
    "                             window = 5, \n",
    "                             min_count = 1, \n",
    "                             negative = 20,\n",
    "                             sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8086"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fastText_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.train(sentences = corpus, \n",
    "                        epochs = 50,\n",
    "                        total_examples = fastText_word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(fastText_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"language_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classifier-like Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, word2vec, hidden_dim, n_layers, dropout = 0, class_weights = None, optimizer = optim.SGD) :\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = False)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = sentence.split(' ')\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1, :, :]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, max_depht = 30) :\n",
    "        sentences = [s[i: i+j] for s in sentences for i in range(len(s)-2) for j in range(2, min(max_depht, len(s)-i)+1)]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack1 = Variable(torch.LongTensor([s[-1] for s in pack0]))\n",
    "            pack0 = [s[:-1] for s in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hidden[-1, :, :]), dim = 1)   # dim = (batch_size, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets   = batch[1].to(self.device).view(-1)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.data[0] / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462138"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = LanguageModel(device,\n",
    "                               word2vec,\n",
    "                               hidden_dim = 50, \n",
    "                               n_layers = 3, \n",
    "                               dropout = 0.1,\n",
    "                               optimizer = optim.SGD)\n",
    "\n",
    "language_model.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (word2vec): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(8088, 75)\n",
       "    )\n",
       "    (embedding): Embedding(8088, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (bigru): GRU(75, 50, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  )\n",
       "  (out): Linear(in_features=50, out_features=8088, bias=True)\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = language_model.generatePackedSentences(corpus, batch_size = 64, max_depht = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106768"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 16s (- 10m 26s) (500 2%) loss : 4.093  accuracy : 27.8 %\n",
      "0m 32s (- 10m 9s) (1000 5%) loss : 4.030  accuracy : 28.6 %\n",
      "0m 48s (- 9m 57s) (1500 7%) loss : 4.009  accuracy : 29.0 %\n",
      "1m 4s (- 9m 41s) (2000 10%) loss : 4.067  accuracy : 28.7 %\n",
      "1m 20s (- 9m 26s) (2500 12%) loss : 4.045  accuracy : 28.4 %\n",
      "1m 36s (- 9m 6s) (3000 15%) loss : 3.992  accuracy : 29.3 %\n",
      "1m 52s (- 8m 48s) (3500 17%) loss : 4.005  accuracy : 29.1 %\n",
      "2m 7s (- 8m 30s) (4000 20%) loss : 3.964  accuracy : 29.6 %\n",
      "2m 23s (- 8m 14s) (4500 22%) loss : 3.982  accuracy : 29.0 %\n",
      "2m 39s (- 7m 59s) (5000 25%) loss : 4.018  accuracy : 29.0 %\n",
      "2m 58s (- 7m 49s) (5500 27%) loss : 3.939  accuracy : 30.0 %\n",
      "3m 14s (- 7m 33s) (6000 30%) loss : 4.029  accuracy : 29.2 %\n",
      "3m 31s (- 7m 18s) (6500 32%) loss : 4.045  accuracy : 29.1 %\n",
      "3m 47s (- 7m 2s) (7000 35%) loss : 4.045  accuracy : 28.4 %\n",
      "4m 5s (- 6m 48s) (7500 37%) loss : 4.083  accuracy : 28.1 %\n",
      "4m 21s (- 6m 31s) (8000 40%) loss : 3.988  accuracy : 29.4 %\n",
      "4m 37s (- 6m 14s) (8500 42%) loss : 4.000  accuracy : 29.4 %\n",
      "4m 52s (- 5m 57s) (9000 45%) loss : 4.027  accuracy : 28.9 %\n",
      "5m 8s (- 5m 40s) (9500 47%) loss : 3.998  accuracy : 29.1 %\n",
      "5m 24s (- 5m 24s) (10000 50%) loss : 4.044  accuracy : 29.0 %\n",
      "5m 40s (- 5m 8s) (10500 52%) loss : 4.025  accuracy : 28.9 %\n",
      "5m 56s (- 4m 51s) (11000 55%) loss : 4.042  accuracy : 28.6 %\n",
      "6m 11s (- 4m 34s) (11500 57%) loss : 3.993  accuracy : 29.3 %\n",
      "6m 27s (- 4m 18s) (12000 60%) loss : 4.056  accuracy : 28.5 %\n",
      "6m 43s (- 4m 2s) (12500 62%) loss : 3.962  accuracy : 29.7 %\n",
      "6m 59s (- 3m 45s) (13000 65%) loss : 4.045  accuracy : 28.5 %\n",
      "7m 14s (- 3m 29s) (13500 67%) loss : 3.966  accuracy : 29.8 %\n",
      "7m 30s (- 3m 12s) (14000 70%) loss : 4.004  accuracy : 29.1 %\n",
      "7m 45s (- 2m 56s) (14500 72%) loss : 3.990  accuracy : 29.1 %\n",
      "8m 1s (- 2m 40s) (15000 75%) loss : 3.926  accuracy : 29.8 %\n",
      "8m 17s (- 2m 24s) (15500 77%) loss : 3.937  accuracy : 29.6 %\n",
      "8m 32s (- 2m 8s) (16000 80%) loss : 3.953  accuracy : 29.4 %\n",
      "8m 48s (- 1m 52s) (16500 82%) loss : 4.001  accuracy : 28.7 %\n",
      "9m 4s (- 1m 36s) (17000 85%) loss : 3.950  accuracy : 29.6 %\n",
      "9m 20s (- 1m 20s) (17500 87%) loss : 4.007  accuracy : 28.6 %\n",
      "9m 36s (- 1m 4s) (18000 90%) loss : 4.066  accuracy : 28.3 %\n",
      "9m 51s (- 0m 47s) (18500 92%) loss : 4.003  accuracy : 29.2 %\n",
      "10m 7s (- 0m 31s) (19000 95%) loss : 4.014  accuracy : 28.7 %\n",
      "10m 23s (- 0m 15s) (19500 97%) loss : 4.032  accuracy : 28.5 %\n",
      "10m 38s (- 0m 0s) (20000 100%) loss : 3.915  accuracy : 30.3 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches, iters = 20000, lr = 0.01, print_every = 500)\n",
    "language_model.fit(batches, iters = 20000, lr = 0.0025, print_every = 500)\n",
    "language_model.fit(batches, iters = 20000, lr = 0.0005, print_every = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". principle . percent adsorption of the hepatitis b antigen onto the alum adjuvant is determined using a \" sandwich \" type enzyme linked immunosorbent assay ( elisa ) by measuring the antigen concentration in the supernatant of a sample compared to the antigen concentration in the total test sample . equipment . standard laboratory equipment in addition to . centrifuge . microplate spectrofluorimeter automated liquid handling workstation plate washer . reagents . standard laboratory reagents in addition to . mouse anti hepatitis b b4 antibody ( elisa ) mouse anti hepatitis b h35c16 antibody ( elisa ) . rat anti mouse kappa alkaline phosphatase conjugate antibody ( elisa ) hepatitis b reference standard ( elisa ) . blocking buffer ( tbs1 / % bsa / 0.05 % tween ) wash buffer ( tbs / 0.05 % tween ) . citrate buffer ( mm sodium citrate mm boric acid % bsa ph 9.0 ) mup ( methylumbelliferyl phosphate ) cocktail diluent . 1 tbs is mm tris and 0.9 % nacl . 4 mup concentrate 1m . preparation of standard . the reference standard monovalent hepatitis b vaccine recombinant is diluted to µg / ml and then pre treated with a solution of mm sodium citrate mm boric acid % bsa ph 9.0 ( citrate buffer ) to dissolve the alum . 125 µl of reference standard is combined with µl of the citrate buffer ( : dilution ) and heated at °c to °c for to min . after citration reference standard is diluted further with citrate buffer before loading onto an assay plate for a point serial dilution . preparation of samples . the hepatitis b protein concentration in pr5i test samples is µg / ml . for percent adsorption testing the total test samples are diluted volume to volume with mm tris 0.9 % nacl 0.05 % tween % bovine serum albumin ( blocking buffer ) to the hepatitis b protein concentration of µg / ml . then the total test samples are pre treated with citrate buffer the same way as the standard ( : dilution ) to dissolve the alum . 125 µl of pre diluted test sample are combined with µl of the citrate buffer ( : dilution ) and heated at °c to °c for to min . after pre \u001b[48;2;255;229;217m the plates are incubated at room temperature . \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Seq2seq-like Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
