{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 3 <br><br><br>\n",
    "  Language Modeling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. <font color=red>**Language Modeling**</font>\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) |\n",
    "\n",
    "# Overview\n",
    "\n",
    "\n",
    "Exemples d'implémentation en PyTorch :\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "\n",
    "\n",
    "Différentes architectures sont décrites dans la litérature :\n",
    "\n",
    "- Regularizing and Optimizing LSTM Language Models - https://arxiv.org/pdf/1708.02182.pdf\n",
    "\n",
    "Un modèle linguistique est intérressant en soi, mais peut aussi servir pour le pré-entrainement de couches basses d'un modèle plus complexe :\n",
    "\n",
    "- Deep contextualized word representations - https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Improving Language Understanding by Generative Pre-Training - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "- Language Models are Unsupervised Multitask Learners - https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\lib DL4NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importSheet(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importCorpus(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importSheet(file_name))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importCorpus(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = FastText(size = 75, \n",
    "                             window = 5, \n",
    "                             min_count = 1, \n",
    "                             negative = 20,\n",
    "                             sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8086"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fastText_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.train(sentences = corpus, \n",
    "                        epochs = 50,\n",
    "                        total_examples = fastText_word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(fastText_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = False)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, depth_range = (2, 10)) :\n",
    "        sentences = [s[i: i+j] \\\n",
    "                     for s in sentences \\\n",
    "                     for i in range(len(s)-depth_range[0]) \\\n",
    "                     for j in range(depth_range[0], min(depth_range[1], len(s)-i)+1) \\\n",
    "                    ]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack1 = Variable(torch.LongTensor([s[-1] for s in pack0]))\n",
    "            pack0 = [s[:-1] for s in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hidden[-1]), dim = 1)   # dim = (batch_size, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets   = batch[1].to(self.device).view(-1)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462138"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = LanguageModel(device,\n",
    "                               tokenizer = lambda s : s.split(' '),\n",
    "                               word2vec = word2vec,\n",
    "                               hidden_dim = 50, \n",
    "                               n_layers = 3, \n",
    "                               dropout = 0.1,\n",
    "                               optimizer = optim.SGD)\n",
    "\n",
    "language_model.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (word2vec): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(8088, 75)\n",
       "    )\n",
       "    (embedding): Embedding(8088, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (bigru): GRU(75, 50, num_layers=3, dropout=0.1)\n",
       "  )\n",
       "  (out): Linear(in_features=50, out_features=8088, bias=True)\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121513"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = language_model.generatePackedSentences(corpus, batch_size = 64, depth_range = (5, 20))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 27s (- 17m 41s) (500 2%) loss : 6.578  accuracy : 6.1 %\n",
      "0m 47s (- 14m 58s) (1000 5%) loss : 6.100  accuracy : 9.2 %\n",
      "1m 7s (- 13m 51s) (1500 7%) loss : 5.799  accuracy : 12.2 %\n",
      "1m 27s (- 13m 10s) (2000 10%) loss : 5.546  accuracy : 14.1 %\n",
      "1m 48s (- 12m 39s) (2500 12%) loss : 5.360  accuracy : 16.1 %\n",
      "2m 8s (- 12m 9s) (3000 15%) loss : 5.170  accuracy : 17.4 %\n",
      "2m 28s (- 11m 42s) (3500 17%) loss : 5.111  accuracy : 18.2 %\n",
      "2m 49s (- 11m 16s) (4000 20%) loss : 4.989  accuracy : 19.0 %\n",
      "3m 9s (- 10m 51s) (4500 22%) loss : 4.835  accuracy : 20.2 %\n",
      "3m 29s (- 10m 29s) (5000 25%) loss : 4.820  accuracy : 20.4 %\n",
      "3m 50s (- 10m 8s) (5500 27%) loss : 4.647  accuracy : 22.0 %\n",
      "4m 12s (- 9m 48s) (6000 30%) loss : 4.619  accuracy : 22.5 %\n",
      "4m 33s (- 9m 27s) (6500 32%) loss : 4.651  accuracy : 22.4 %\n",
      "4m 54s (- 9m 6s) (7000 35%) loss : 4.602  accuracy : 22.7 %\n",
      "5m 14s (- 8m 44s) (7500 37%) loss : 4.540  accuracy : 23.8 %\n",
      "5m 37s (- 8m 26s) (8000 40%) loss : 4.520  accuracy : 23.6 %\n",
      "5m 57s (- 8m 4s) (8500 42%) loss : 4.516  accuracy : 23.4 %\n",
      "6m 19s (- 7m 43s) (9000 45%) loss : 4.536  accuracy : 22.9 %\n",
      "6m 39s (- 7m 22s) (9500 47%) loss : 4.435  accuracy : 24.6 %\n",
      "7m 0s (- 7m 0s) (10000 50%) loss : 4.384  accuracy : 24.9 %\n",
      "7m 21s (- 6m 39s) (10500 52%) loss : 4.447  accuracy : 23.7 %\n",
      "7m 42s (- 6m 18s) (11000 55%) loss : 4.300  accuracy : 25.7 %\n",
      "8m 4s (- 5m 58s) (11500 57%) loss : 4.390  accuracy : 24.7 %\n",
      "8m 27s (- 5m 38s) (12000 60%) loss : 4.302  accuracy : 25.3 %\n",
      "8m 48s (- 5m 16s) (12500 62%) loss : 4.339  accuracy : 24.9 %\n",
      "9m 9s (- 4m 55s) (13000 65%) loss : 4.416  accuracy : 24.9 %\n",
      "9m 31s (- 4m 35s) (13500 67%) loss : 4.309  accuracy : 24.8 %\n",
      "9m 54s (- 4m 14s) (14000 70%) loss : 4.286  accuracy : 25.5 %\n",
      "10m 15s (- 3m 53s) (14500 72%) loss : 4.328  accuracy : 25.4 %\n",
      "10m 36s (- 3m 32s) (15000 75%) loss : 4.305  accuracy : 25.1 %\n",
      "10m 58s (- 3m 11s) (15500 77%) loss : 4.323  accuracy : 25.3 %\n",
      "11m 19s (- 2m 49s) (16000 80%) loss : 4.231  accuracy : 25.6 %\n",
      "11m 39s (- 2m 28s) (16500 82%) loss : 4.255  accuracy : 25.8 %\n",
      "12m 0s (- 2m 7s) (17000 85%) loss : 4.220  accuracy : 26.4 %\n",
      "12m 24s (- 1m 46s) (17500 87%) loss : 4.258  accuracy : 25.8 %\n",
      "12m 54s (- 1m 26s) (18000 90%) loss : 4.233  accuracy : 26.0 %\n",
      "13m 24s (- 1m 5s) (18500 92%) loss : 4.257  accuracy : 26.0 %\n",
      "13m 45s (- 0m 43s) (19000 95%) loss : 4.137  accuracy : 26.5 %\n",
      "14m 5s (- 0m 21s) (19500 97%) loss : 4.220  accuracy : 26.4 %\n",
      "14m 31s (- 0m 0s) (20000 100%) loss : 4.206  accuracy : 26.2 %\n",
      "0m 30s (- 19m 58s) (500 2%) loss : 4.050  accuracy : 28.0 %\n",
      "1m 1s (- 19m 31s) (1000 5%) loss : 4.134  accuracy : 27.8 %\n",
      "1m 33s (- 19m 10s) (1500 7%) loss : 4.003  accuracy : 29.0 %\n",
      "2m 4s (- 18m 36s) (2000 10%) loss : 4.021  accuracy : 28.9 %\n",
      "2m 35s (- 18m 8s) (2500 12%) loss : 4.031  accuracy : 28.4 %\n",
      "3m 7s (- 17m 45s) (3000 15%) loss : 4.011  accuracy : 29.0 %\n",
      "3m 39s (- 17m 14s) (3500 17%) loss : 4.024  accuracy : 29.0 %\n",
      "4m 11s (- 16m 47s) (4000 20%) loss : 4.112  accuracy : 27.8 %\n",
      "4m 43s (- 16m 16s) (4500 22%) loss : 4.022  accuracy : 28.8 %\n",
      "5m 15s (- 15m 45s) (5000 25%) loss : 4.019  accuracy : 28.9 %\n",
      "5m 46s (- 15m 12s) (5500 27%) loss : 4.054  accuracy : 28.8 %\n",
      "6m 19s (- 14m 45s) (6000 30%) loss : 3.983  accuracy : 29.2 %\n",
      "6m 50s (- 14m 11s) (6500 32%) loss : 4.018  accuracy : 28.7 %\n",
      "7m 18s (- 13m 34s) (7000 35%) loss : 3.995  accuracy : 29.1 %\n",
      "7m 51s (- 13m 6s) (7500 37%) loss : 4.016  accuracy : 28.6 %\n",
      "8m 24s (- 12m 36s) (8000 40%) loss : 4.068  accuracy : 28.1 %\n",
      "8m 56s (- 12m 5s) (8500 42%) loss : 3.908  accuracy : 30.1 %\n",
      "9m 28s (- 11m 34s) (9000 45%) loss : 3.966  accuracy : 30.0 %\n",
      "9m 59s (- 11m 2s) (9500 47%) loss : 3.985  accuracy : 29.1 %\n",
      "10m 31s (- 10m 31s) (10000 50%) loss : 4.012  accuracy : 28.6 %\n",
      "11m 2s (- 9m 59s) (10500 52%) loss : 3.918  accuracy : 30.0 %\n",
      "11m 37s (- 9m 30s) (11000 55%) loss : 3.946  accuracy : 29.4 %\n",
      "12m 2s (- 8m 53s) (11500 57%) loss : 3.931  accuracy : 30.1 %\n",
      "12m 23s (- 8m 15s) (12000 60%) loss : 3.975  accuracy : 29.2 %\n",
      "12m 45s (- 7m 39s) (12500 62%) loss : 3.929  accuracy : 29.6 %\n",
      "13m 6s (- 7m 3s) (13000 65%) loss : 3.964  accuracy : 29.3 %\n",
      "13m 26s (- 6m 28s) (13500 67%) loss : 3.926  accuracy : 29.6 %\n",
      "13m 46s (- 5m 54s) (14000 70%) loss : 3.827  accuracy : 30.7 %\n",
      "14m 6s (- 5m 20s) (14500 72%) loss : 3.907  accuracy : 30.0 %\n",
      "14m 25s (- 4m 48s) (15000 75%) loss : 3.936  accuracy : 29.9 %\n",
      "14m 46s (- 4m 17s) (15500 77%) loss : 4.005  accuracy : 28.9 %\n",
      "15m 6s (- 3m 46s) (16000 80%) loss : 3.914  accuracy : 29.8 %\n",
      "15m 26s (- 3m 16s) (16500 82%) loss : 3.929  accuracy : 29.6 %\n",
      "15m 45s (- 2m 46s) (17000 85%) loss : 3.941  accuracy : 29.6 %\n",
      "16m 6s (- 2m 18s) (17500 87%) loss : 3.953  accuracy : 29.6 %\n",
      "16m 25s (- 1m 49s) (18000 90%) loss : 3.943  accuracy : 29.3 %\n",
      "16m 45s (- 1m 21s) (18500 92%) loss : 3.932  accuracy : 29.7 %\n",
      "17m 6s (- 0m 54s) (19000 95%) loss : 3.985  accuracy : 29.2 %\n",
      "17m 26s (- 0m 26s) (19500 97%) loss : 3.974  accuracy : 29.3 %\n",
      "17m 46s (- 0m 0s) (20000 100%) loss : 3.823  accuracy : 31.1 %\n",
      "0m 20s (- 13m 7s) (500 2%) loss : 3.848  accuracy : 30.9 %\n",
      "0m 40s (- 12m 47s) (1000 5%) loss : 3.921  accuracy : 29.5 %\n",
      "1m 0s (- 12m 26s) (1500 7%) loss : 3.916  accuracy : 29.8 %\n",
      "1m 20s (- 12m 5s) (2000 10%) loss : 3.908  accuracy : 30.2 %\n",
      "1m 40s (- 11m 46s) (2500 12%) loss : 3.931  accuracy : 29.9 %\n",
      "2m 1s (- 11m 27s) (3000 15%) loss : 3.903  accuracy : 30.4 %\n",
      "2m 21s (- 11m 7s) (3500 17%) loss : 3.923  accuracy : 29.9 %\n",
      "2m 41s (- 10m 47s) (4000 20%) loss : 3.879  accuracy : 30.4 %\n",
      "3m 1s (- 10m 26s) (4500 22%) loss : 3.888  accuracy : 30.5 %\n",
      "3m 22s (- 10m 7s) (5000 25%) loss : 3.876  accuracy : 30.3 %\n",
      "3m 42s (- 9m 47s) (5500 27%) loss : 3.845  accuracy : 30.5 %\n",
      "4m 2s (- 9m 26s) (6000 30%) loss : 3.895  accuracy : 30.1 %\n",
      "4m 23s (- 9m 6s) (6500 32%) loss : 3.896  accuracy : 30.2 %\n",
      "4m 43s (- 8m 46s) (7000 35%) loss : 3.906  accuracy : 29.9 %\n",
      "5m 3s (- 8m 25s) (7500 37%) loss : 3.945  accuracy : 29.8 %\n",
      "5m 23s (- 8m 5s) (8000 40%) loss : 3.897  accuracy : 29.9 %\n",
      "5m 44s (- 7m 45s) (8500 42%) loss : 3.922  accuracy : 29.5 %\n",
      "6m 4s (- 7m 25s) (9000 45%) loss : 3.896  accuracy : 30.4 %\n",
      "6m 24s (- 7m 5s) (9500 47%) loss : 3.883  accuracy : 30.4 %\n",
      "6m 45s (- 6m 45s) (10000 50%) loss : 3.861  accuracy : 30.5 %\n",
      "7m 5s (- 6m 24s) (10500 52%) loss : 3.887  accuracy : 30.5 %\n",
      "7m 25s (- 6m 4s) (11000 55%) loss : 3.854  accuracy : 30.9 %\n",
      "7m 45s (- 5m 44s) (11500 57%) loss : 3.893  accuracy : 30.2 %\n",
      "8m 5s (- 5m 23s) (12000 60%) loss : 3.876  accuracy : 30.4 %\n",
      "8m 26s (- 5m 3s) (12500 62%) loss : 3.808  accuracy : 30.9 %\n",
      "8m 46s (- 4m 43s) (13000 65%) loss : 3.936  accuracy : 30.2 %\n",
      "9m 6s (- 4m 23s) (13500 67%) loss : 3.883  accuracy : 30.4 %\n",
      "9m 26s (- 4m 2s) (14000 70%) loss : 3.843  accuracy : 30.7 %\n",
      "9m 47s (- 3m 42s) (14500 72%) loss : 3.851  accuracy : 30.9 %\n",
      "10m 7s (- 3m 22s) (15000 75%) loss : 3.899  accuracy : 30.3 %\n",
      "10m 27s (- 3m 2s) (15500 77%) loss : 4.003  accuracy : 29.0 %\n",
      "10m 47s (- 2m 41s) (16000 80%) loss : 3.875  accuracy : 30.4 %\n",
      "11m 7s (- 2m 21s) (16500 82%) loss : 3.867  accuracy : 30.6 %\n",
      "11m 28s (- 2m 1s) (17000 85%) loss : 3.870  accuracy : 30.9 %\n",
      "11m 48s (- 1m 41s) (17500 87%) loss : 3.870  accuracy : 30.5 %\n",
      "12m 8s (- 1m 20s) (18000 90%) loss : 3.881  accuracy : 30.5 %\n",
      "12m 28s (- 1m 0s) (18500 92%) loss : 3.854  accuracy : 30.9 %\n",
      "12m 49s (- 0m 40s) (19000 95%) loss : 3.924  accuracy : 29.6 %\n",
      "13m 9s (- 0m 20s) (19500 97%) loss : 3.802  accuracy : 31.6 %\n",
      "13m 29s (- 0m 0s) (20000 100%) loss : 3.887  accuracy : 30.7 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches, iters = 20000, lr = 0.01, print_every = 500)\n",
    "language_model.fit(batches, iters = 20000, lr = 0.0025, print_every = 500)\n",
    "language_model.fit(batches, iters = 20000, lr = 0.0005, print_every = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "#torch.save(language_model.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I3_language_model.pth')\n",
    "\n",
    "# load\n",
    "language_model.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I3_language_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". section 3.2.s.4.3 validation of analytical procedures . the analytical procedures used for the control tests carried out on the drug substance are described in 3.2.s.4.2 analytical procedures . validation data of compendial general methods ( described in european pharmacopoeia ) are not provided . validation data for other analytical procedures are summarized hereafter . protein content . the procedure of protein content using lowry ' s method in purified vi polysaccharide was validated in accordance with ich q2 ( r1 ) \" validation of analytical procedures : text and methodology \" . since the method is a quantitative assay characteristics studied are specificity linearity accuracy precision and lower quantification limit . a summary of the validation is provided in table 1 . table : protein content in purified vi polysaccharide summary of the validation . principle . the addition of a copper salt in an alkaline medium followed by folin ciocalteu reagent to a protein solution gives a dark blue colour the intensity of which is measured by spectrophotometry at nm . specificity . three independent runs were performed by three operators on three different days . for specificity spike levels of albumin reference standard are used for calculation ( spikes at µg and at µg ) . the data subjected to analysis is the concentration of proteins expressed in µg / ml . the \u001b[48;2;255;229;217m results obtained for the linearity assessment are presented in table 1 . \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# fastText gensim, n_layers = 3, dh = 50\n",
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
