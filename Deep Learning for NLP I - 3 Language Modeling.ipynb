{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 3 <br><br><br>\n",
    "  Language Modeling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "    _Applications :_\n",
    "    \n",
    "    - Extractive Summarization\n",
    "    - Sentiment Analysis\n",
    "    - Text segmentation\n",
    "\n",
    "\n",
    "3. <font color=red>**Language Modeling**</font>\n",
    "\n",
    "4. Sentence tagging\n",
    "\n",
    "    _Applications :_\n",
    "    \n",
    "    - Part-of-speech Tagging\n",
    "    - Named Entity Recognition\n",
    "    - Automatic Value Extraction\n",
    "    \n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global structure of the [language model](#language_model) is the pipeline of two modules, followed by a final classification layer :\n",
    "\n",
    "\n",
    "\n",
    "| | Module |  | |\n",
    "|------|------|------|------|\n",
    "| 1 | **Word Embedding** | [I.1 Custom model](#word_level_custom) | [I.2 Gensim Model](#gensim) | [I.3 FastText model](#fastText) |\n",
    "| 2 | **Contextualization** | [II.1 bidirectionnal GRU](#bi_gru) | [II.2 Transformer](#transformer) |\n",
    "| 3 | **Attention** | [III.1 Attention](#attention) | [III.2 Multi-head Attention](#attention) |\n",
    "\n",
    "\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**.\n",
    "\n",
    "Exemples d'implémentation en PyTorch :\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "\n",
    "\n",
    "Différentes architectures sont décrites dans la litérature :\n",
    "\n",
    "- Regularizing and Optimizing LSTM Language Models - https://arxiv.org/pdf/1708.02182.pdf\n",
    "\n",
    "Un modèle linguistique est intérressant en soi, mais peut aussi servir pour le pré-entrainement de couches basses d'un modèle plus complexe :\n",
    "\n",
    "- Deep contextualized word representations - https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Improving Language Understanding by Generative Pre-Training - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "- Language Models are Unsupervised Multitask Learners - https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 0.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\chatNLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus et donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text, label]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importSheet(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importCorpus(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importSheet(file_name))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importCorpus(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "## 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from chatNLP.models.Word_Embedding import Word2VecConnector\n",
    "from chatNLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = FastText(size = 75, \n",
    "                             window = 5, \n",
    "                             min_count = 1, \n",
    "                             negative = 20,\n",
    "                             sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8086"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fastText_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.train(sentences = corpus, \n",
    "                        epochs = 50,\n",
    "                        total_examples = fastText_word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(fastText_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "<a id=\"bi_gru\"></a>\n",
    "\n",
    "#### 1.2.1 Bi-directionnal GRU contextualization\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"language_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, word2vec, hidden_dim, n_layers, dropout = 0, class_weights = None, optimizer = optim.SGD) :\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = False)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = sentence.split(' ')\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1, :, :]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, max_depht = 30) :\n",
    "        sentences = [s[i: i+j] for s in sentences for i in range(len(s)-2) for j in range(2, min(max_depht, len(s)-i)+1)]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack1 = Variable(torch.LongTensor([s[-1] for s in pack0]))\n",
    "            pack0 = [s[:-1] for s in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hidden[-1, :, :]), dim = 1)   # dim = (batch_size, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets   = batch[1].to(self.device).view(-1)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.data[0] / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462138"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = LanguageModel(device,\n",
    "                               word2vec,\n",
    "                               hidden_dim = 50, \n",
    "                               n_layers = 3, \n",
    "                               dropout = 0.1,\n",
    "                               optimizer = optim.SGD)\n",
    "\n",
    "language_model.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (word2vec): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(8088, 75)\n",
       "    )\n",
       "    (embedding): Embedding(8088, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (bigru): GRU(75, 50, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  )\n",
       "  (out): Linear(in_features=50, out_features=8088, bias=True)\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = language_model.generatePackedSentences(corpus, batch_size = 64, max_depht = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106768"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 16s (- 10m 44s) (500 2%) loss : 6.602  accuracy : 5.9 %\n",
      "0m 32s (- 10m 21s) (1000 5%) loss : 6.300  accuracy : 5.9 %\n",
      "0m 48s (- 10m 2s) (1500 7%) loss : 6.080  accuracy : 10.3 %\n",
      "1m 5s (- 9m 45s) (2000 10%) loss : 5.872  accuracy : 11.8 %\n",
      "1m 21s (- 9m 28s) (2500 12%) loss : 5.669  accuracy : 12.9 %\n",
      "1m 37s (- 9m 11s) (3000 15%) loss : 5.481  accuracy : 14.5 %\n",
      "1m 53s (- 8m 54s) (3500 17%) loss : 5.346  accuracy : 15.9 %\n",
      "2m 9s (- 8m 37s) (4000 20%) loss : 5.279  accuracy : 16.7 %\n",
      "2m 25s (- 8m 20s) (4500 22%) loss : 5.086  accuracy : 17.9 %\n",
      "2m 41s (- 8m 3s) (5000 25%) loss : 4.999  accuracy : 18.7 %\n",
      "2m 57s (- 7m 47s) (5500 27%) loss : 4.892  accuracy : 19.9 %\n",
      "3m 13s (- 7m 31s) (6000 30%) loss : 4.863  accuracy : 20.0 %\n",
      "3m 29s (- 7m 14s) (6500 32%) loss : 4.803  accuracy : 20.3 %\n",
      "3m 45s (- 6m 58s) (7000 35%) loss : 4.780  accuracy : 21.1 %\n",
      "4m 1s (- 6m 41s) (7500 37%) loss : 4.792  accuracy : 20.8 %\n",
      "4m 17s (- 6m 25s) (8000 40%) loss : 4.675  accuracy : 22.2 %\n",
      "4m 34s (- 6m 10s) (8500 42%) loss : 4.698  accuracy : 21.8 %\n",
      "4m 51s (- 5m 56s) (9000 45%) loss : 4.613  accuracy : 22.2 %\n",
      "5m 8s (- 5m 40s) (9500 47%) loss : 4.590  accuracy : 23.0 %\n",
      "5m 25s (- 5m 25s) (10000 50%) loss : 4.499  accuracy : 23.6 %\n",
      "5m 42s (- 5m 10s) (10500 52%) loss : 4.507  accuracy : 23.8 %\n",
      "6m 0s (- 4m 54s) (11000 55%) loss : 4.531  accuracy : 23.1 %\n",
      "6m 17s (- 4m 39s) (11500 57%) loss : 4.528  accuracy : 23.1 %\n",
      "6m 34s (- 4m 23s) (12000 60%) loss : 4.498  accuracy : 23.7 %\n",
      "6m 52s (- 4m 7s) (12500 62%) loss : 4.404  accuracy : 24.2 %\n",
      "7m 9s (- 3m 51s) (13000 65%) loss : 4.409  accuracy : 24.2 %\n",
      "7m 26s (- 3m 34s) (13500 67%) loss : 4.410  accuracy : 24.7 %\n",
      "7m 42s (- 3m 18s) (14000 70%) loss : 4.449  accuracy : 24.5 %\n",
      "7m 58s (- 3m 1s) (14500 72%) loss : 4.326  accuracy : 25.1 %\n",
      "8m 14s (- 2m 44s) (15000 75%) loss : 4.372  accuracy : 24.5 %\n",
      "8m 30s (- 2m 28s) (15500 77%) loss : 4.331  accuracy : 25.2 %\n",
      "8m 46s (- 2m 11s) (16000 80%) loss : 4.395  accuracy : 24.3 %\n",
      "9m 2s (- 1m 55s) (16500 82%) loss : 4.358  accuracy : 24.6 %\n",
      "9m 18s (- 1m 38s) (17000 85%) loss : 4.285  accuracy : 25.2 %\n",
      "9m 35s (- 1m 22s) (17500 87%) loss : 4.410  accuracy : 24.3 %\n",
      "9m 51s (- 1m 5s) (18000 90%) loss : 4.365  accuracy : 24.6 %\n",
      "10m 7s (- 0m 49s) (18500 92%) loss : 4.325  accuracy : 25.1 %\n",
      "10m 23s (- 0m 32s) (19000 95%) loss : 4.274  accuracy : 25.3 %\n",
      "10m 39s (- 0m 16s) (19500 97%) loss : 4.222  accuracy : 25.9 %\n",
      "10m 55s (- 0m 0s) (20000 100%) loss : 4.287  accuracy : 25.3 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches, iters = 20000, lr = 0.01, print_every = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 16s (- 10m 43s) (500 2%) loss : 4.203  accuracy : 26.7 %\n",
      "0m 32s (- 10m 13s) (1000 5%) loss : 4.135  accuracy : 27.2 %\n",
      "0m 48s (- 9m 52s) (1500 7%) loss : 4.174  accuracy : 27.0 %\n",
      "1m 3s (- 9m 33s) (2000 10%) loss : 4.134  accuracy : 27.4 %\n",
      "1m 19s (- 9m 18s) (2500 12%) loss : 4.100  accuracy : 28.3 %\n",
      "1m 35s (- 9m 0s) (3000 15%) loss : 4.154  accuracy : 27.5 %\n",
      "1m 51s (- 8m 45s) (3500 17%) loss : 4.074  accuracy : 28.0 %\n",
      "2m 7s (- 8m 29s) (4000 20%) loss : 4.115  accuracy : 27.5 %\n",
      "2m 23s (- 8m 14s) (4500 22%) loss : 4.157  accuracy : 26.6 %\n",
      "2m 39s (- 7m 58s) (5000 25%) loss : 4.091  accuracy : 27.8 %\n",
      "2m 55s (- 7m 41s) (5500 27%) loss : 4.120  accuracy : 27.8 %\n",
      "3m 10s (- 7m 24s) (6000 30%) loss : 4.105  accuracy : 27.9 %\n",
      "3m 26s (- 7m 8s) (6500 32%) loss : 4.087  accuracy : 28.1 %\n",
      "3m 41s (- 6m 52s) (7000 35%) loss : 4.049  accuracy : 28.4 %\n",
      "3m 57s (- 6m 36s) (7500 37%) loss : 4.108  accuracy : 27.9 %\n",
      "4m 13s (- 6m 20s) (8000 40%) loss : 4.149  accuracy : 27.3 %\n",
      "4m 29s (- 6m 4s) (8500 42%) loss : 4.103  accuracy : 27.6 %\n",
      "4m 44s (- 5m 48s) (9000 45%) loss : 4.109  accuracy : 27.7 %\n",
      "5m 0s (- 5m 32s) (9500 47%) loss : 4.001  accuracy : 29.4 %\n",
      "5m 16s (- 5m 16s) (10000 50%) loss : 4.081  accuracy : 27.8 %\n",
      "5m 31s (- 5m 0s) (10500 52%) loss : 4.074  accuracy : 28.4 %\n",
      "5m 47s (- 4m 44s) (11000 55%) loss : 4.147  accuracy : 27.5 %\n",
      "6m 3s (- 4m 28s) (11500 57%) loss : 4.059  accuracy : 28.4 %\n",
      "6m 19s (- 4m 12s) (12000 60%) loss : 4.044  accuracy : 28.6 %\n",
      "6m 35s (- 3m 57s) (12500 62%) loss : 4.111  accuracy : 28.3 %\n",
      "6m 50s (- 3m 41s) (13000 65%) loss : 4.047  accuracy : 28.8 %\n",
      "7m 7s (- 3m 25s) (13500 67%) loss : 4.126  accuracy : 27.9 %\n",
      "7m 23s (- 3m 9s) (14000 70%) loss : 4.117  accuracy : 27.8 %\n",
      "7m 39s (- 2m 54s) (14500 72%) loss : 4.099  accuracy : 27.7 %\n",
      "7m 54s (- 2m 38s) (15000 75%) loss : 4.052  accuracy : 28.5 %\n",
      "8m 10s (- 2m 22s) (15500 77%) loss : 4.015  accuracy : 28.5 %\n",
      "8m 27s (- 2m 6s) (16000 80%) loss : 4.056  accuracy : 28.6 %\n",
      "8m 44s (- 1m 51s) (16500 82%) loss : 4.105  accuracy : 27.8 %\n",
      "9m 1s (- 1m 35s) (17000 85%) loss : 4.042  accuracy : 28.9 %\n",
      "9m 18s (- 1m 19s) (17500 87%) loss : 4.028  accuracy : 28.7 %\n",
      "9m 34s (- 1m 3s) (18000 90%) loss : 4.048  accuracy : 28.1 %\n",
      "9m 52s (- 0m 48s) (18500 92%) loss : 4.052  accuracy : 28.4 %\n",
      "10m 8s (- 0m 32s) (19000 95%) loss : 4.045  accuracy : 28.2 %\n",
      "10m 24s (- 0m 16s) (19500 97%) loss : 4.032  accuracy : 28.4 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches, iters = 20000, lr = 0.0025, print_every = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "language_model.fit(batches, iters = 20000, lr = 0.0005, print_every = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". introduction . this section provides a detailed summary of the validation reports of the analytical methods used to release the quadrivalent influenza vaccine ( qiv ) multidose vial presentation at final bulk product ( fbp ) stage and filled product ( fp ) stage when the test is not performed in compliance with the ph eur or when the test is not detailed in the ph eur . compendial tests detailed in the european pharmacopeia . the bacterial and fungal sterility test is performed by membrane filtration in compliance with . ph eur 2.6.1 current edition \" sterility \" . this test was validated according to the requirements of the ph eur ( validation report available but not provided ) . overview . the free formaldehyde content is determined by a colorimetric assay according to nash method . with acetyl acetone and in the presence of excess ammonia salts formaldehyde gives the . 3.5 diacetyl 1.4 dihydrolutidine a yellow compound which is measured photometrically at nm ( absorption peak ) . the intensity of the color is proportional to the quantity of formaldehyde in the sample . since the test method for the determination of free formaldehyde content is a quantitative assay of an impurity the studied characteristics are specificity precision ( intermediate precision and repeatability ) linearity accuracy and quantitation limit ( ql ) . the validation is performed on fbp batch fdnc3386 . the analytical procedure and the treatment of the sample test are the same for fbp and fp of the qiv multidose vials . the composition of the sample test is the same . the only difference between the two stages is the filling step . in consequence the validation results can be applied to the fbp and to the fp stages . summary of validation . a summary of the validation results is provided in table 1 . table : free formaldehyde content validation summary . the method is valid to quantify the residual formaldehyde in the qiv multidose vial presentation at the fbp and fp stages on the range [ 0.42 76.47 ] µg / ml . specificity . the specificity is evaluated from the results obtained on the spiked solutions ( level and ) of the linearity study : the quantity of measured spiked formaldehyde is compared to the theoretical quantity of spiked formaldehyde . the measured quantity of spiked formaldehyde is calculated as the difference between the concentration of formaldehyde obtained during the analysis of the spiked vaccine and the concentration of formaldehyde obtained during the analysis of the unspiked vaccine . the theoretical quantity of spiked formaldehyde is calculated as the difference between the theoretical concentration of formaldehyde at the point with spike and the theoretical concentration at the point without spike . the percent recoveries are calculated as follows . the results subject to analysis are the concentrations of residual formaldehyde found during the linearity study expressed in µg / ml and are presented in table 2 . table : specificity : measured concentrations vs theoretical concentrations of formaldehyde content ( µg / ml ) . the percent recoveries calculated between the measured added formaldehyde and theoretical added formaldehyde are presented in table 3 . table : specificity percent recoveries calculated between the measured added formaldehyde and the theoretical added formaldehyde . the average recovery of each spiked level is included between % and % . the specificity is satisfactory . precision . the experimental design was as follows . intermediate precision conditions : operators carried out series of tests in the same laboratory on different days and using different preparations of the reference . repeatability conditions : each operator carried out independent assays ( independent preparation of the samples ) using the same equipment within a short range of time and using the same preparation of the reference . the residual formaldehyde concentration results expressed in µg / ml used for the precision assessment are presented in the table below . table : precision : concentrations of residual formaldehyde ( µg / ml ) . all calculations are carried out in logarithms since the distribution of the data is log normal . the precision of the method is tested through the following steps applied to data from table . the homogeneity of the variances of the series is verified by cochran's test . when acquired the homogeneity the characteristics of repeatability and intermediate precision are calculated . the results of the statistical analysis are . cochran's test shows that the variances of the series are homogeneous . overall mean 1.481 log ( µg / ml ) that is 30.26 µg / ml in arithmetic form ; the results for precision are presented in table 5 . table : precision : repeatability and intermediate precision characteristics free formaldehyde content . the % confidence interval of intermediate precision is lower than x / ÷ 1.20 . the precision is satisfactory . linearity . for linearity tests were performed by operators on different days within the same laboratory and using different preparations of the reference ( intermediate precision conditions ) . each operator performed a range of concentration levels of residual formaldehyde using the same preparation of the reference . ( diluted vaccines undiluted vaccine and spiked vaccines ) . the formaldehyde concentration results expressed in µg / ml used for the linearity assessment are presented in the table below . table : linearity theoretical and measured formaldehyde concentrations ( µg / ml ) . the theoretical concentrations of residual formaldehyde are calculated from the mean of the precision assays according to the following formulas . if qt dil 0 . if qt dil 0 . where . ctheo : theoretical concentration in µg / ml . qt vac : quantity of vaccine in ml ( volume of the pure qiv fbp for dilution ) . mean : mean of the precision assays in µg / ml that is 30.257 µg / ml . qt spike : quantity of the spiking solution of formaldehyde in ml . c spike : concentration of the spiking solution of formaldehyde that is ml . qt dil : quantity of the diluent in ml ( sufficient volume of water for injection ( wfi ) for fbp dilution ) . the linearity over the chosen range is tested through the following steps applied to the data in table 6 . homogeneity of the variances is verified by cochran's test . the dependence between the theoretical expected concentration of residual formaldehyde and measured concentration of residual formaldehyde and the linearity of this relation are tested by an unweighted linear regression using least squares method . a significant slope and a . non significant deviation from linearity must be shown . as a result . the cochran's test was at the limit of significance . a slight heteroscedasticity is accepted . the dependence between the theoretical and measured residual formaldehyde concentrations and the linearity of this relation were verified by an unweighted linear regression using least squares method . a significant slope at the % level and a non significant deviation from linearity at the % level were shown . pregression < 0.0001 . pdeviation from linearity 0.06 . the results of the statistical analysis and the equation of the regression line are presented in table 7 . the linearity graphic is presented in figure 1 . table : linearity : equation of the straight regression line . measured concentration ( microg / ml ) measured concentration ( microg / ml ) figure : linearity graph formaldehyde concentration . theoretical concentration ( microg / ml ) theoretical concentration ( microg / ml ) pregression is less than 0.01 and pdeviation from linearity is greater than 0.05 . the linearity is satisfactory . accuracy . accuracy is evaluated from the results obtained for the linearity study on the diluted and spiked solutions . the percent recoveries are calculated by the division of the measured concentrations by the theoretical concentrations and are presented in table 8 . table : accuracy percent recoveries . the average percent recovery is included between % and % for all concentration levels . the accuracy is satisfactory . quantitation limit . the quantitation limit ( ql ) was determined by calculating the mean and standard deviation from the optical density ( od ) measures of the blank ( without log transformation ) . the value \" mean + * standard deviation \" was compared to the mean value of the od of the first point of the reference standard range . if the value \" mean + * standard deviation \" is inferior to the od of the first point of the reference standard range then the ql is the first point of the reference standard range . the ql was determined from linear regression curves and od measures of the blank . the tests were performed by different operators on different days . the results are summarized below . the mean of the od measures of the blank ( without log transformation ) is : 0.0031 . the standard deviation of the od measures of the blank ( without log transformation ) is : 0.0035 . the value \" mean + * standard deviation \" is : 0.0385 . the mean value of the od of the first point of the reference standard range is : 0.0462 . the value \" mean + * standard deviation \" is inferior to the mean value of the od of the first point of the reference standard range . as the concentration associated with the first point of the reference standard range corresponds to . 0.63 µg of formaldehyde in the sample test and taking into account the volume of the sample test ( ml ) then . ql 0.315 µg / ml ( 0.63 / ) . the precision and the accuracy of the ql have been assessed . 9 independent assays were carried out by operators in the same laboratory on different days at a concentration close to the ql . each assay is carried independently ( independent preparation of the sample and of the reference standard range ) . the percent recoveries are calculated by the division of the measured dilutions by the theoretical dilutions and are presented in table 9 . table : accuracy percent recoveries for quantitation limit . * geometric mean of the linearity assays corresponding to % . the results of the analysis for precision are presented in table and for accuracy in table 11 . table : precision characteristics quantitation limit . : : the % confidence interval of the intermediate precision at the point of the linearity range close to the ql is : x 1.11 . the precision of the measured values close to the ql is verified . table : accuracy characteristics quantitation limit . the average recovery at the point of the linearity range close to the ql is : 104.9 % [ 101.4 % 108.4 % ] . the accuracy of the measured values close to the ql is verified . 2.2.1.4 conclusion . the method is valid to quantify the residual formaldehyde in the qiv multidose vial presentation at the fbp and fp stages on the range [ 0.42 76.47 ] µg / ml . overview . the proteins are precipitated by a sulfo tungstic reagent then the precipitate is collected . during the sulfuric digestion organic nitrogen is transformed into ammonium sulfate . after alkalinisation by sodium hydroxide distilled ammonia is recovered and then assayed by a titrated solution of hydrochloric acid . since the method is a quantitative assay the following validation characteristics were assessed : precision ( intermediate precision and repeatability ) linearity and accuracy . the specificity was not evaluated during this validation as the total protein test ( nitrogen determination after sulfuric mineralization ) is based upon the ph eur 2.5.9 current edition . this method is specific to nitrogen determination . the validation is performed on fbp batch fdnc3386 . the analytical procedure and the treatment of the sample test are the same for fbp and fp of the qiv multidose vials . the composition of the sample test is the same . the only difference between the two stages is the filling step . in consequence the validation results can be applied to the fbp and to the fp stages . summary of validation . a summary of the validation results is provided in table 12 . table : total protein content validation summary . the method is valid to quantify the total protein content in the qiv multidose vial presentation on the fbp and fp stages on the range [ ] µg / ml . precision . the experimental design was as follows . intermediate precision conditions : operators carried out series of tests in the same laboratory on different days . repeatability conditions : each operator carried out independent assays ( independent preparation of the sample ) using the same equipment within a short range of time . the total protein content results expressed in µg / ml used for the precision assessment are presented in the table 13 . table : precision : total protein content ( µg / ml ) . the precision of the method is tested through the following steps applied to data from table 13 . according to statistical analysis performed on the data from table 13 . the homogeneity of the variances of the series is verified by cochran's test . when acquired the homogeneity the characteristics of repeatability and intermediate precision are calculated . the results of the statistical analysis are . cochran's test shows that the variances of the series are homogeneous . overall mean 287.874 µg / ml . the results for precision are presented in table 14 . table : repeatability and intermediate precision characteristics total protein content . the % confidence interval of intermediate precision is lower than ± µg / ml . the precision is satisfactory . linearity . for linearity tests were performed by operators on different days within the same laboratory ( intermediate precision conditions ) . each operator performed a range of concentration levels of total protein . the results obtained for the linearity assessment are presented in table 15 . table : linearity theoretical and measured total protein concentrations ( µg / ml ) . invalid : volume difference between the two tests > 0.12 ml . the theoretical concentrations of total protein are calculated from the mean of the precision assays according to the following formula . where . ctheo : theoretical concentration in µg / ml . qt vac : quantity of vaccine in ml ( volume of the pure qiv fbp for dilution ) . mean : mean of the precision assays in µg / ml that is 287.874 µg / ml . the linearity over the chosen range is tested through the following steps applied to the data in table 15 . homogeneity of the variances is verified by cochran's test . the dependence between the theoretical expected concentration of total protein and measured concentration of total protein and the linearity of this relation are tested by an unweighted linear regression using least squares method . a significant slope and a non significant deviation from linearity must be shown . as a result . the cochran's test was not significant . the hypothesis of homogeneity of variances is not rejected . the variances of the concentration levels are considered homogeneous . the dependence between the theoretical and measured total protein concentrations and the linearity of this relation were verified by an unweighted linear regression using least squares method . a significant slope at the % level and a non significant deviation from linearity at the % level were shown . pregression < 0.0001 . pdeviation from linearity 0.42 . the results of the statistical analysis and the equation of the regression line are presented in table 16 . the linearity graphic is presented in figure 2 . table : linearity : equation of the straight regression line . measured concentration ( microg / ml ) measured concentration ( microg / ml ) figure : linearity graph protein concentration . theoretical concentration ( microg / ml ) theoretical concentration ( microg / ml ) pregression is less than 0.01 and pdeviation from linearity is greater than 0.05 . the linearity is satisfactory . accuracy . accuracy is evaluated from the results obtained for the linearity study on the diluted and spiked solutions . the percent recoveries are calculated by the division of the measured concentrations by the theoretical concentrations and are presented in table 17 . table : accuracy percent recoveries for total protein content . * invalid : volume difference between the two tests > 0.12 ml . the average percent recovery is included between % and % for all concentration levels . the accuracy is satisfactory . conclusion . the method is valid to quantify the total protein content in the qiv multidose vial presentation on the fbp and fp stages on the range [ ] µg / ml . overview . the ovalbumin content of the fbp is assessed by an enzyme linked immunosorbent \" sandwich \" assay ( elisa ) in comparison to a reference solution of chicken egg albumin ( ovalbumin ) . the ovalbumin present in the samples is captured by a goat anti ovalbumin immunoglobulin g ( igg ) coated on a microplate . a second antibody ( rabbit anti ovalbumin igg ) is fixed to the immobilized antigen . a conjugate ( goat anti rabbit igg conjugated with alkaline phosphatase ) is added and fixed to the complex antibody antigen second antibody . detection is performed by adding a chromogen solution ( pnpp ) . after incubation the color intensity developed is proportional to the amount of ovalbumin contained in the sample . the enzyme reaction is ended by the addition of sodium hydroxide and the absorbance is determined by a spectrophotometer at nm and nm . a standard curve is established from a reference ovalbumin solution . since the method is a quantitative assay of an impurity the parameters studied were specificity precision ( intermediate precision and repeatability ) linearity accuracy and ql . the validation is performed on fbp batch fdnc3386 . the analytical procedure and the treatment of the sample test are the same for fbp and fp of the qiv multidose vials . the composition of the sample test is the same . the only difference between the two stages is the filling step . in consequence the validation results can be applied to the fbp and to the fp stages . summary of validation . a summary of the validation results is provided in table 18 . table : ovalbumin content validation summary . the method is valid to quantify the ovalbumin content on the fbp and fp stages on the range [ ] ng / ml . specificity . the experimental design was as follows ( two studies were carried out ) . a study on spiked samples : this characteristic was evaluated from the results obtained during the linearity study on the spiked samples . the quantity of measured ovalbumin is compared to the theoretical added quantity : a percent recovery is obtained . the quantity of measured ovalbumin is calculated as the difference between the concentration measured at the spiked point studied and the concentration measured at the unspiked point . the quantity of theoretical spiked ovalbumin is calculated as the difference between the theoretical concentration at the spiked point and the theoretical concentration at the unspiked point ( i e the average of intermediate precision ) . the calculation of the recoveries is summarized in the equation below . a study on the test diluent containing no ovalbumin : the test diluent is assayed once independently by three operators . results on the spiked samples . the percent recoveries are presented in table 19 . table : specificity percent recoveries calculated between the measured added ovalbumin and theoretical added ovalbumin . * additional testing carried out since the average percent recovery was less than % at the concentration level 5 . the average recovery of the last concentration level is less than % . however since the average recoveries of the first and the second concentration levels are between % and % . the second concentration level ; i e ng / ml is superior to the specification the specificity is satisfactory . results on the diluent . the following results were obtained . table : specificity testing results on the diluent . for the series the test results on the diluent are lower than the first point of the linearity range . i e the quantitation limit . the specificity is satisfactory . precision . the experimental design was as follows . intermediate precision conditions : operators carried out series of tests in the same laboratory on different days and using different preparations of the reference . repeatability conditions : each operator carried out independent assays ( independent preparation of the sample ) using the same equipment within a short range of time and using the same preparation of the reference . the ovalbumin content results expressed in ng / ml used for the precision assessment are presented in the table 21 . table : precision measured concentrations of ovalbumin ( ng / ml ) . the precision of the method is tested through the following steps applied to data from table 21 . the homogeneity of the variances of the series is verified by cochran's test . when acquired the homogeneity the characteristics of repeatability and intermediate precision are calculated . the results of the statistical analysis are . cochran's test shows that the variances of the series are homogeneous . overall mean 1.06 log ( ng / ml ) that is 11.46 ng / ml in arithmetic form . the results for precision are presented in table 22 . table : repeatability and intermediate precision characteristics ovalbumin concentration . the % confidence interval of intermediate precision is lower than x / ÷ 1.50 . the precision is satisfactory . linearity . for linearity tests were performed by operators on different days within the same laboratory and using different preparations of the reference and the sample ( intermediate precision conditions ) . each operator performed a range of concentration levels of ovalbumin using the same preparation of the reference . ( diluted vaccines undiluted vaccine and spiked vaccines ) . the results obtained for the linearity assessment are presented in table 23 . table : linearity theoretical and measured ovalbumin concentrations ( ng \u001b[48;2;255;229;217m / ml ) . \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
