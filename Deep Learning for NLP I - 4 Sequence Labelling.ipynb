{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 4 <br><br><br>\n",
    "  Sequence Labelling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=red>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "We consider as Sequence labelling task a **Sentence Denoising** problem, which consists in transforming a noisy sequence of words into a correctly formed sentence.<br> Training follows a denoising objective known as _Cloze task_, which is used :\n",
    "\n",
    "- For the BERT model in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\libDL4NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importWords(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importAllWords(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importWords(file_name))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def importSentences(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        sentences = []\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                sentences.append(cleanSentence(current_tile))\n",
    "                title = current_tile\n",
    "            sentences.append(cleanSentence(str(pair[1]).split(' | ')[-1]))\n",
    "        return sentences\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    sentences = cleanDatabase(db)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def importAllSentences(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus += importSentences(file_name)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importAllWords(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31574"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = importAllSentences(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'test',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'assessing',\n",
       " 'the',\n",
       " 'dissolution',\n",
       " 'time',\n",
       " 'of',\n",
       " 'the',\n",
       " 'freeze',\n",
       " 'dried',\n",
       " 'yellow',\n",
       " 'fever',\n",
       " 'vaccine',\n",
       " 'after',\n",
       " 'adding',\n",
       " 'the',\n",
       " 'suitable',\n",
       " 'diluent',\n",
       " 'directly',\n",
       " 'into',\n",
       " 'the',\n",
       " 'original',\n",
       " 'container']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = FastText(size = 75, \n",
    "                    window = 5, \n",
    "                    min_count = 3, \n",
    "                    negative = 20,\n",
    "                    sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4662"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.train(sentences = sentences, \n",
    "               epochs = 50,\n",
    "               total_examples = word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentence denoising Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDenoiser(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SentenceDenoiser, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, lang_size)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length, lang_size)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        new_ws = [self.word2vec.lang.index2word[ind] for ind in inds]\n",
    "        print(' '.join([addColor(w1, w2, color) for w1, w2 in zip(words, new_ws)]))\n",
    "        return\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0.15,\n",
    "                                predict_masked_only = True,\n",
    "                                max_sentence_length = 50,\n",
    "                                tol = 10,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index\n",
    "        \n",
    "        def splitLongs(words, threshold = 50, tol = 10):\n",
    "            news = []\n",
    "            for i in range(0, len(words), threshold) :\n",
    "                if len(words)-i-threshold > tol : \n",
    "                    news.append(words[i : i + threshold])\n",
    "                else : \n",
    "                    news.append(words[i:])\n",
    "                    break\n",
    "            return news\n",
    "        \n",
    "        packed_data = []\n",
    "        random.seed(seed)\n",
    "        # prepare sentences\n",
    "        #sentences = [self.tokenizer(s) for s in sentences]\n",
    "        sentences = [[self.word2vec.lang.getIndex(w) for w in s] for s in sentences]\n",
    "        sentences = [[w for w in words if w is not None] for words in sentences]\n",
    "        sentences = [s for S in sentences for s in splitLongs(S, max_sentence_length, tol) if len([w for w in s if w != self.word2vec.lang.getIndex('UNK')]) > 1]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        # collect packs\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            # prepare mask\n",
    "            mask = [[i for i, w in enumerate(p) if w != self.word2vec.lang.getIndex('UNK')] for p in pack]\n",
    "            mask = [random.sample(m, k = int(mask_ratio*len(m) +1)) for m in mask]\n",
    "            # prepare input and target packs\n",
    "            pack0 = [[ maskInput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)]\n",
    "            pack1 = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)] \\\n",
    "                    if predict_masked_only else \\\n",
    "                    [[maskOutput(w, w != self.word2vec.lang.getIndex('UNK')) for w in p] for p in pack]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            # padd\n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index)) \n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index))\n",
    "            # turn into torch variables\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length)\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            # store pack\n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index)\n",
    "            success = sum([self.ignore_index != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets   = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627164"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser = SentenceDenoiser(device = torch.device('cpu'), # device\n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = Word2VecConnector(word2vec),\n",
    "                            hidden_dim = 75, \n",
    "                            n_layers = 3, \n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.AdamW)\n",
    "\n",
    "denoiser.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n",
      "9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for seed in [42, 854, 7956, 657, 124] :\n",
    "    batches += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                predict_masked_only = True,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 13s (- 20m 51s) (100 1%) loss : 6.979  accuracy : 6.1 %\n",
      "0m 25s (- 19m 5s) (200 2%) loss : 6.401  accuracy : 6.5 %\n",
      "0m 39s (- 19m 13s) (300 3%) loss : 6.426  accuracy : 7.7 %\n",
      "0m 51s (- 18m 46s) (400 4%) loss : 6.137  accuracy : 9.5 %\n",
      "1m 5s (- 18m 54s) (500 5%) loss : 6.047  accuracy : 9.0 %\n",
      "1m 17s (- 18m 31s) (600 6%) loss : 5.885  accuracy : 10.7 %\n",
      "1m 31s (- 18m 22s) (700 7%) loss : 5.729  accuracy : 12.7 %\n",
      "1m 44s (- 18m 12s) (800 8%) loss : 5.481  accuracy : 14.5 %\n",
      "1m 59s (- 18m 13s) (900 9%) loss : 5.596  accuracy : 14.0 %\n",
      "2m 13s (- 18m 4s) (1000 10%) loss : 5.339  accuracy : 16.4 %\n",
      "2m 27s (- 18m 1s) (1100 12%) loss : 5.288  accuracy : 17.0 %\n",
      "2m 40s (- 17m 42s) (1200 13%) loss : 5.274  accuracy : 17.6 %\n",
      "2m 54s (- 17m 33s) (1300 14%) loss : 5.218  accuracy : 17.3 %\n",
      "3m 9s (- 17m 28s) (1400 15%) loss : 5.125  accuracy : 18.8 %\n",
      "3m 22s (- 17m 10s) (1500 16%) loss : 4.939  accuracy : 18.6 %\n",
      "3m 35s (- 16m 54s) (1600 17%) loss : 4.913  accuracy : 21.5 %\n",
      "3m 48s (- 16m 40s) (1700 18%) loss : 4.879  accuracy : 20.4 %\n",
      "4m 1s (- 16m 25s) (1800 19%) loss : 4.866  accuracy : 19.6 %\n",
      "4m 13s (- 16m 9s) (1900 20%) loss : 4.809  accuracy : 22.0 %\n",
      "4m 26s (- 15m 53s) (2000 21%) loss : 4.704  accuracy : 21.1 %\n",
      "4m 41s (- 15m 45s) (2100 22%) loss : 4.626  accuracy : 23.1 %\n",
      "4m 53s (- 15m 27s) (2200 24%) loss : 4.459  accuracy : 26.2 %\n",
      "5m 6s (- 15m 13s) (2300 25%) loss : 4.517  accuracy : 24.6 %\n",
      "5m 20s (- 15m 0s) (2400 26%) loss : 4.365  accuracy : 26.0 %\n",
      "5m 32s (- 14m 45s) (2500 27%) loss : 4.461  accuracy : 24.5 %\n",
      "5m 45s (- 14m 30s) (2600 28%) loss : 4.504  accuracy : 24.8 %\n",
      "5m 59s (- 14m 17s) (2700 29%) loss : 4.310  accuracy : 26.2 %\n",
      "6m 11s (- 14m 3s) (2800 30%) loss : 4.337  accuracy : 24.6 %\n",
      "6m 23s (- 13m 47s) (2900 31%) loss : 4.385  accuracy : 25.1 %\n",
      "6m 37s (- 13m 34s) (3000 32%) loss : 4.410  accuracy : 24.5 %\n",
      "6m 50s (- 13m 20s) (3100 33%) loss : 4.304  accuracy : 26.1 %\n",
      "7m 4s (- 13m 8s) (3200 34%) loss : 4.298  accuracy : 26.3 %\n",
      "7m 16s (- 12m 53s) (3300 36%) loss : 4.103  accuracy : 28.5 %\n",
      "7m 29s (- 12m 40s) (3400 37%) loss : 4.202  accuracy : 27.0 %\n",
      "7m 42s (- 12m 26s) (3500 38%) loss : 4.132  accuracy : 27.8 %\n",
      "7m 55s (- 12m 13s) (3600 39%) loss : 4.205  accuracy : 26.5 %\n",
      "8m 8s (- 11m 59s) (3700 40%) loss : 4.052  accuracy : 29.0 %\n",
      "8m 21s (- 11m 45s) (3800 41%) loss : 4.133  accuracy : 27.7 %\n",
      "8m 34s (- 11m 32s) (3900 42%) loss : 4.117  accuracy : 27.3 %\n",
      "8m 46s (- 11m 18s) (4000 43%) loss : 4.192  accuracy : 28.0 %\n",
      "8m 58s (- 11m 2s) (4100 44%) loss : 4.057  accuracy : 29.2 %\n",
      "9m 11s (- 10m 49s) (4200 45%) loss : 4.013  accuracy : 29.9 %\n",
      "9m 24s (- 10m 36s) (4300 46%) loss : 3.898  accuracy : 29.5 %\n",
      "9m 37s (- 10m 23s) (4400 48%) loss : 3.826  accuracy : 30.8 %\n",
      "9m 49s (- 10m 8s) (4500 49%) loss : 3.974  accuracy : 30.8 %\n",
      "10m 1s (- 9m 55s) (4600 50%) loss : 4.041  accuracy : 28.7 %\n",
      "10m 15s (- 9m 42s) (4700 51%) loss : 3.837  accuracy : 30.8 %\n",
      "10m 27s (- 9m 28s) (4800 52%) loss : 3.917  accuracy : 31.3 %\n",
      "10m 41s (- 9m 16s) (4900 53%) loss : 3.878  accuracy : 30.2 %\n",
      "10m 53s (- 9m 2s) (5000 54%) loss : 3.866  accuracy : 31.0 %\n",
      "11m 8s (- 8m 50s) (5100 55%) loss : 3.965  accuracy : 30.3 %\n",
      "11m 20s (- 8m 36s) (5200 56%) loss : 3.934  accuracy : 30.2 %\n",
      "11m 33s (- 8m 23s) (5300 57%) loss : 3.847  accuracy : 31.4 %\n",
      "11m 46s (- 8m 10s) (5400 59%) loss : 3.836  accuracy : 31.4 %\n",
      "11m 58s (- 7m 56s) (5500 60%) loss : 3.601  accuracy : 34.5 %\n",
      "12m 11s (- 7m 43s) (5600 61%) loss : 3.695  accuracy : 34.5 %\n",
      "12m 25s (- 7m 31s) (5700 62%) loss : 3.665  accuracy : 34.1 %\n",
      "12m 39s (- 7m 18s) (5800 63%) loss : 3.812  accuracy : 31.9 %\n",
      "12m 51s (- 7m 4s) (5900 64%) loss : 3.795  accuracy : 31.7 %\n",
      "13m 4s (- 6m 51s) (6000 65%) loss : 3.713  accuracy : 33.6 %\n",
      "13m 17s (- 6m 38s) (6100 66%) loss : 3.573  accuracy : 35.3 %\n",
      "13m 29s (- 6m 25s) (6200 67%) loss : 3.600  accuracy : 34.7 %\n",
      "13m 41s (- 6m 11s) (6300 68%) loss : 3.609  accuracy : 35.2 %\n",
      "13m 55s (- 5m 58s) (6400 69%) loss : 3.637  accuracy : 34.8 %\n",
      "14m 8s (- 5m 45s) (6500 71%) loss : 3.596  accuracy : 34.4 %\n",
      "14m 21s (- 5m 32s) (6600 72%) loss : 3.537  accuracy : 34.7 %\n",
      "14m 34s (- 5m 19s) (6700 73%) loss : 3.647  accuracy : 34.4 %\n",
      "14m 47s (- 5m 6s) (6800 74%) loss : 3.525  accuracy : 36.6 %\n",
      "14m 59s (- 4m 53s) (6900 75%) loss : 3.663  accuracy : 34.1 %\n",
      "15m 13s (- 4m 40s) (7000 76%) loss : 3.549  accuracy : 35.7 %\n",
      "15m 26s (- 4m 27s) (7100 77%) loss : 3.535  accuracy : 34.1 %\n",
      "15m 38s (- 4m 14s) (7200 78%) loss : 3.590  accuracy : 35.7 %\n",
      "15m 52s (- 4m 1s) (7300 79%) loss : 3.644  accuracy : 33.9 %\n",
      "16m 7s (- 3m 48s) (7400 80%) loss : 3.623  accuracy : 32.9 %\n",
      "16m 20s (- 3m 35s) (7500 81%) loss : 3.706  accuracy : 33.2 %\n",
      "16m 33s (- 3m 22s) (7600 83%) loss : 3.454  accuracy : 37.4 %\n",
      "16m 45s (- 3m 9s) (7700 84%) loss : 3.414  accuracy : 36.0 %\n",
      "16m 59s (- 2m 56s) (7800 85%) loss : 3.497  accuracy : 35.1 %\n",
      "17m 12s (- 2m 43s) (7900 86%) loss : 3.493  accuracy : 36.9 %\n",
      "17m 25s (- 2m 30s) (8000 87%) loss : 3.496  accuracy : 37.1 %\n",
      "17m 39s (- 2m 17s) (8100 88%) loss : 3.504  accuracy : 36.3 %\n",
      "17m 51s (- 2m 4s) (8200 89%) loss : 3.551  accuracy : 36.0 %\n",
      "18m 4s (- 1m 51s) (8300 90%) loss : 3.463  accuracy : 36.7 %\n",
      "18m 18s (- 1m 38s) (8400 91%) loss : 3.502  accuracy : 36.1 %\n",
      "18m 32s (- 1m 25s) (8500 92%) loss : 3.431  accuracy : 37.6 %\n",
      "18m 44s (- 1m 11s) (8600 93%) loss : 3.534  accuracy : 35.2 %\n",
      "18m 56s (- 0m 58s) (8700 95%) loss : 3.396  accuracy : 37.2 %\n",
      "19m 9s (- 0m 45s) (8800 96%) loss : 3.500  accuracy : 36.5 %\n",
      "19m 23s (- 0m 32s) (8900 97%) loss : 3.403  accuracy : 37.5 %\n",
      "19m 36s (- 0m 19s) (9000 98%) loss : 3.525  accuracy : 36.4 %\n",
      "19m 49s (- 0m 6s) (9100 99%) loss : 3.391  accuracy : 37.4 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches, epochs = 1, lr = 0.001, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n",
      "9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches2 = []\n",
    "for seed in [627, 87569, 21, 334, 2] :\n",
    "    batches2 += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                predict_masked_only = False,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches2))\n",
    "len(batches2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 15s (- 23m 28s) (100 1%) loss : 2.759  accuracy : 46.2 %\n",
      "0m 29s (- 21m 43s) (200 2%) loss : 2.612  accuracy : 50.0 %\n",
      "0m 44s (- 21m 39s) (300 3%) loss : 2.727  accuracy : 49.1 %\n",
      "0m 58s (- 21m 18s) (400 4%) loss : 2.546  accuracy : 51.9 %\n",
      "1m 14s (- 21m 27s) (500 5%) loss : 2.491  accuracy : 52.8 %\n",
      "1m 28s (- 21m 2s) (600 6%) loss : 2.413  accuracy : 54.7 %\n",
      "1m 43s (- 20m 53s) (700 7%) loss : 2.345  accuracy : 55.1 %\n",
      "1m 58s (- 20m 40s) (800 8%) loss : 2.252  accuracy : 56.8 %\n",
      "2m 14s (- 20m 36s) (900 9%) loss : 2.346  accuracy : 56.0 %\n",
      "2m 29s (- 20m 18s) (1000 10%) loss : 2.172  accuracy : 59.5 %\n",
      "2m 45s (- 20m 11s) (1100 12%) loss : 2.226  accuracy : 58.1 %\n",
      "2m 59s (- 19m 47s) (1200 13%) loss : 2.177  accuracy : 59.7 %\n",
      "3m 14s (- 19m 33s) (1300 14%) loss : 2.201  accuracy : 59.1 %\n",
      "3m 30s (- 19m 22s) (1400 15%) loss : 2.145  accuracy : 60.3 %\n",
      "3m 44s (- 19m 3s) (1500 16%) loss : 2.054  accuracy : 62.2 %\n",
      "3m 59s (- 18m 48s) (1600 17%) loss : 2.186  accuracy : 60.5 %\n",
      "4m 13s (- 18m 32s) (1700 18%) loss : 1.998  accuracy : 63.5 %\n",
      "4m 29s (- 18m 19s) (1800 19%) loss : 2.095  accuracy : 61.6 %\n",
      "4m 43s (- 18m 0s) (1900 20%) loss : 2.072  accuracy : 61.8 %\n",
      "4m 58s (- 17m 45s) (2000 21%) loss : 2.001  accuracy : 63.9 %\n",
      "5m 14s (- 17m 37s) (2100 22%) loss : 1.988  accuracy : 64.8 %\n",
      "5m 28s (- 17m 18s) (2200 24%) loss : 1.914  accuracy : 65.3 %\n",
      "5m 43s (- 17m 2s) (2300 25%) loss : 1.980  accuracy : 64.7 %\n",
      "5m 58s (- 16m 49s) (2400 26%) loss : 1.864  accuracy : 66.3 %\n",
      "6m 13s (- 16m 33s) (2500 27%) loss : 1.992  accuracy : 64.8 %\n",
      "6m 27s (- 16m 17s) (2600 28%) loss : 1.936  accuracy : 65.1 %\n",
      "6m 43s (- 16m 3s) (2700 29%) loss : 1.903  accuracy : 66.2 %\n",
      "6m 57s (- 15m 46s) (2800 30%) loss : 1.893  accuracy : 66.4 %\n",
      "7m 11s (- 15m 29s) (2900 31%) loss : 1.943  accuracy : 65.7 %\n",
      "7m 26s (- 15m 15s) (3000 32%) loss : 1.919  accuracy : 66.2 %\n",
      "7m 41s (- 15m 0s) (3100 33%) loss : 1.842  accuracy : 67.2 %\n",
      "7m 56s (- 14m 46s) (3200 34%) loss : 1.824  accuracy : 67.9 %\n",
      "8m 10s (- 14m 28s) (3300 36%) loss : 1.822  accuracy : 67.9 %\n",
      "8m 25s (- 14m 14s) (3400 37%) loss : 1.779  accuracy : 68.4 %\n",
      "8m 39s (- 13m 58s) (3500 38%) loss : 1.829  accuracy : 67.6 %\n",
      "8m 55s (- 13m 44s) (3600 39%) loss : 1.793  accuracy : 68.7 %\n",
      "9m 8s (- 13m 28s) (3700 40%) loss : 1.707  accuracy : 70.2 %\n",
      "9m 24s (- 13m 14s) (3800 41%) loss : 1.822  accuracy : 67.9 %\n",
      "9m 39s (- 12m 59s) (3900 42%) loss : 1.776  accuracy : 69.2 %\n",
      "9m 53s (- 12m 43s) (4000 43%) loss : 1.820  accuracy : 68.6 %\n",
      "10m 6s (- 12m 26s) (4100 44%) loss : 1.802  accuracy : 68.7 %\n",
      "10m 20s (- 12m 11s) (4200 45%) loss : 1.754  accuracy : 69.8 %\n",
      "10m 35s (- 11m 56s) (4300 46%) loss : 1.655  accuracy : 71.1 %\n",
      "10m 50s (- 11m 42s) (4400 48%) loss : 1.659  accuracy : 71.0 %\n",
      "11m 3s (- 11m 25s) (4500 49%) loss : 1.760  accuracy : 69.5 %\n",
      "11m 17s (- 11m 10s) (4600 50%) loss : 1.822  accuracy : 68.5 %\n",
      "11m 32s (- 10m 55s) (4700 51%) loss : 1.669  accuracy : 70.6 %\n",
      "11m 46s (- 10m 40s) (4800 52%) loss : 1.810  accuracy : 69.0 %\n",
      "12m 2s (- 10m 26s) (4900 53%) loss : 1.664  accuracy : 71.3 %\n",
      "12m 16s (- 10m 11s) (5000 54%) loss : 1.676  accuracy : 71.3 %\n",
      "12m 32s (- 9m 57s) (5100 55%) loss : 1.664  accuracy : 71.3 %\n",
      "12m 46s (- 9m 41s) (5200 56%) loss : 1.748  accuracy : 70.0 %\n",
      "13m 1s (- 9m 27s) (5300 57%) loss : 1.633  accuracy : 71.4 %\n",
      "13m 15s (- 9m 12s) (5400 59%) loss : 1.689  accuracy : 70.7 %\n",
      "13m 29s (- 8m 57s) (5500 60%) loss : 1.610  accuracy : 72.0 %\n",
      "13m 43s (- 8m 41s) (5600 61%) loss : 1.640  accuracy : 71.9 %\n",
      "13m 58s (- 8m 27s) (5700 62%) loss : 1.631  accuracy : 72.1 %\n",
      "14m 13s (- 8m 12s) (5800 63%) loss : 1.722  accuracy : 71.1 %\n",
      "14m 27s (- 7m 57s) (5900 64%) loss : 1.707  accuracy : 71.0 %\n",
      "14m 41s (- 7m 42s) (6000 65%) loss : 1.669  accuracy : 71.8 %\n",
      "14m 56s (- 7m 28s) (6100 66%) loss : 1.595  accuracy : 72.7 %\n",
      "15m 10s (- 7m 13s) (6200 67%) loss : 1.554  accuracy : 73.3 %\n",
      "15m 24s (- 6m 58s) (6300 68%) loss : 1.598  accuracy : 72.9 %\n",
      "15m 39s (- 6m 43s) (6400 69%) loss : 1.577  accuracy : 73.0 %\n",
      "15m 55s (- 6m 29s) (6500 71%) loss : 1.562  accuracy : 73.7 %\n",
      "16m 10s (- 6m 14s) (6600 72%) loss : 1.583  accuracy : 72.8 %\n",
      "16m 25s (- 6m 0s) (6700 73%) loss : 1.544  accuracy : 74.1 %\n",
      "16m 40s (- 5m 45s) (6800 74%) loss : 1.562  accuracy : 73.0 %\n",
      "16m 53s (- 5m 30s) (6900 75%) loss : 1.660  accuracy : 72.0 %\n",
      "17m 8s (- 5m 15s) (7000 76%) loss : 1.624  accuracy : 72.5 %\n",
      "17m 24s (- 5m 1s) (7100 77%) loss : 1.481  accuracy : 74.8 %\n",
      "17m 37s (- 4m 46s) (7200 78%) loss : 1.578  accuracy : 73.0 %\n",
      "17m 54s (- 4m 32s) (7300 79%) loss : 1.547  accuracy : 73.8 %\n",
      "18m 10s (- 4m 17s) (7400 80%) loss : 1.521  accuracy : 74.0 %\n",
      "18m 25s (- 4m 3s) (7500 81%) loss : 1.666  accuracy : 72.3 %\n",
      "18m 39s (- 3m 48s) (7600 83%) loss : 1.433  accuracy : 75.3 %\n",
      "18m 53s (- 3m 33s) (7700 84%) loss : 1.473  accuracy : 74.9 %\n",
      "19m 10s (- 3m 19s) (7800 85%) loss : 1.454  accuracy : 75.4 %\n",
      "19m 24s (- 3m 4s) (7900 86%) loss : 1.551  accuracy : 73.9 %\n",
      "19m 38s (- 2m 49s) (8000 87%) loss : 1.494  accuracy : 74.7 %\n",
      "19m 54s (- 2m 34s) (8100 88%) loss : 1.485  accuracy : 74.6 %\n",
      "20m 8s (- 2m 20s) (8200 89%) loss : 1.521  accuracy : 74.6 %\n",
      "20m 23s (- 2m 5s) (8300 90%) loss : 1.473  accuracy : 75.5 %\n",
      "20m 39s (- 1m 50s) (8400 91%) loss : 1.487  accuracy : 75.3 %\n",
      "20m 55s (- 1m 35s) (8500 92%) loss : 1.416  accuracy : 76.0 %\n",
      "21m 9s (- 1m 21s) (8600 93%) loss : 1.506  accuracy : 74.9 %\n",
      "21m 23s (- 1m 6s) (8700 95%) loss : 1.493  accuracy : 74.8 %\n",
      "21m 37s (- 0m 51s) (8800 96%) loss : 1.455  accuracy : 75.5 %\n",
      "21m 53s (- 0m 36s) (8900 97%) loss : 1.430  accuracy : 75.5 %\n",
      "22m 8s (- 0m 22s) (9000 98%) loss : 1.497  accuracy : 74.9 %\n",
      "22m 23s (- 0m 7s) (9100 99%) loss : 1.458  accuracy : 75.2 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches2, epochs = 1, lr = 0.0001, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 14s (- 21m 56s) (100 1%) loss : 3.914  accuracy : 31.7 %\n",
      "0m 27s (- 20m 39s) (200 2%) loss : 3.909  accuracy : 33.0 %\n",
      "0m 39s (- 19m 36s) (300 3%) loss : 3.889  accuracy : 31.4 %\n",
      "0m 52s (- 19m 16s) (400 4%) loss : 3.764  accuracy : 33.2 %\n",
      "1m 6s (- 19m 11s) (500 5%) loss : 3.825  accuracy : 31.1 %\n",
      "1m 17s (- 18m 23s) (600 6%) loss : 3.789  accuracy : 33.2 %\n",
      "1m 31s (- 18m 20s) (700 7%) loss : 3.692  accuracy : 33.1 %\n",
      "1m 43s (- 17m 57s) (800 8%) loss : 3.704  accuracy : 33.9 %\n",
      "1m 57s (- 17m 54s) (900 9%) loss : 3.659  accuracy : 33.8 %\n",
      "2m 8s (- 17m 30s) (1000 10%) loss : 3.604  accuracy : 33.9 %\n",
      "2m 21s (- 17m 17s) (1100 12%) loss : 3.685  accuracy : 33.7 %\n",
      "2m 36s (- 17m 13s) (1200 13%) loss : 3.698  accuracy : 33.1 %\n",
      "2m 48s (- 16m 59s) (1300 14%) loss : 3.809  accuracy : 32.8 %\n",
      "3m 2s (- 16m 48s) (1400 15%) loss : 3.662  accuracy : 34.4 %\n",
      "3m 15s (- 16m 38s) (1500 16%) loss : 3.786  accuracy : 31.3 %\n",
      "3m 28s (- 16m 23s) (1600 17%) loss : 3.682  accuracy : 32.8 %\n",
      "3m 40s (- 16m 8s) (1700 18%) loss : 3.590  accuracy : 35.7 %\n",
      "3m 53s (- 15m 55s) (1800 19%) loss : 3.620  accuracy : 33.5 %\n",
      "4m 7s (- 15m 42s) (1900 20%) loss : 3.705  accuracy : 33.3 %\n",
      "4m 20s (- 15m 30s) (2000 21%) loss : 3.664  accuracy : 33.0 %\n",
      "4m 31s (- 15m 12s) (2100 22%) loss : 3.572  accuracy : 35.0 %\n",
      "4m 45s (- 15m 1s) (2200 24%) loss : 3.632  accuracy : 33.8 %\n",
      "4m 57s (- 14m 44s) (2300 25%) loss : 3.430  accuracy : 36.1 %\n",
      "5m 10s (- 14m 32s) (2400 26%) loss : 3.711  accuracy : 33.2 %\n",
      "5m 23s (- 14m 21s) (2500 27%) loss : 3.661  accuracy : 32.9 %\n",
      "5m 36s (- 14m 6s) (2600 28%) loss : 3.587  accuracy : 33.9 %\n",
      "5m 50s (- 13m 56s) (2700 29%) loss : 3.638  accuracy : 34.3 %\n",
      "6m 3s (- 13m 43s) (2800 30%) loss : 3.639  accuracy : 33.8 %\n",
      "6m 15s (- 13m 30s) (2900 31%) loss : 3.729  accuracy : 32.7 %\n",
      "6m 29s (- 13m 18s) (3000 32%) loss : 3.623  accuracy : 34.7 %\n",
      "6m 41s (- 13m 4s) (3100 33%) loss : 3.540  accuracy : 34.5 %\n",
      "6m 54s (- 12m 51s) (3200 34%) loss : 3.483  accuracy : 36.3 %\n",
      "7m 8s (- 12m 40s) (3300 36%) loss : 3.668  accuracy : 33.0 %\n",
      "7m 21s (- 12m 26s) (3400 37%) loss : 3.544  accuracy : 34.5 %\n",
      "7m 34s (- 12m 12s) (3500 38%) loss : 3.427  accuracy : 37.6 %\n",
      "7m 46s (- 11m 58s) (3600 39%) loss : 3.590  accuracy : 34.3 %\n",
      "7m 59s (- 11m 46s) (3700 40%) loss : 3.476  accuracy : 35.7 %\n",
      "8m 13s (- 11m 34s) (3800 41%) loss : 3.517  accuracy : 34.9 %\n",
      "8m 27s (- 11m 22s) (3900 42%) loss : 3.457  accuracy : 35.4 %\n",
      "8m 40s (- 11m 10s) (4000 43%) loss : 3.527  accuracy : 34.3 %\n",
      "8m 55s (- 10m 59s) (4100 44%) loss : 3.596  accuracy : 35.0 %\n",
      "9m 7s (- 10m 45s) (4200 45%) loss : 3.363  accuracy : 36.3 %\n",
      "9m 19s (- 10m 31s) (4300 46%) loss : 3.639  accuracy : 33.7 %\n",
      "9m 32s (- 10m 17s) (4400 48%) loss : 3.381  accuracy : 37.2 %\n",
      "9m 45s (- 10m 5s) (4500 49%) loss : 3.622  accuracy : 34.8 %\n",
      "9m 59s (- 9m 52s) (4600 50%) loss : 3.502  accuracy : 37.2 %\n",
      "10m 12s (- 9m 39s) (4700 51%) loss : 3.543  accuracy : 34.3 %\n",
      "10m 25s (- 9m 26s) (4800 52%) loss : 3.369  accuracy : 36.1 %\n",
      "10m 38s (- 9m 13s) (4900 53%) loss : 3.430  accuracy : 36.3 %\n",
      "10m 50s (- 8m 59s) (5000 54%) loss : 3.466  accuracy : 35.7 %\n",
      "11m 2s (- 8m 46s) (5100 55%) loss : 3.373  accuracy : 35.7 %\n",
      "11m 16s (- 8m 33s) (5200 56%) loss : 3.504  accuracy : 35.1 %\n",
      "11m 28s (- 8m 20s) (5300 57%) loss : 3.384  accuracy : 36.1 %\n",
      "11m 42s (- 8m 7s) (5400 59%) loss : 3.562  accuracy : 33.7 %\n",
      "11m 54s (- 7m 54s) (5500 60%) loss : 3.456  accuracy : 35.8 %\n",
      "12m 6s (- 7m 40s) (5600 61%) loss : 3.316  accuracy : 37.7 %\n",
      "12m 20s (- 7m 28s) (5700 62%) loss : 3.529  accuracy : 34.4 %\n",
      "12m 31s (- 7m 13s) (5800 63%) loss : 3.467  accuracy : 36.3 %\n",
      "12m 43s (- 7m 0s) (5900 64%) loss : 3.435  accuracy : 35.0 %\n",
      "12m 55s (- 6m 47s) (6000 65%) loss : 3.424  accuracy : 35.4 %\n",
      "13m 7s (- 6m 33s) (6100 66%) loss : 3.435  accuracy : 35.7 %\n",
      "13m 20s (- 6m 21s) (6200 67%) loss : 3.368  accuracy : 36.4 %\n",
      "13m 33s (- 6m 8s) (6300 68%) loss : 3.493  accuracy : 34.7 %\n",
      "13m 45s (- 5m 54s) (6400 69%) loss : 3.344  accuracy : 37.3 %\n",
      "13m 58s (- 5m 42s) (6500 71%) loss : 3.491  accuracy : 33.9 %\n",
      "14m 11s (- 5m 28s) (6600 72%) loss : 3.328  accuracy : 36.8 %\n",
      "14m 24s (- 5m 16s) (6700 73%) loss : 3.364  accuracy : 36.8 %\n",
      "14m 37s (- 5m 3s) (6800 74%) loss : 3.396  accuracy : 35.8 %\n",
      "14m 49s (- 4m 50s) (6900 75%) loss : 3.526  accuracy : 33.2 %\n",
      "15m 3s (- 4m 37s) (7000 76%) loss : 3.475  accuracy : 34.9 %\n",
      "15m 14s (- 4m 24s) (7100 77%) loss : 3.515  accuracy : 33.9 %\n",
      "15m 27s (- 4m 11s) (7200 78%) loss : 3.408  accuracy : 35.5 %\n",
      "15m 40s (- 3m 58s) (7300 79%) loss : 3.341  accuracy : 36.7 %\n",
      "15m 53s (- 3m 45s) (7400 80%) loss : 3.487  accuracy : 35.0 %\n",
      "16m 8s (- 3m 33s) (7500 81%) loss : 3.484  accuracy : 35.7 %\n",
      "16m 22s (- 3m 20s) (7600 83%) loss : 3.401  accuracy : 36.7 %\n",
      "16m 35s (- 3m 7s) (7700 84%) loss : 3.498  accuracy : 35.3 %\n",
      "16m 49s (- 2m 54s) (7800 85%) loss : 3.341  accuracy : 37.7 %\n",
      "17m 2s (- 2m 41s) (7900 86%) loss : 3.284  accuracy : 35.8 %\n",
      "17m 14s (- 2m 28s) (8000 87%) loss : 3.193  accuracy : 38.2 %\n",
      "17m 28s (- 2m 15s) (8100 88%) loss : 3.384  accuracy : 35.1 %\n",
      "17m 41s (- 2m 2s) (8200 89%) loss : 3.429  accuracy : 35.6 %\n",
      "17m 55s (- 1m 50s) (8300 90%) loss : 3.488  accuracy : 34.5 %\n",
      "18m 8s (- 1m 37s) (8400 91%) loss : 3.431  accuracy : 35.8 %\n",
      "18m 22s (- 1m 24s) (8500 92%) loss : 3.351  accuracy : 35.9 %\n",
      "18m 35s (- 1m 11s) (8600 93%) loss : 3.313  accuracy : 37.8 %\n",
      "18m 49s (- 0m 58s) (8700 95%) loss : 3.397  accuracy : 36.5 %\n",
      "19m 4s (- 0m 45s) (8800 96%) loss : 3.398  accuracy : 36.4 %\n",
      "19m 16s (- 0m 32s) (8900 97%) loss : 3.352  accuracy : 36.4 %\n",
      "19m 30s (- 0m 19s) (9000 98%) loss : 3.188  accuracy : 39.3 %\n",
      "19m 43s (- 0m 6s) (9100 99%) loss : 3.366  accuracy : 37.1 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches, epochs = 1, lr = 0.00001, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(denoiser.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth')\n",
    "\n",
    "# load\n",
    "#denoiser.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this test consists of assessing the dissolution time of the freeze dried yellow fever vaccine after adding the suitable diluent directly into the original container\n",
      "\n",
      "\n",
      "this test consists of \u001b[93mdetermining\u001b[0m the dissolution time of the freeze \u001b[93myellow\u001b[0m yellow fever vaccine after adding the \u001b[93mendotoxin\u001b[0m \u001b[93mis\u001b[0m \u001b[93mprepared\u001b[0m into the \u001b[93mappropriate\u001b[0m container\n"
     ]
    }
   ],
   "source": [
    "denoiser.eval()\n",
    "sentence = ' '.join(sentences[10]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "denoiser(sentence, color = '\\033[93m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
