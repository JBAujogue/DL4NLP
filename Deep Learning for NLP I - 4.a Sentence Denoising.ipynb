{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 4.a <br><br><br>\n",
    "  Sentence Denoising\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=red>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global structure of the [sentence denoiser](#sentence_denoiser) is the pipeline of two modules, followed by a final classification layer, as for Language Models in **Part I - 3**.\n",
    "\n",
    "Training follows a denoising objective known as _Cloze task_, which is used :\n",
    "\n",
    "- For the BERT model in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.3.1\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\chatNLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importWords(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importAllWords(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importWords(file_name))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def importSentences(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        sentences = []\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                sentences.append(cleanSentence(current_tile))\n",
    "                title = current_tile\n",
    "            sentences.append(cleanSentence(str(pair[1]).split(' | ')[-1]))\n",
    "        return sentences\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    sentences = cleanDatabase(db)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def importAllSentences(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus += importSentences(file_name)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importAllWords(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31574"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = importAllSentences(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "## 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from chatNLP.models.Word_Embedding import Word2VecConnector\n",
    "from chatNLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = FastText(size = 100, \n",
    "                             window = 5, \n",
    "                             min_count = 1, \n",
    "                             negative = 20,\n",
    "                             sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8086"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fastText_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.train(sentences = corpus, \n",
    "                        epochs = 50,\n",
    "                        total_examples = fastText_word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(fastText_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sentence_denoiser\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentence denoising Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDenoiser(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SentenceDenoiser, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1, :, :]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0.15,\n",
    "                                predict_masked_only = True,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.ignore_index\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index\n",
    "            \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            # prepare pack\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            pack = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack]\n",
    "            pack = [[w for w in words if w is not None] for words in pack]\n",
    "            mask = [random.sample([_ for _ in range(len(p))], k = int(mask_ratio*len(p) +1)) for p in pack]\n",
    "            # split into input and target pack\n",
    "            pack0 = [[maskInput( s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)]\n",
    "            pack1 = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)] if predict_masked_only else pack\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index))\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index)\n",
    "            success = sum([self.ignore_index != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets   = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDenoiser(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SentenceDenoiser, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.transfert = nn.Linear(self.context.output_dim, self.word2vec.output_dim)\n",
    "        self.out       = nn.Linear(self.word2vec.output_dim, self.word2vec.lang.n_words, bias = False)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        self.out.weight = nn.Parameter(torch.FloatTensor(self.word2vec.lookupTable()))\n",
    "        for param in self.out.parameters() : param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(self.transfert(hidden[-1, :, :])), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0.15,\n",
    "                                predict_masked_only = True,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.ignore_index\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index\n",
    "            \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            # prepare pack\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            pack = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack]\n",
    "            pack = [[w for w in words if w is not None] for words in pack]\n",
    "            mask = [random.sample([_ for _ in range(len(p))], k = int(mask_ratio*len(p) +1)) for p in pack]\n",
    "            # split into input and target pack\n",
    "            pack0 = [[maskInput( s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)]\n",
    "            pack1 = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)] if predict_masked_only else pack\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index))\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(self.transfert(hiddens)), dim = 2)    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index)\n",
    "            success = sum([self.ignore_index != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets   = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196900"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser = SentenceDenoiser(device,\n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = word2vec,\n",
    "                            hidden_dim = 75, \n",
    "                            n_layers = 2, \n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.SGD)\n",
    "\n",
    "denoiser.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceDenoiser(\n",
       "  (word2vec): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(8088, 100)\n",
       "    )\n",
       "    (embedding): Embedding(8088, 100)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (bigru): GRU(100, 75, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (transfert): Linear(in_features=150, out_features=100, bias=True)\n",
       "  (out): Linear(in_features=100, out_features=8088, bias=False)\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14805"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for seed in [42, 854, 3, 7956, 881125, 76, 5721, 1499, 8752, 374, 14758, 23, 9543, 856, 75] :\n",
    "    batches += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 32,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                predict_masked_only = True,\n",
    "                                                seed = seed)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 11s (- 27m 30s) (100 0%) loss : 17.892  accuracy : 1.5 %\n",
      "0m 22s (- 27m 54s) (200 1%) loss : 15.222  accuracy : 0.6 %\n",
      "0m 33s (- 26m 47s) (300 2%) loss : 14.162  accuracy : 0.9 %\n",
      "0m 46s (- 27m 43s) (400 2%) loss : 11.374  accuracy : 1.0 %\n",
      "0m 57s (- 27m 16s) (500 3%) loss : 14.998  accuracy : 1.0 %\n",
      "1m 8s (- 27m 13s) (600 4%) loss : 11.702  accuracy : 1.2 %\n",
      "1m 19s (- 26m 50s) (700 4%) loss : 11.730  accuracy : 1.3 %\n",
      "1m 30s (- 26m 27s) (800 5%) loss : 9.129  accuracy : 2.1 %\n",
      "1m 44s (- 26m 49s) (900 6%) loss : 8.346  accuracy : 2.7 %\n",
      "1m 55s (- 26m 33s) (1000 6%) loss : 7.973  accuracy : 2.7 %\n",
      "2m 7s (- 26m 25s) (1100 7%) loss : 9.001  accuracy : 2.2 %\n",
      "2m 19s (- 26m 19s) (1200 8%) loss : 8.712  accuracy : 1.9 %\n",
      "2m 30s (- 26m 1s) (1300 8%) loss : 8.049  accuracy : 2.5 %\n",
      "2m 41s (- 25m 45s) (1400 9%) loss : 7.410  accuracy : 3.3 %\n",
      "2m 52s (- 25m 29s) (1500 10%) loss : 7.337  accuracy : 4.2 %\n",
      "3m 3s (- 25m 16s) (1600 10%) loss : 7.358  accuracy : 3.6 %\n",
      "3m 14s (- 25m 2s) (1700 11%) loss : 7.264  accuracy : 3.8 %\n",
      "3m 27s (- 25m 1s) (1800 12%) loss : 7.465  accuracy : 3.7 %\n",
      "3m 39s (- 24m 49s) (1900 12%) loss : 7.201  accuracy : 4.1 %\n",
      "3m 51s (- 24m 41s) (2000 13%) loss : 7.216  accuracy : 4.5 %\n",
      "4m 3s (- 24m 34s) (2100 14%) loss : 7.182  accuracy : 4.9 %\n",
      "4m 15s (- 24m 22s) (2200 14%) loss : 7.212  accuracy : 3.9 %\n",
      "4m 28s (- 24m 20s) (2300 15%) loss : 7.307  accuracy : 3.6 %\n",
      "4m 39s (- 24m 2s) (2400 16%) loss : 7.123  accuracy : 4.2 %\n",
      "4m 54s (- 24m 9s) (2500 16%) loss : 7.313  accuracy : 3.9 %\n",
      "5m 6s (- 23m 57s) (2600 17%) loss : 7.007  accuracy : 4.3 %\n",
      "5m 17s (- 23m 45s) (2700 18%) loss : 7.035  accuracy : 4.1 %\n",
      "5m 29s (- 23m 34s) (2800 18%) loss : 6.951  accuracy : 4.5 %\n",
      "5m 40s (- 23m 16s) (2900 19%) loss : 6.933  accuracy : 4.2 %\n",
      "5m 53s (- 23m 10s) (3000 20%) loss : 7.030  accuracy : 4.6 %\n",
      "6m 5s (- 23m 0s) (3100 20%) loss : 6.871  accuracy : 4.4 %\n",
      "6m 18s (- 22m 52s) (3200 21%) loss : 6.837  accuracy : 5.3 %\n",
      "6m 31s (- 22m 45s) (3300 22%) loss : 6.977  accuracy : 5.3 %\n",
      "6m 44s (- 22m 35s) (3400 22%) loss : 6.903  accuracy : 4.9 %\n",
      "6m 55s (- 22m 22s) (3500 23%) loss : 6.818  accuracy : 4.5 %\n",
      "7m 7s (- 22m 10s) (3600 24%) loss : 6.822  accuracy : 4.7 %\n",
      "7m 18s (- 21m 54s) (3700 24%) loss : 6.611  accuracy : 5.8 %\n",
      "7m 30s (- 21m 45s) (3800 25%) loss : 6.654  accuracy : 6.1 %\n",
      "7m 41s (- 21m 31s) (3900 26%) loss : 6.546  accuracy : 6.1 %\n",
      "7m 53s (- 21m 18s) (4000 27%) loss : 6.888  accuracy : 4.7 %\n",
      "8m 4s (- 21m 5s) (4100 27%) loss : 6.575  accuracy : 5.3 %\n",
      "8m 16s (- 20m 53s) (4200 28%) loss : 6.695  accuracy : 5.5 %\n",
      "8m 26s (- 20m 38s) (4300 29%) loss : 6.502  accuracy : 6.2 %\n",
      "8m 37s (- 20m 22s) (4400 29%) loss : 6.625  accuracy : 5.0 %\n",
      "8m 48s (- 20m 11s) (4500 30%) loss : 6.473  accuracy : 6.8 %\n",
      "9m 0s (- 19m 59s) (4600 31%) loss : 6.491  accuracy : 5.8 %\n",
      "9m 11s (- 19m 46s) (4700 31%) loss : 6.405  accuracy : 6.2 %\n",
      "9m 24s (- 19m 36s) (4800 32%) loss : 6.273  accuracy : 7.3 %\n",
      "9m 35s (- 19m 24s) (4900 33%) loss : 6.437  accuracy : 6.4 %\n",
      "9m 48s (- 19m 14s) (5000 33%) loss : 6.185  accuracy : 7.7 %\n",
      "10m 0s (- 19m 3s) (5100 34%) loss : 6.356  accuracy : 6.8 %\n",
      "10m 12s (- 18m 51s) (5200 35%) loss : 6.358  accuracy : 7.0 %\n",
      "10m 25s (- 18m 42s) (5300 35%) loss : 6.310  accuracy : 6.6 %\n",
      "10m 38s (- 18m 32s) (5400 36%) loss : 6.232  accuracy : 7.1 %\n",
      "10m 50s (- 18m 20s) (5500 37%) loss : 6.096  accuracy : 7.7 %\n",
      "11m 2s (- 18m 8s) (5600 37%) loss : 6.141  accuracy : 7.5 %\n",
      "11m 14s (- 17m 56s) (5700 38%) loss : 6.146  accuracy : 7.3 %\n",
      "11m 25s (- 17m 44s) (5800 39%) loss : 6.114  accuracy : 7.1 %\n",
      "11m 36s (- 17m 31s) (5900 39%) loss : 6.051  accuracy : 7.5 %\n",
      "11m 48s (- 17m 20s) (6000 40%) loss : 5.969  accuracy : 7.7 %\n",
      "12m 1s (- 17m 9s) (6100 41%) loss : 6.046  accuracy : 8.3 %\n",
      "12m 14s (- 16m 59s) (6200 41%) loss : 5.921  accuracy : 8.5 %\n",
      "12m 25s (- 16m 46s) (6300 42%) loss : 6.059  accuracy : 8.9 %\n",
      "12m 36s (- 16m 32s) (6400 43%) loss : 6.056  accuracy : 7.7 %\n",
      "12m 46s (- 16m 19s) (6500 43%) loss : 5.801  accuracy : 8.9 %\n",
      "12m 59s (- 16m 8s) (6600 44%) loss : 5.909  accuracy : 7.8 %\n",
      "13m 11s (- 15m 57s) (6700 45%) loss : 5.928  accuracy : 7.8 %\n",
      "13m 24s (- 15m 46s) (6800 45%) loss : 5.853  accuracy : 9.0 %\n",
      "13m 36s (- 15m 35s) (6900 46%) loss : 5.798  accuracy : 9.5 %\n",
      "13m 47s (- 15m 22s) (7000 47%) loss : 5.840  accuracy : 8.5 %\n",
      "13m 58s (- 15m 10s) (7100 47%) loss : 5.673  accuracy : 8.8 %\n",
      "14m 10s (- 14m 58s) (7200 48%) loss : 5.867  accuracy : 9.0 %\n",
      "14m 21s (- 14m 45s) (7300 49%) loss : 5.789  accuracy : 8.9 %\n",
      "14m 34s (- 14m 34s) (7400 49%) loss : 5.790  accuracy : 8.6 %\n",
      "14m 47s (- 14m 24s) (7500 50%) loss : 5.760  accuracy : 8.6 %\n",
      "14m 58s (- 14m 11s) (7600 51%) loss : 5.680  accuracy : 9.5 %\n",
      "15m 10s (- 13m 59s) (7700 52%) loss : 5.624  accuracy : 9.9 %\n",
      "15m 21s (- 13m 47s) (7800 52%) loss : 5.768  accuracy : 9.7 %\n",
      "15m 34s (- 13m 36s) (7900 53%) loss : 5.598  accuracy : 9.5 %\n",
      "15m 46s (- 13m 25s) (8000 54%) loss : 5.575  accuracy : 10.3 %\n",
      "15m 59s (- 13m 14s) (8100 54%) loss : 5.649  accuracy : 10.0 %\n",
      "16m 11s (- 13m 2s) (8200 55%) loss : 5.602  accuracy : 10.7 %\n",
      "16m 23s (- 12m 50s) (8300 56%) loss : 5.656  accuracy : 9.4 %\n",
      "16m 36s (- 12m 39s) (8400 56%) loss : 5.441  accuracy : 11.8 %\n",
      "16m 48s (- 12m 28s) (8500 57%) loss : 5.586  accuracy : 10.4 %\n",
      "17m 1s (- 12m 17s) (8600 58%) loss : 5.491  accuracy : 10.7 %\n",
      "17m 14s (- 12m 5s) (8700 58%) loss : 5.450  accuracy : 11.1 %\n",
      "17m 25s (- 11m 53s) (8800 59%) loss : 5.421  accuracy : 11.0 %\n",
      "17m 36s (- 11m 41s) (8900 60%) loss : 5.409  accuracy : 11.1 %\n",
      "17m 47s (- 11m 28s) (9000 60%) loss : 5.516  accuracy : 11.4 %\n",
      "17m 58s (- 11m 16s) (9100 61%) loss : 5.319  accuracy : 12.0 %\n",
      "18m 11s (- 11m 4s) (9200 62%) loss : 5.380  accuracy : 12.3 %\n",
      "18m 23s (- 10m 52s) (9300 62%) loss : 5.279  accuracy : 11.8 %\n",
      "18m 34s (- 10m 40s) (9400 63%) loss : 5.398  accuracy : 12.3 %\n",
      "18m 46s (- 10m 28s) (9500 64%) loss : 5.325  accuracy : 12.3 %\n",
      "18m 58s (- 10m 17s) (9600 64%) loss : 5.335  accuracy : 12.3 %\n",
      "19m 9s (- 10m 4s) (9700 65%) loss : 5.281  accuracy : 13.0 %\n",
      "19m 20s (- 9m 52s) (9800 66%) loss : 5.275  accuracy : 12.4 %\n",
      "19m 31s (- 9m 40s) (9900 66%) loss : 5.304  accuracy : 12.4 %\n",
      "19m 44s (- 9m 28s) (10000 67%) loss : 5.289  accuracy : 12.0 %\n",
      "19m 54s (- 9m 16s) (10100 68%) loss : 5.186  accuracy : 13.4 %\n",
      "20m 4s (- 9m 3s) (10200 68%) loss : 5.150  accuracy : 14.8 %\n",
      "20m 17s (- 8m 52s) (10300 69%) loss : 5.225  accuracy : 12.5 %\n",
      "20m 28s (- 8m 40s) (10400 70%) loss : 5.129  accuracy : 14.2 %\n",
      "20m 42s (- 8m 29s) (10500 70%) loss : 5.202  accuracy : 13.6 %\n",
      "20m 53s (- 8m 17s) (10600 71%) loss : 5.129  accuracy : 13.1 %\n",
      "21m 5s (- 8m 5s) (10700 72%) loss : 5.143  accuracy : 13.5 %\n",
      "21m 16s (- 7m 53s) (10800 72%) loss : 5.111  accuracy : 13.3 %\n",
      "21m 28s (- 7m 41s) (10900 73%) loss : 5.092  accuracy : 13.4 %\n",
      "21m 40s (- 7m 29s) (11000 74%) loss : 5.194  accuracy : 13.1 %\n",
      "21m 51s (- 7m 17s) (11100 74%) loss : 5.064  accuracy : 14.9 %\n",
      "22m 3s (- 7m 6s) (11200 75%) loss : 5.125  accuracy : 14.4 %\n",
      "22m 14s (- 6m 53s) (11300 76%) loss : 4.896  accuracy : 16.4 %\n",
      "22m 25s (- 6m 41s) (11400 77%) loss : 5.203  accuracy : 12.7 %\n",
      "22m 39s (- 6m 30s) (11500 77%) loss : 5.216  accuracy : 11.6 %\n",
      "22m 50s (- 6m 18s) (11600 78%) loss : 5.171  accuracy : 13.8 %\n",
      "23m 2s (- 6m 6s) (11700 79%) loss : 4.941  accuracy : 15.4 %\n",
      "23m 13s (- 5m 54s) (11800 79%) loss : 5.036  accuracy : 14.9 %\n",
      "23m 26s (- 5m 43s) (11900 80%) loss : 5.054  accuracy : 14.1 %\n",
      "23m 37s (- 5m 31s) (12000 81%) loss : 4.848  accuracy : 16.2 %\n",
      "23m 48s (- 5m 19s) (12100 81%) loss : 4.931  accuracy : 15.1 %\n",
      "24m 1s (- 5m 7s) (12200 82%) loss : 5.081  accuracy : 13.0 %\n",
      "24m 13s (- 4m 56s) (12300 83%) loss : 4.883  accuracy : 16.1 %\n",
      "24m 25s (- 4m 44s) (12400 83%) loss : 4.850  accuracy : 15.7 %\n",
      "24m 36s (- 4m 32s) (12500 84%) loss : 5.022  accuracy : 14.2 %\n",
      "24m 48s (- 4m 20s) (12600 85%) loss : 4.961  accuracy : 15.0 %\n",
      "25m 1s (- 4m 8s) (12700 85%) loss : 4.908  accuracy : 13.9 %\n",
      "25m 13s (- 3m 57s) (12800 86%) loss : 4.955  accuracy : 15.0 %\n",
      "25m 26s (- 3m 45s) (12900 87%) loss : 4.944  accuracy : 15.0 %\n",
      "25m 36s (- 3m 33s) (13000 87%) loss : 4.875  accuracy : 16.1 %\n",
      "25m 47s (- 3m 21s) (13100 88%) loss : 4.869  accuracy : 14.8 %\n",
      "25m 58s (- 3m 9s) (13200 89%) loss : 4.837  accuracy : 16.5 %\n",
      "26m 10s (- 2m 57s) (13300 89%) loss : 4.844  accuracy : 15.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26m 22s (- 2m 45s) (13400 90%) loss : 4.873  accuracy : 16.9 %\n",
      "26m 34s (- 2m 34s) (13500 91%) loss : 4.876  accuracy : 15.6 %\n",
      "26m 46s (- 2m 22s) (13600 91%) loss : 4.676  accuracy : 18.1 %\n",
      "26m 57s (- 2m 10s) (13700 92%) loss : 4.898  accuracy : 16.9 %\n",
      "27m 9s (- 1m 58s) (13800 93%) loss : 4.945  accuracy : 16.8 %\n",
      "27m 22s (- 1m 46s) (13900 93%) loss : 4.800  accuracy : 16.1 %\n",
      "27m 34s (- 1m 35s) (14000 94%) loss : 4.961  accuracy : 15.4 %\n",
      "27m 45s (- 1m 23s) (14100 95%) loss : 4.730  accuracy : 16.7 %\n",
      "27m 57s (- 1m 11s) (14200 95%) loss : 4.869  accuracy : 16.6 %\n",
      "28m 8s (- 0m 59s) (14300 96%) loss : 4.731  accuracy : 17.3 %\n",
      "28m 21s (- 0m 47s) (14400 97%) loss : 4.793  accuracy : 16.7 %\n",
      "28m 31s (- 0m 35s) (14500 97%) loss : 4.635  accuracy : 19.0 %\n",
      "28m 42s (- 0m 24s) (14600 98%) loss : 4.821  accuracy : 17.6 %\n",
      "28m 54s (- 0m 12s) (14700 99%) loss : 4.725  accuracy : 16.9 %\n",
      "29m 4s (- 0m 0s) (14800 99%) loss : 4.863  accuracy : 16.7 %\n",
      "epoch 1\n",
      "0m 12s (- 30m 52s) (100 0%) loss : 4.652  accuracy : 16.0 %\n",
      "0m 23s (- 29m 9s) (200 1%) loss : 4.579  accuracy : 18.4 %\n",
      "0m 36s (- 29m 25s) (300 2%) loss : 4.740  accuracy : 17.8 %\n",
      "0m 50s (- 30m 23s) (400 2%) loss : 4.567  accuracy : 18.7 %\n",
      "1m 2s (- 29m 43s) (500 3%) loss : 4.762  accuracy : 17.1 %\n",
      "1m 12s (- 28m 43s) (600 4%) loss : 4.638  accuracy : 18.4 %\n",
      "1m 24s (- 28m 14s) (700 4%) loss : 4.450  accuracy : 19.6 %\n",
      "1m 36s (- 28m 15s) (800 5%) loss : 4.429  accuracy : 20.9 %\n",
      "1m 49s (- 28m 8s) (900 6%) loss : 4.666  accuracy : 17.6 %\n",
      "2m 0s (- 27m 48s) (1000 6%) loss : 4.549  accuracy : 19.1 %\n",
      "2m 12s (- 27m 33s) (1100 7%) loss : 4.606  accuracy : 19.6 %\n",
      "2m 24s (- 27m 22s) (1200 8%) loss : 4.678  accuracy : 17.8 %\n",
      "2m 36s (- 27m 3s) (1300 8%) loss : 4.476  accuracy : 19.8 %\n",
      "2m 47s (- 26m 41s) (1400 9%) loss : 4.560  accuracy : 19.9 %\n",
      "2m 58s (- 26m 23s) (1500 10%) loss : 4.512  accuracy : 19.8 %\n",
      "3m 8s (- 25m 54s) (1600 10%) loss : 4.530  accuracy : 20.4 %\n",
      "3m 19s (- 25m 38s) (1700 11%) loss : 4.551  accuracy : 20.1 %\n",
      "3m 31s (- 25m 29s) (1800 12%) loss : 4.587  accuracy : 20.1 %\n",
      "3m 42s (- 25m 10s) (1900 12%) loss : 4.557  accuracy : 20.8 %\n",
      "3m 53s (- 24m 52s) (2000 13%) loss : 4.599  accuracy : 18.8 %\n",
      "4m 4s (- 24m 39s) (2100 14%) loss : 4.653  accuracy : 18.8 %\n",
      "4m 16s (- 24m 32s) (2200 14%) loss : 4.557  accuracy : 19.3 %\n",
      "4m 27s (- 24m 16s) (2300 15%) loss : 4.501  accuracy : 19.5 %\n",
      "4m 39s (- 24m 4s) (2400 16%) loss : 4.519  accuracy : 20.6 %\n",
      "4m 50s (- 23m 50s) (2500 16%) loss : 4.436  accuracy : 20.2 %\n",
      "5m 4s (- 23m 49s) (2600 17%) loss : 4.610  accuracy : 18.3 %\n",
      "5m 16s (- 23m 39s) (2700 18%) loss : 4.600  accuracy : 18.9 %\n",
      "5m 29s (- 23m 32s) (2800 18%) loss : 4.631  accuracy : 19.3 %\n",
      "5m 41s (- 23m 22s) (2900 19%) loss : 4.532  accuracy : 19.3 %\n",
      "5m 53s (- 23m 11s) (3000 20%) loss : 4.625  accuracy : 18.3 %\n",
      "6m 3s (- 22m 53s) (3100 20%) loss : 4.431  accuracy : 20.6 %\n",
      "6m 15s (- 22m 42s) (3200 21%) loss : 4.568  accuracy : 19.3 %\n",
      "6m 26s (- 22m 27s) (3300 22%) loss : 4.392  accuracy : 21.3 %\n",
      "6m 36s (- 22m 11s) (3400 22%) loss : 4.492  accuracy : 20.9 %\n",
      "6m 48s (- 21m 59s) (3500 23%) loss : 4.627  accuracy : 18.0 %\n",
      "6m 59s (- 21m 46s) (3600 24%) loss : 4.457  accuracy : 20.7 %\n",
      "7m 9s (- 21m 28s) (3700 24%) loss : 4.488  accuracy : 20.6 %\n",
      "7m 20s (- 21m 16s) (3800 25%) loss : 4.420  accuracy : 20.2 %\n",
      "7m 33s (- 21m 8s) (3900 26%) loss : 4.577  accuracy : 18.5 %\n",
      "7m 47s (- 21m 1s) (4000 27%) loss : 4.516  accuracy : 19.8 %\n",
      "7m 59s (- 20m 51s) (4100 27%) loss : 4.575  accuracy : 19.0 %\n",
      "8m 10s (- 20m 38s) (4200 28%) loss : 4.404  accuracy : 21.6 %\n",
      "8m 22s (- 20m 26s) (4300 29%) loss : 4.430  accuracy : 21.0 %\n",
      "8m 33s (- 20m 13s) (4400 29%) loss : 4.461  accuracy : 21.5 %\n",
      "8m 45s (- 20m 2s) (4500 30%) loss : 4.509  accuracy : 19.2 %\n",
      "8m 57s (- 19m 52s) (4600 31%) loss : 4.346  accuracy : 21.7 %\n",
      "9m 7s (- 19m 36s) (4700 31%) loss : 4.462  accuracy : 20.5 %\n",
      "9m 17s (- 19m 22s) (4800 32%) loss : 4.395  accuracy : 21.8 %\n",
      "9m 29s (- 19m 11s) (4900 33%) loss : 4.477  accuracy : 20.7 %\n",
      "9m 40s (- 18m 59s) (5000 33%) loss : 4.271  accuracy : 22.5 %\n",
      "9m 52s (- 18m 47s) (5100 34%) loss : 4.375  accuracy : 22.3 %\n",
      "10m 3s (- 18m 35s) (5200 35%) loss : 4.465  accuracy : 19.8 %\n",
      "10m 16s (- 18m 24s) (5300 35%) loss : 4.502  accuracy : 21.2 %\n",
      "10m 30s (- 18m 18s) (5400 36%) loss : 4.388  accuracy : 21.0 %\n",
      "10m 41s (- 18m 5s) (5500 37%) loss : 4.329  accuracy : 22.1 %\n",
      "10m 54s (- 17m 55s) (5600 37%) loss : 4.282  accuracy : 22.7 %\n",
      "11m 5s (- 17m 43s) (5700 38%) loss : 4.453  accuracy : 21.6 %\n",
      "11m 18s (- 17m 33s) (5800 39%) loss : 4.370  accuracy : 21.9 %\n",
      "11m 29s (- 17m 21s) (5900 39%) loss : 4.373  accuracy : 22.9 %\n",
      "11m 42s (- 17m 10s) (6000 40%) loss : 4.344  accuracy : 21.4 %\n",
      "11m 54s (- 16m 59s) (6100 41%) loss : 4.368  accuracy : 20.7 %\n",
      "12m 7s (- 16m 49s) (6200 41%) loss : 4.411  accuracy : 20.9 %\n",
      "12m 19s (- 16m 38s) (6300 42%) loss : 4.416  accuracy : 20.4 %\n",
      "12m 32s (- 16m 28s) (6400 43%) loss : 4.246  accuracy : 23.7 %\n",
      "12m 43s (- 16m 16s) (6500 43%) loss : 4.356  accuracy : 22.1 %\n",
      "12m 54s (- 16m 2s) (6600 44%) loss : 4.168  accuracy : 24.5 %\n",
      "13m 7s (- 15m 52s) (6700 45%) loss : 4.538  accuracy : 20.4 %\n",
      "13m 17s (- 15m 38s) (6800 45%) loss : 4.473  accuracy : 20.7 %\n",
      "13m 29s (- 15m 27s) (6900 46%) loss : 4.377  accuracy : 21.7 %\n",
      "13m 41s (- 15m 15s) (7000 47%) loss : 4.480  accuracy : 20.3 %\n",
      "13m 53s (- 15m 5s) (7100 47%) loss : 4.391  accuracy : 20.6 %\n",
      "14m 6s (- 14m 53s) (7200 48%) loss : 4.295  accuracy : 22.6 %\n",
      "14m 17s (- 14m 41s) (7300 49%) loss : 4.374  accuracy : 20.8 %\n",
      "14m 30s (- 14m 30s) (7400 49%) loss : 4.300  accuracy : 22.4 %\n",
      "14m 43s (- 14m 20s) (7500 50%) loss : 4.319  accuracy : 21.7 %\n",
      "14m 54s (- 14m 7s) (7600 51%) loss : 4.429  accuracy : 22.7 %\n",
      "15m 5s (- 13m 55s) (7700 52%) loss : 4.291  accuracy : 22.2 %\n",
      "15m 18s (- 13m 45s) (7800 52%) loss : 4.358  accuracy : 21.3 %\n",
      "15m 29s (- 13m 32s) (7900 53%) loss : 4.270  accuracy : 23.9 %\n",
      "15m 41s (- 13m 20s) (8000 54%) loss : 4.427  accuracy : 21.3 %\n",
      "15m 53s (- 13m 9s) (8100 54%) loss : 4.191  accuracy : 23.7 %\n",
      "16m 4s (- 12m 57s) (8200 55%) loss : 4.279  accuracy : 22.8 %\n",
      "16m 17s (- 12m 45s) (8300 56%) loss : 4.397  accuracy : 21.1 %\n",
      "16m 28s (- 12m 33s) (8400 56%) loss : 4.320  accuracy : 23.1 %\n",
      "16m 40s (- 12m 22s) (8500 57%) loss : 4.334  accuracy : 24.0 %\n",
      "16m 51s (- 12m 9s) (8600 58%) loss : 4.339  accuracy : 22.2 %\n",
      "17m 0s (- 11m 56s) (8700 58%) loss : 4.370  accuracy : 22.8 %\n",
      "17m 12s (- 11m 44s) (8800 59%) loss : 4.450  accuracy : 22.2 %\n",
      "17m 24s (- 11m 33s) (8900 60%) loss : 4.364  accuracy : 20.8 %\n",
      "17m 36s (- 11m 21s) (9000 60%) loss : 4.243  accuracy : 22.1 %\n",
      "17m 47s (- 11m 9s) (9100 61%) loss : 4.346  accuracy : 22.1 %\n",
      "17m 57s (- 10m 56s) (9200 62%) loss : 4.324  accuracy : 22.2 %\n",
      "18m 10s (- 10m 45s) (9300 62%) loss : 4.372  accuracy : 20.9 %\n",
      "18m 21s (- 10m 33s) (9400 63%) loss : 4.337  accuracy : 21.6 %\n",
      "18m 32s (- 10m 21s) (9500 64%) loss : 4.370  accuracy : 21.7 %\n",
      "18m 44s (- 10m 9s) (9600 64%) loss : 4.351  accuracy : 21.6 %\n",
      "18m 56s (- 9m 57s) (9700 65%) loss : 4.181  accuracy : 24.2 %\n",
      "19m 7s (- 9m 46s) (9800 66%) loss : 4.342  accuracy : 22.0 %\n",
      "19m 18s (- 9m 34s) (9900 66%) loss : 4.428  accuracy : 22.1 %\n",
      "19m 28s (- 9m 21s) (10000 67%) loss : 4.280  accuracy : 24.4 %\n",
      "19m 40s (- 9m 9s) (10100 68%) loss : 4.353  accuracy : 21.4 %\n",
      "19m 52s (- 8m 58s) (10200 68%) loss : 4.416  accuracy : 23.4 %\n",
      "20m 4s (- 8m 46s) (10300 69%) loss : 4.282  accuracy : 22.5 %\n",
      "20m 16s (- 8m 35s) (10400 70%) loss : 4.235  accuracy : 23.4 %\n",
      "20m 29s (- 8m 23s) (10500 70%) loss : 4.237  accuracy : 23.7 %\n",
      "20m 41s (- 8m 12s) (10600 71%) loss : 4.278  accuracy : 22.5 %\n",
      "20m 54s (- 8m 1s) (10700 72%) loss : 4.334  accuracy : 23.2 %\n",
      "21m 6s (- 7m 49s) (10800 72%) loss : 4.217  accuracy : 23.5 %\n",
      "21m 18s (- 7m 38s) (10900 73%) loss : 4.138  accuracy : 23.7 %\n",
      "21m 29s (- 7m 26s) (11000 74%) loss : 4.296  accuracy : 23.8 %\n",
      "21m 41s (- 7m 14s) (11100 74%) loss : 4.350  accuracy : 22.1 %\n",
      "21m 51s (- 7m 2s) (11200 75%) loss : 4.214  accuracy : 23.4 %\n",
      "22m 4s (- 6m 50s) (11300 76%) loss : 4.224  accuracy : 23.7 %\n",
      "22m 14s (- 6m 38s) (11400 77%) loss : 4.083  accuracy : 25.3 %\n",
      "22m 27s (- 6m 27s) (11500 77%) loss : 4.211  accuracy : 23.7 %\n",
      "22m 39s (- 6m 15s) (11600 78%) loss : 4.259  accuracy : 22.8 %\n",
      "22m 51s (- 6m 3s) (11700 79%) loss : 4.191  accuracy : 23.8 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 3s (- 5m 52s) (11800 79%) loss : 4.325  accuracy : 22.8 %\n",
      "23m 15s (- 5m 40s) (11900 80%) loss : 4.151  accuracy : 24.4 %\n",
      "23m 28s (- 5m 29s) (12000 81%) loss : 4.298  accuracy : 22.9 %\n",
      "23m 39s (- 5m 17s) (12100 81%) loss : 4.247  accuracy : 24.2 %\n",
      "23m 50s (- 5m 5s) (12200 82%) loss : 4.215  accuracy : 24.8 %\n",
      "24m 1s (- 4m 53s) (12300 83%) loss : 4.242  accuracy : 24.3 %\n",
      "24m 11s (- 4m 41s) (12400 83%) loss : 4.305  accuracy : 23.4 %\n",
      "24m 22s (- 4m 29s) (12500 84%) loss : 4.184  accuracy : 24.1 %\n",
      "24m 33s (- 4m 17s) (12600 85%) loss : 4.281  accuracy : 23.1 %\n",
      "24m 45s (- 4m 6s) (12700 85%) loss : 4.257  accuracy : 24.3 %\n",
      "24m 56s (- 3m 54s) (12800 86%) loss : 4.259  accuracy : 23.2 %\n",
      "25m 9s (- 3m 42s) (12900 87%) loss : 4.281  accuracy : 22.6 %\n",
      "25m 21s (- 3m 31s) (13000 87%) loss : 4.288  accuracy : 22.2 %\n",
      "25m 33s (- 3m 19s) (13100 88%) loss : 4.277  accuracy : 22.9 %\n",
      "25m 44s (- 3m 7s) (13200 89%) loss : 4.179  accuracy : 25.2 %\n",
      "25m 56s (- 2m 56s) (13300 89%) loss : 4.057  accuracy : 25.9 %\n",
      "26m 10s (- 2m 44s) (13400 90%) loss : 4.322  accuracy : 23.0 %\n",
      "26m 22s (- 2m 32s) (13500 91%) loss : 4.098  accuracy : 25.1 %\n",
      "26m 34s (- 2m 21s) (13600 91%) loss : 4.225  accuracy : 23.8 %\n",
      "26m 46s (- 2m 9s) (13700 92%) loss : 4.271  accuracy : 23.7 %\n",
      "26m 57s (- 1m 57s) (13800 93%) loss : 4.145  accuracy : 24.0 %\n",
      "27m 8s (- 1m 46s) (13900 93%) loss : 4.200  accuracy : 24.4 %\n",
      "27m 21s (- 1m 34s) (14000 94%) loss : 4.217  accuracy : 23.8 %\n",
      "27m 33s (- 1m 22s) (14100 95%) loss : 4.408  accuracy : 23.0 %\n",
      "27m 45s (- 1m 10s) (14200 95%) loss : 4.158  accuracy : 24.3 %\n",
      "27m 58s (- 0m 59s) (14300 96%) loss : 4.254  accuracy : 22.9 %\n",
      "28m 8s (- 0m 47s) (14400 97%) loss : 4.100  accuracy : 25.0 %\n",
      "28m 21s (- 0m 35s) (14500 97%) loss : 4.204  accuracy : 23.7 %\n",
      "28m 32s (- 0m 24s) (14600 98%) loss : 4.184  accuracy : 24.2 %\n",
      "28m 45s (- 0m 12s) (14700 99%) loss : 4.253  accuracy : 22.9 %\n",
      "28m 57s (- 0m 0s) (14800 99%) loss : 4.288  accuracy : 23.2 %\n",
      "epoch 1\n",
      "0m 12s (- 29m 54s) (100 0%) loss : 4.210  accuracy : 23.4 %\n",
      "0m 23s (- 28m 50s) (200 1%) loss : 4.125  accuracy : 25.3 %\n",
      "0m 34s (- 28m 8s) (300 2%) loss : 4.095  accuracy : 25.9 %\n",
      "0m 45s (- 27m 8s) (400 2%) loss : 4.081  accuracy : 28.0 %\n",
      "0m 56s (- 27m 7s) (500 3%) loss : 4.113  accuracy : 24.8 %\n",
      "1m 9s (- 27m 19s) (600 4%) loss : 4.232  accuracy : 22.8 %\n",
      "1m 21s (- 27m 22s) (700 4%) loss : 4.102  accuracy : 25.4 %\n",
      "1m 32s (- 26m 51s) (800 5%) loss : 3.947  accuracy : 27.5 %\n",
      "1m 43s (- 26m 39s) (900 6%) loss : 3.904  accuracy : 27.2 %\n",
      "1m 55s (- 26m 37s) (1000 6%) loss : 4.044  accuracy : 26.2 %\n",
      "2m 7s (- 26m 32s) (1100 7%) loss : 4.108  accuracy : 24.7 %\n",
      "2m 19s (- 26m 22s) (1200 8%) loss : 4.175  accuracy : 24.7 %\n",
      "2m 31s (- 26m 10s) (1300 8%) loss : 4.134  accuracy : 24.3 %\n",
      "2m 44s (- 26m 19s) (1400 9%) loss : 4.105  accuracy : 25.6 %\n",
      "2m 57s (- 26m 17s) (1500 10%) loss : 4.191  accuracy : 23.6 %\n",
      "3m 10s (- 26m 14s) (1600 10%) loss : 4.123  accuracy : 25.5 %\n",
      "3m 22s (- 25m 57s) (1700 11%) loss : 4.113  accuracy : 24.8 %\n",
      "3m 33s (- 25m 43s) (1800 12%) loss : 4.057  accuracy : 25.4 %\n",
      "3m 44s (- 25m 25s) (1900 12%) loss : 4.153  accuracy : 25.3 %\n",
      "3m 57s (- 25m 21s) (2000 13%) loss : 4.002  accuracy : 26.6 %\n",
      "4m 9s (- 25m 8s) (2100 14%) loss : 4.148  accuracy : 24.2 %\n",
      "4m 23s (- 25m 9s) (2200 14%) loss : 4.129  accuracy : 25.7 %\n",
      "4m 38s (- 25m 14s) (2300 15%) loss : 4.074  accuracy : 24.4 %\n",
      "4m 52s (- 25m 9s) (2400 16%) loss : 4.027  accuracy : 26.4 %\n",
      "5m 5s (- 25m 2s) (2500 16%) loss : 4.082  accuracy : 23.9 %\n",
      "5m 18s (- 24m 56s) (2600 17%) loss : 4.101  accuracy : 25.9 %\n",
      "5m 29s (- 24m 39s) (2700 18%) loss : 3.984  accuracy : 26.9 %\n",
      "5m 41s (- 24m 24s) (2800 18%) loss : 4.080  accuracy : 25.2 %\n",
      "5m 54s (- 24m 13s) (2900 19%) loss : 4.079  accuracy : 25.1 %\n",
      "6m 5s (- 24m 0s) (3000 20%) loss : 4.059  accuracy : 25.9 %\n",
      "6m 18s (- 23m 48s) (3100 20%) loss : 4.040  accuracy : 26.6 %\n",
      "6m 28s (- 23m 30s) (3200 21%) loss : 4.069  accuracy : 24.5 %\n",
      "6m 40s (- 23m 17s) (3300 22%) loss : 4.122  accuracy : 24.7 %\n",
      "6m 52s (- 23m 3s) (3400 22%) loss : 4.182  accuracy : 24.4 %\n",
      "7m 5s (- 22m 52s) (3500 23%) loss : 3.989  accuracy : 27.1 %\n",
      "7m 17s (- 22m 41s) (3600 24%) loss : 4.091  accuracy : 25.3 %\n",
      "7m 30s (- 22m 31s) (3700 24%) loss : 4.114  accuracy : 25.0 %\n",
      "7m 41s (- 22m 16s) (3800 25%) loss : 4.155  accuracy : 24.5 %\n",
      "7m 51s (- 21m 59s) (3900 26%) loss : 4.131  accuracy : 24.8 %\n",
      "8m 3s (- 21m 44s) (4000 27%) loss : 4.206  accuracy : 24.1 %\n",
      "8m 16s (- 21m 35s) (4100 27%) loss : 4.105  accuracy : 25.6 %\n",
      "8m 27s (- 21m 22s) (4200 28%) loss : 3.925  accuracy : 27.1 %\n",
      "8m 38s (- 21m 5s) (4300 29%) loss : 4.083  accuracy : 26.5 %\n",
      "8m 49s (- 20m 52s) (4400 29%) loss : 4.005  accuracy : 25.7 %\n",
      "9m 1s (- 20m 39s) (4500 30%) loss : 4.068  accuracy : 25.9 %\n",
      "9m 13s (- 20m 26s) (4600 31%) loss : 4.213  accuracy : 24.1 %\n",
      "9m 25s (- 20m 15s) (4700 31%) loss : 4.077  accuracy : 25.2 %\n",
      "9m 35s (- 19m 58s) (4800 32%) loss : 4.052  accuracy : 26.4 %\n",
      "9m 47s (- 19m 47s) (4900 33%) loss : 4.085  accuracy : 25.0 %\n",
      "9m 59s (- 19m 34s) (5000 33%) loss : 4.138  accuracy : 24.2 %\n",
      "10m 12s (- 19m 25s) (5100 34%) loss : 4.217  accuracy : 24.6 %\n",
      "10m 23s (- 19m 12s) (5200 35%) loss : 3.977  accuracy : 28.1 %\n",
      "10m 34s (- 18m 57s) (5300 35%) loss : 3.986  accuracy : 26.1 %\n",
      "10m 44s (- 18m 42s) (5400 36%) loss : 4.072  accuracy : 25.4 %\n",
      "10m 57s (- 18m 33s) (5500 37%) loss : 4.104  accuracy : 26.4 %\n",
      "11m 8s (- 18m 19s) (5600 37%) loss : 3.971  accuracy : 27.1 %\n",
      "11m 20s (- 18m 7s) (5700 38%) loss : 4.156  accuracy : 25.8 %\n",
      "11m 31s (- 17m 53s) (5800 39%) loss : 4.066  accuracy : 25.4 %\n",
      "11m 45s (- 17m 44s) (5900 39%) loss : 4.053  accuracy : 25.8 %\n",
      "11m 58s (- 17m 34s) (6000 40%) loss : 4.095  accuracy : 24.6 %\n",
      "12m 8s (- 17m 19s) (6100 41%) loss : 3.950  accuracy : 27.8 %\n",
      "12m 19s (- 17m 6s) (6200 41%) loss : 4.084  accuracy : 25.6 %\n",
      "12m 31s (- 16m 55s) (6300 42%) loss : 4.023  accuracy : 25.3 %\n",
      "12m 44s (- 16m 44s) (6400 43%) loss : 4.185  accuracy : 24.2 %\n",
      "12m 56s (- 16m 32s) (6500 43%) loss : 4.053  accuracy : 26.5 %\n",
      "13m 8s (- 16m 19s) (6600 44%) loss : 4.056  accuracy : 25.7 %\n",
      "13m 19s (- 16m 7s) (6700 45%) loss : 4.142  accuracy : 25.1 %\n",
      "13m 32s (- 15m 55s) (6800 45%) loss : 4.049  accuracy : 26.4 %\n",
      "13m 45s (- 15m 45s) (6900 46%) loss : 4.053  accuracy : 25.5 %\n",
      "13m 57s (- 15m 33s) (7000 47%) loss : 3.972  accuracy : 28.1 %\n",
      "14m 9s (- 15m 22s) (7100 47%) loss : 3.918  accuracy : 27.9 %\n",
      "14m 21s (- 15m 9s) (7200 48%) loss : 3.945  accuracy : 27.2 %\n",
      "14m 33s (- 14m 58s) (7300 49%) loss : 4.024  accuracy : 27.2 %\n",
      "14m 46s (- 14m 47s) (7400 49%) loss : 4.086  accuracy : 26.1 %\n",
      "15m 1s (- 14m 38s) (7500 50%) loss : 4.068  accuracy : 25.5 %\n",
      "15m 14s (- 14m 26s) (7600 51%) loss : 3.973  accuracy : 26.7 %\n",
      "15m 26s (- 14m 14s) (7700 52%) loss : 4.120  accuracy : 24.1 %\n",
      "15m 38s (- 14m 2s) (7800 52%) loss : 4.022  accuracy : 25.9 %\n",
      "15m 48s (- 13m 49s) (7900 53%) loss : 4.073  accuracy : 25.9 %\n",
      "16m 1s (- 13m 38s) (8000 54%) loss : 4.050  accuracy : 25.0 %\n",
      "16m 12s (- 13m 24s) (8100 54%) loss : 4.040  accuracy : 26.1 %\n",
      "16m 23s (- 13m 11s) (8200 55%) loss : 4.086  accuracy : 25.4 %\n",
      "16m 37s (- 13m 2s) (8300 56%) loss : 4.139  accuracy : 25.0 %\n",
      "16m 49s (- 12m 49s) (8400 56%) loss : 4.037  accuracy : 27.0 %\n",
      "17m 1s (- 12m 38s) (8500 57%) loss : 3.945  accuracy : 26.8 %\n",
      "17m 14s (- 12m 26s) (8600 58%) loss : 3.994  accuracy : 27.2 %\n",
      "17m 24s (- 12m 13s) (8700 58%) loss : 3.998  accuracy : 26.8 %\n",
      "17m 35s (- 12m 0s) (8800 59%) loss : 4.087  accuracy : 25.6 %\n",
      "17m 46s (- 11m 47s) (8900 60%) loss : 4.012  accuracy : 26.7 %\n",
      "17m 58s (- 11m 35s) (9000 60%) loss : 4.106  accuracy : 25.7 %\n",
      "18m 10s (- 11m 23s) (9100 61%) loss : 4.007  accuracy : 27.5 %\n",
      "18m 21s (- 11m 10s) (9200 62%) loss : 4.082  accuracy : 24.9 %\n",
      "18m 33s (- 10m 58s) (9300 62%) loss : 4.101  accuracy : 26.0 %\n",
      "18m 45s (- 10m 47s) (9400 63%) loss : 4.031  accuracy : 27.0 %\n",
      "18m 55s (- 10m 34s) (9500 64%) loss : 3.948  accuracy : 27.6 %\n",
      "19m 7s (- 10m 22s) (9600 64%) loss : 3.988  accuracy : 25.9 %\n",
      "19m 19s (- 10m 10s) (9700 65%) loss : 4.096  accuracy : 24.3 %\n",
      "19m 31s (- 9m 58s) (9800 66%) loss : 4.082  accuracy : 26.1 %\n",
      "19m 43s (- 9m 46s) (9900 66%) loss : 4.002  accuracy : 26.2 %\n",
      "19m 53s (- 9m 33s) (10000 67%) loss : 4.039  accuracy : 25.5 %\n",
      "20m 5s (- 9m 21s) (10100 68%) loss : 3.981  accuracy : 26.6 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 16s (- 9m 9s) (10200 68%) loss : 4.018  accuracy : 25.9 %\n",
      "20m 28s (- 8m 57s) (10300 69%) loss : 4.069  accuracy : 25.3 %\n",
      "20m 41s (- 8m 45s) (10400 70%) loss : 4.095  accuracy : 24.7 %\n",
      "20m 52s (- 8m 33s) (10500 70%) loss : 4.046  accuracy : 25.2 %\n",
      "21m 5s (- 8m 21s) (10600 71%) loss : 4.196  accuracy : 24.3 %\n",
      "21m 18s (- 8m 10s) (10700 72%) loss : 4.078  accuracy : 26.1 %\n",
      "21m 29s (- 7m 58s) (10800 72%) loss : 4.139  accuracy : 25.8 %\n",
      "21m 40s (- 7m 46s) (10900 73%) loss : 4.040  accuracy : 26.3 %\n",
      "21m 53s (- 7m 34s) (11000 74%) loss : 4.004  accuracy : 26.8 %\n",
      "22m 4s (- 7m 22s) (11100 74%) loss : 3.991  accuracy : 26.1 %\n",
      "22m 17s (- 7m 10s) (11200 75%) loss : 3.903  accuracy : 26.8 %\n",
      "22m 28s (- 6m 58s) (11300 76%) loss : 4.058  accuracy : 25.5 %\n",
      "22m 38s (- 6m 45s) (11400 77%) loss : 3.994  accuracy : 25.9 %\n",
      "22m 50s (- 6m 33s) (11500 77%) loss : 4.099  accuracy : 25.5 %\n",
      "23m 3s (- 6m 22s) (11600 78%) loss : 3.958  accuracy : 26.8 %\n",
      "23m 16s (- 6m 10s) (11700 79%) loss : 4.024  accuracy : 27.4 %\n",
      "23m 29s (- 5m 58s) (11800 79%) loss : 4.019  accuracy : 27.8 %\n",
      "23m 41s (- 5m 46s) (11900 80%) loss : 3.921  accuracy : 28.0 %\n",
      "23m 52s (- 5m 34s) (12000 81%) loss : 4.125  accuracy : 26.2 %\n",
      "24m 3s (- 5m 22s) (12100 81%) loss : 3.908  accuracy : 28.2 %\n",
      "24m 16s (- 5m 10s) (12200 82%) loss : 3.973  accuracy : 26.3 %\n",
      "24m 28s (- 4m 59s) (12300 83%) loss : 3.913  accuracy : 28.1 %\n",
      "24m 40s (- 4m 47s) (12400 83%) loss : 4.078  accuracy : 24.9 %\n",
      "24m 52s (- 4m 35s) (12500 84%) loss : 4.018  accuracy : 26.4 %\n",
      "25m 3s (- 4m 23s) (12600 85%) loss : 4.163  accuracy : 25.5 %\n",
      "25m 16s (- 4m 11s) (12700 85%) loss : 4.032  accuracy : 26.7 %\n",
      "25m 29s (- 3m 59s) (12800 86%) loss : 4.036  accuracy : 25.5 %\n",
      "25m 43s (- 3m 47s) (12900 87%) loss : 4.045  accuracy : 25.1 %\n",
      "25m 55s (- 3m 35s) (13000 87%) loss : 4.114  accuracy : 25.0 %\n",
      "26m 7s (- 3m 24s) (13100 88%) loss : 4.021  accuracy : 26.2 %\n",
      "26m 18s (- 3m 11s) (13200 89%) loss : 3.951  accuracy : 28.1 %\n",
      "26m 30s (- 2m 59s) (13300 89%) loss : 4.057  accuracy : 26.1 %\n",
      "26m 42s (- 2m 47s) (13400 90%) loss : 4.109  accuracy : 24.3 %\n",
      "26m 54s (- 2m 36s) (13500 91%) loss : 4.027  accuracy : 26.9 %\n",
      "27m 6s (- 2m 24s) (13600 91%) loss : 4.011  accuracy : 26.0 %\n",
      "27m 18s (- 2m 12s) (13700 92%) loss : 4.116  accuracy : 25.0 %\n",
      "27m 29s (- 2m 0s) (13800 93%) loss : 4.143  accuracy : 24.4 %\n",
      "27m 42s (- 1m 48s) (13900 93%) loss : 4.049  accuracy : 25.1 %\n",
      "27m 53s (- 1m 36s) (14000 94%) loss : 4.039  accuracy : 25.4 %\n",
      "28m 5s (- 1m 24s) (14100 95%) loss : 3.991  accuracy : 27.1 %\n",
      "28m 17s (- 1m 12s) (14200 95%) loss : 4.017  accuracy : 26.3 %\n",
      "28m 28s (- 1m 0s) (14300 96%) loss : 4.078  accuracy : 25.4 %\n",
      "28m 38s (- 0m 48s) (14400 97%) loss : 3.967  accuracy : 28.1 %\n",
      "28m 51s (- 0m 36s) (14500 97%) loss : 4.054  accuracy : 26.2 %\n",
      "29m 5s (- 0m 24s) (14600 98%) loss : 4.013  accuracy : 26.4 %\n",
      "29m 17s (- 0m 12s) (14700 99%) loss : 4.065  accuracy : 26.8 %\n",
      "29m 28s (- 0m 0s) (14800 99%) loss : 3.987  accuracy : 26.9 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches, epochs = 1, lr = 0.005,  print_every = 100)\n",
    "denoiser.fit(batches, epochs = 1, lr = 0.002,  print_every = 100)\n",
    "denoiser.fit(batches, epochs = 1, lr = 0.0005, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(denoiser.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser_2.pth')\n",
    "\n",
    "# load\n",
    "#denoiser.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
