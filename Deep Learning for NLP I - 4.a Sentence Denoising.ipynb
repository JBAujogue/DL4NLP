{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 4.a <br><br><br>\n",
    "  Sentence Denoising\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=red>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global structure of the [sentence denoiser](#sentence_denoiser) is the pipeline of two modules, followed by a final classification layer, as for Language Models in **Part I - 3**.\n",
    "\n",
    "Training follows a denoising objective known as _Cloze task_, which is used :\n",
    "\n",
    "- For the BERT model in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 0.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\chatNLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus et donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text, label]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importWords(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importAllWords(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importWords(file_name))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def importSentences(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        sentences = []\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                sentences.append(cleanSentence(current_tile))\n",
    "                title = current_tile\n",
    "            sentences.append(cleanSentence(str(pair[1]).split(' | ')[-1]))\n",
    "        return sentences\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.ix[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.ix[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    sentences = cleanDatabase(db)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def importAllSentences(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus += importSentences(file_name)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importAllWords(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31574"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = importAllSentences(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "## 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from chatNLP.models.Word_Embedding import Word2VecConnector\n",
    "from chatNLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = FastText(size = 75, \n",
    "                             window = 5, \n",
    "                             min_count = 1, \n",
    "                             negative = 20,\n",
    "                             sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8086"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fastText_word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec.train(sentences = corpus, \n",
    "                        epochs = 50,\n",
    "                        total_examples = fastText_word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(fastText_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatNLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sentence_denoiser\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentence denoising Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDenoiser(nn.Module) :\n",
    "    def __init__(self, device, word2vec, hidden_dim, n_layers, dropout = 0, class_weights = None, optimizer = optim.SGD) :\n",
    "        super(SentenceDenoiser, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence = '.', hidden = None, limit = 10, color_code = '\\033[94m'):\n",
    "        words  = sentence.split(' ')\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1, :, :]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return\n",
    "    \n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0.15,\n",
    "                                predict_masked_only = True,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.ignore_index\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index\n",
    "            \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            # prepare pack\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            pack = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack]\n",
    "            pack = [[w for w in words if w is not None] for words in pack]\n",
    "            mask = [random.sample([_ for _ in range(len(p))], k = int(mask_ratio*len(p) +1)) for p in pack]\n",
    "            # split into input and target pack\n",
    "            pack0 = [[maskInput( s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)]\n",
    "            pack1 = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask)] if predict_masked_only else pack\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index))\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index)\n",
    "            success = sum([self.ignore_index != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets   = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900588"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser = SentenceDenoiser(device,\n",
    "                            word2vec,\n",
    "                            hidden_dim = 50, \n",
    "                            n_layers = 2, \n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.SGD)\n",
    "\n",
    "denoiser.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceDenoiser(\n",
       "  (word2vec): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(8088, 75)\n",
       "    )\n",
       "    (embedding): Embedding(8088, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (bigru): GRU(75, 50, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (out): Linear(in_features=100, out_features=8088, bias=True)\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14805"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for seed in [42, 854, 3, 7956, 881125, 76, 5721, 1499, 8752, 374, 14758, 23, 9543, 856, 75] :\n",
    "    batches += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 32,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                predict_masked_only = True,\n",
    "                                                seed = seed)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 12s (- 29m 52s) (100 0%) loss : 8.184  accuracy : 3.5 %\n",
      "0m 24s (- 29m 18s) (200 1%) loss : 7.031  accuracy : 4.2 %\n",
      "0m 36s (- 29m 34s) (300 2%) loss : 6.894  accuracy : 4.6 %\n",
      "0m 49s (- 29m 25s) (400 2%) loss : 6.775  accuracy : 4.9 %\n",
      "1m 0s (- 29m 1s) (500 3%) loss : 6.688  accuracy : 5.1 %\n",
      "1m 11s (- 28m 15s) (600 4%) loss : 6.405  accuracy : 5.9 %\n",
      "1m 23s (- 28m 5s) (700 4%) loss : 6.514  accuracy : 5.9 %\n",
      "1m 37s (- 28m 19s) (800 5%) loss : 6.348  accuracy : 7.0 %\n",
      "1m 48s (- 27m 49s) (900 6%) loss : 6.070  accuracy : 9.0 %\n",
      "1m 59s (- 27m 34s) (1000 6%) loss : 6.109  accuracy : 9.3 %\n",
      "2m 11s (- 27m 21s) (1100 7%) loss : 5.909  accuracy : 11.1 %\n",
      "2m 23s (- 27m 6s) (1200 8%) loss : 5.835  accuracy : 11.9 %\n",
      "2m 34s (- 26m 41s) (1300 8%) loss : 5.645  accuracy : 13.0 %\n",
      "2m 46s (- 26m 33s) (1400 9%) loss : 5.633  accuracy : 13.2 %\n",
      "2m 57s (- 26m 16s) (1500 10%) loss : 5.474  accuracy : 14.0 %\n",
      "3m 9s (- 26m 0s) (1600 10%) loss : 5.436  accuracy : 14.8 %\n",
      "3m 19s (- 25m 40s) (1700 11%) loss : 5.292  accuracy : 15.5 %\n",
      "3m 30s (- 25m 22s) (1800 12%) loss : 5.277  accuracy : 16.4 %\n",
      "3m 42s (- 25m 8s) (1900 12%) loss : 5.248  accuracy : 16.8 %\n",
      "3m 53s (- 24m 54s) (2000 13%) loss : 5.176  accuracy : 17.1 %\n",
      "4m 4s (- 24m 40s) (2100 14%) loss : 5.158  accuracy : 18.0 %\n",
      "4m 16s (- 24m 30s) (2200 14%) loss : 5.011  accuracy : 19.9 %\n",
      "4m 28s (- 24m 19s) (2300 15%) loss : 4.928  accuracy : 19.6 %\n",
      "4m 40s (- 24m 7s) (2400 16%) loss : 4.970  accuracy : 20.4 %\n",
      "4m 51s (- 23m 53s) (2500 16%) loss : 4.886  accuracy : 20.2 %\n",
      "5m 3s (- 23m 45s) (2600 17%) loss : 4.731  accuracy : 23.0 %\n",
      "5m 15s (- 23m 32s) (2700 18%) loss : 4.813  accuracy : 21.7 %\n",
      "5m 26s (- 23m 21s) (2800 18%) loss : 4.696  accuracy : 21.3 %\n",
      "5m 38s (- 23m 8s) (2900 19%) loss : 4.536  accuracy : 24.3 %\n",
      "5m 50s (- 22m 57s) (3000 20%) loss : 4.478  accuracy : 23.9 %\n",
      "6m 2s (- 22m 46s) (3100 20%) loss : 4.664  accuracy : 21.5 %\n",
      "6m 13s (- 22m 34s) (3200 21%) loss : 4.515  accuracy : 24.8 %\n",
      "6m 25s (- 22m 25s) (3300 22%) loss : 4.426  accuracy : 24.9 %\n",
      "6m 38s (- 22m 16s) (3400 22%) loss : 4.439  accuracy : 24.2 %\n",
      "6m 51s (- 22m 8s) (3500 23%) loss : 4.407  accuracy : 25.1 %\n",
      "7m 2s (- 21m 53s) (3600 24%) loss : 4.446  accuracy : 25.8 %\n",
      "7m 13s (- 21m 41s) (3700 24%) loss : 4.355  accuracy : 25.5 %\n",
      "7m 25s (- 21m 30s) (3800 25%) loss : 4.258  accuracy : 28.1 %\n",
      "7m 38s (- 21m 22s) (3900 26%) loss : 4.265  accuracy : 27.9 %\n",
      "7m 51s (- 21m 14s) (4000 27%) loss : 4.348  accuracy : 25.7 %\n",
      "8m 3s (- 21m 2s) (4100 27%) loss : 4.163  accuracy : 29.1 %\n",
      "8m 15s (- 20m 49s) (4200 28%) loss : 4.248  accuracy : 27.6 %\n",
      "8m 24s (- 20m 32s) (4300 29%) loss : 4.175  accuracy : 27.6 %\n",
      "8m 35s (- 20m 19s) (4400 29%) loss : 4.021  accuracy : 29.0 %\n",
      "8m 47s (- 20m 8s) (4500 30%) loss : 4.150  accuracy : 28.0 %\n",
      "8m 59s (- 19m 57s) (4600 31%) loss : 4.022  accuracy : 31.4 %\n",
      "9m 11s (- 19m 45s) (4700 31%) loss : 4.066  accuracy : 28.4 %\n",
      "9m 22s (- 19m 32s) (4800 32%) loss : 4.088  accuracy : 28.9 %\n",
      "9m 34s (- 19m 20s) (4900 33%) loss : 4.057  accuracy : 28.6 %\n",
      "9m 46s (- 19m 9s) (5000 33%) loss : 4.047  accuracy : 29.6 %\n",
      "9m 56s (- 18m 55s) (5100 34%) loss : 3.967  accuracy : 30.8 %\n",
      "10m 7s (- 18m 41s) (5200 35%) loss : 3.972  accuracy : 29.4 %\n",
      "10m 18s (- 18m 29s) (5300 35%) loss : 4.005  accuracy : 29.9 %\n",
      "10m 30s (- 18m 17s) (5400 36%) loss : 3.834  accuracy : 32.5 %\n",
      "10m 41s (- 18m 5s) (5500 37%) loss : 4.000  accuracy : 29.3 %\n",
      "10m 53s (- 17m 53s) (5600 37%) loss : 4.071  accuracy : 28.9 %\n",
      "11m 2s (- 17m 38s) (5700 38%) loss : 3.964  accuracy : 30.2 %\n",
      "11m 14s (- 17m 27s) (5800 39%) loss : 3.974  accuracy : 29.4 %\n",
      "11m 25s (- 17m 14s) (5900 39%) loss : 3.862  accuracy : 30.9 %\n",
      "11m 36s (- 17m 1s) (6000 40%) loss : 3.831  accuracy : 31.4 %\n",
      "11m 48s (- 16m 51s) (6100 41%) loss : 3.731  accuracy : 33.6 %\n",
      "12m 0s (- 16m 40s) (6200 41%) loss : 3.820  accuracy : 32.6 %\n",
      "12m 11s (- 16m 27s) (6300 42%) loss : 3.842  accuracy : 31.3 %\n",
      "12m 23s (- 16m 15s) (6400 43%) loss : 3.906  accuracy : 31.4 %\n",
      "12m 34s (- 16m 4s) (6500 43%) loss : 3.889  accuracy : 31.1 %\n",
      "12m 46s (- 15m 53s) (6600 44%) loss : 3.803  accuracy : 32.2 %\n",
      "12m 57s (- 15m 41s) (6700 45%) loss : 3.791  accuracy : 30.6 %\n",
      "13m 8s (- 15m 28s) (6800 45%) loss : 3.933  accuracy : 30.8 %\n",
      "13m 20s (- 15m 16s) (6900 46%) loss : 3.613  accuracy : 34.8 %\n",
      "13m 30s (- 15m 3s) (7000 47%) loss : 3.765  accuracy : 32.8 %\n",
      "13m 42s (- 14m 52s) (7100 47%) loss : 3.885  accuracy : 29.9 %\n",
      "13m 53s (- 14m 40s) (7200 48%) loss : 3.662  accuracy : 33.8 %\n",
      "14m 5s (- 14m 29s) (7300 49%) loss : 3.691  accuracy : 31.7 %\n",
      "14m 17s (- 14m 18s) (7400 49%) loss : 3.740  accuracy : 32.2 %\n",
      "14m 30s (- 14m 7s) (7500 50%) loss : 3.669  accuracy : 32.7 %\n",
      "14m 41s (- 13m 55s) (7600 51%) loss : 3.751  accuracy : 32.3 %\n",
      "14m 53s (- 13m 44s) (7700 52%) loss : 3.673  accuracy : 34.0 %\n",
      "15m 4s (- 13m 31s) (7800 52%) loss : 3.632  accuracy : 33.8 %\n",
      "15m 15s (- 13m 19s) (7900 53%) loss : 3.566  accuracy : 33.2 %\n",
      "15m 27s (- 13m 8s) (8000 54%) loss : 3.631  accuracy : 33.9 %\n",
      "15m 37s (- 12m 56s) (8100 54%) loss : 3.656  accuracy : 34.6 %\n",
      "15m 48s (- 12m 44s) (8200 55%) loss : 3.592  accuracy : 33.7 %\n",
      "16m 0s (- 12m 32s) (8300 56%) loss : 3.610  accuracy : 34.7 %\n",
      "16m 13s (- 12m 21s) (8400 56%) loss : 3.678  accuracy : 34.4 %\n",
      "16m 25s (- 12m 11s) (8500 57%) loss : 3.568  accuracy : 34.2 %\n",
      "16m 38s (- 12m 0s) (8600 58%) loss : 3.746  accuracy : 31.8 %\n",
      "16m 48s (- 11m 47s) (8700 58%) loss : 3.538  accuracy : 33.5 %\n",
      "17m 0s (- 11m 36s) (8800 59%) loss : 3.530  accuracy : 34.2 %\n",
      "17m 11s (- 11m 24s) (8900 60%) loss : 3.693  accuracy : 33.2 %\n",
      "17m 22s (- 11m 12s) (9000 60%) loss : 3.549  accuracy : 35.1 %\n",
      "17m 35s (- 11m 1s) (9100 61%) loss : 3.692  accuracy : 32.9 %\n",
      "17m 46s (- 10m 49s) (9200 62%) loss : 3.599  accuracy : 35.6 %\n",
      "17m 57s (- 10m 38s) (9300 62%) loss : 3.520  accuracy : 36.4 %\n",
      "18m 8s (- 10m 25s) (9400 63%) loss : 3.512  accuracy : 34.0 %\n",
      "18m 21s (- 10m 15s) (9500 64%) loss : 3.630  accuracy : 33.3 %\n",
      "18m 32s (- 10m 3s) (9600 64%) loss : 3.659  accuracy : 33.7 %\n",
      "18m 45s (- 9m 52s) (9700 65%) loss : 3.678  accuracy : 33.0 %\n",
      "18m 57s (- 9m 40s) (9800 66%) loss : 3.574  accuracy : 34.3 %\n",
      "19m 9s (- 9m 29s) (9900 66%) loss : 3.606  accuracy : 34.1 %\n",
      "19m 21s (- 9m 18s) (10000 67%) loss : 3.602  accuracy : 33.6 %\n",
      "19m 33s (- 9m 6s) (10100 68%) loss : 3.484  accuracy : 36.4 %\n",
      "19m 45s (- 8m 54s) (10200 68%) loss : 3.562  accuracy : 34.2 %\n",
      "19m 55s (- 8m 42s) (10300 69%) loss : 3.580  accuracy : 34.1 %\n",
      "20m 7s (- 8m 31s) (10400 70%) loss : 3.558  accuracy : 35.1 %\n",
      "20m 19s (- 8m 20s) (10500 70%) loss : 3.438  accuracy : 35.9 %\n",
      "20m 31s (- 8m 8s) (10600 71%) loss : 3.528  accuracy : 34.7 %\n",
      "20m 44s (- 7m 57s) (10700 72%) loss : 3.535  accuracy : 34.6 %\n",
      "20m 56s (- 7m 46s) (10800 72%) loss : 3.497  accuracy : 35.2 %\n",
      "21m 8s (- 7m 34s) (10900 73%) loss : 3.547  accuracy : 33.3 %\n",
      "21m 20s (- 7m 23s) (11000 74%) loss : 3.465  accuracy : 35.6 %\n",
      "21m 32s (- 7m 11s) (11100 74%) loss : 3.401  accuracy : 36.4 %\n",
      "21m 43s (- 6m 59s) (11200 75%) loss : 3.440  accuracy : 37.9 %\n",
      "21m 54s (- 6m 47s) (11300 76%) loss : 3.501  accuracy : 35.4 %\n",
      "22m 5s (- 6m 36s) (11400 77%) loss : 3.572  accuracy : 34.3 %\n",
      "22m 18s (- 6m 24s) (11500 77%) loss : 3.390  accuracy : 36.7 %\n",
      "22m 31s (- 6m 13s) (11600 78%) loss : 3.332  accuracy : 37.0 %\n",
      "22m 42s (- 6m 1s) (11700 79%) loss : 3.498  accuracy : 35.0 %\n",
      "22m 53s (- 5m 49s) (11800 79%) loss : 3.484  accuracy : 34.9 %\n",
      "23m 4s (- 5m 37s) (11900 80%) loss : 3.381  accuracy : 37.5 %\n",
      "23m 16s (- 5m 26s) (12000 81%) loss : 3.516  accuracy : 34.0 %\n",
      "23m 29s (- 5m 15s) (12100 81%) loss : 3.425  accuracy : 37.4 %\n",
      "23m 41s (- 5m 3s) (12200 82%) loss : 3.485  accuracy : 35.7 %\n",
      "23m 52s (- 4m 51s) (12300 83%) loss : 3.499  accuracy : 34.0 %\n",
      "24m 3s (- 4m 39s) (12400 83%) loss : 3.463  accuracy : 36.4 %\n",
      "24m 14s (- 4m 28s) (12500 84%) loss : 3.345  accuracy : 37.2 %\n",
      "24m 25s (- 4m 16s) (12600 85%) loss : 3.498  accuracy : 33.7 %\n",
      "24m 37s (- 4m 4s) (12700 85%) loss : 3.313  accuracy : 35.9 %\n",
      "24m 47s (- 3m 53s) (12800 86%) loss : 3.519  accuracy : 35.6 %\n",
      "24m 58s (- 3m 41s) (12900 87%) loss : 3.335  accuracy : 37.0 %\n",
      "25m 10s (- 3m 29s) (13000 87%) loss : 3.457  accuracy : 33.3 %\n",
      "25m 22s (- 3m 18s) (13100 88%) loss : 3.564  accuracy : 32.3 %\n",
      "25m 34s (- 3m 6s) (13200 89%) loss : 3.450  accuracy : 35.9 %\n",
      "25m 45s (- 2m 54s) (13300 89%) loss : 3.415  accuracy : 35.3 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25m 55s (- 2m 43s) (13400 90%) loss : 3.214  accuracy : 38.6 %\n",
      "26m 8s (- 2m 31s) (13500 91%) loss : 3.499  accuracy : 33.9 %\n",
      "26m 20s (- 2m 20s) (13600 91%) loss : 3.324  accuracy : 37.3 %\n",
      "26m 34s (- 2m 8s) (13700 92%) loss : 3.397  accuracy : 36.6 %\n",
      "26m 45s (- 1m 56s) (13800 93%) loss : 3.315  accuracy : 35.8 %\n",
      "26m 55s (- 1m 45s) (13900 93%) loss : 3.408  accuracy : 35.8 %\n",
      "27m 6s (- 1m 33s) (14000 94%) loss : 3.375  accuracy : 37.9 %\n",
      "27m 20s (- 1m 22s) (14100 95%) loss : 3.438  accuracy : 36.9 %\n",
      "27m 32s (- 1m 10s) (14200 95%) loss : 3.396  accuracy : 36.6 %\n",
      "27m 42s (- 0m 58s) (14300 96%) loss : 3.297  accuracy : 36.3 %\n",
      "27m 55s (- 0m 47s) (14400 97%) loss : 3.403  accuracy : 36.5 %\n",
      "28m 6s (- 0m 35s) (14500 97%) loss : 3.399  accuracy : 33.6 %\n",
      "28m 17s (- 0m 23s) (14600 98%) loss : 3.377  accuracy : 36.6 %\n",
      "28m 29s (- 0m 12s) (14700 99%) loss : 3.453  accuracy : 34.8 %\n",
      "28m 40s (- 0m 0s) (14800 99%) loss : 3.254  accuracy : 38.7 %\n",
      "epoch 1\n",
      "0m 10s (- 25m 40s) (100 0%) loss : 3.330  accuracy : 36.2 %\n",
      "0m 22s (- 27m 14s) (200 1%) loss : 3.160  accuracy : 38.8 %\n",
      "0m 33s (- 26m 59s) (300 2%) loss : 3.118  accuracy : 39.8 %\n",
      "0m 43s (- 26m 17s) (400 2%) loss : 3.151  accuracy : 40.1 %\n",
      "0m 54s (- 26m 13s) (500 3%) loss : 3.189  accuracy : 39.5 %\n",
      "1m 6s (- 26m 18s) (600 4%) loss : 2.995  accuracy : 42.6 %\n",
      "1m 18s (- 26m 23s) (700 4%) loss : 3.203  accuracy : 39.3 %\n",
      "1m 29s (- 26m 11s) (800 5%) loss : 3.164  accuracy : 40.3 %\n",
      "1m 40s (- 25m 52s) (900 6%) loss : 3.031  accuracy : 43.5 %\n",
      "1m 52s (- 25m 56s) (1000 6%) loss : 3.208  accuracy : 39.2 %\n",
      "2m 3s (- 25m 36s) (1100 7%) loss : 3.141  accuracy : 38.5 %\n",
      "2m 13s (- 25m 14s) (1200 8%) loss : 3.042  accuracy : 41.6 %\n",
      "2m 25s (- 25m 16s) (1300 8%) loss : 3.222  accuracy : 38.2 %\n",
      "2m 37s (- 25m 7s) (1400 9%) loss : 3.178  accuracy : 37.9 %\n",
      "2m 48s (- 24m 58s) (1500 10%) loss : 3.205  accuracy : 38.0 %\n",
      "3m 1s (- 24m 55s) (1600 10%) loss : 3.239  accuracy : 38.0 %\n",
      "3m 14s (- 24m 57s) (1700 11%) loss : 3.131  accuracy : 40.1 %\n",
      "3m 26s (- 24m 51s) (1800 12%) loss : 3.236  accuracy : 38.0 %\n",
      "3m 36s (- 24m 30s) (1900 12%) loss : 3.057  accuracy : 41.0 %\n",
      "3m 46s (- 24m 12s) (2000 13%) loss : 3.326  accuracy : 36.9 %\n",
      "3m 58s (- 24m 2s) (2100 14%) loss : 3.260  accuracy : 38.9 %\n",
      "4m 10s (- 23m 56s) (2200 14%) loss : 3.139  accuracy : 39.2 %\n",
      "4m 23s (- 23m 51s) (2300 15%) loss : 3.115  accuracy : 40.1 %\n",
      "4m 35s (- 23m 43s) (2400 16%) loss : 3.144  accuracy : 40.8 %\n",
      "4m 46s (- 23m 29s) (2500 16%) loss : 2.948  accuracy : 42.9 %\n",
      "4m 56s (- 23m 10s) (2600 17%) loss : 3.245  accuracy : 37.4 %\n",
      "5m 9s (- 23m 6s) (2700 18%) loss : 2.968  accuracy : 41.6 %\n",
      "5m 20s (- 22m 53s) (2800 18%) loss : 3.152  accuracy : 41.2 %\n",
      "5m 32s (- 22m 43s) (2900 19%) loss : 3.163  accuracy : 38.9 %\n",
      "5m 43s (- 22m 32s) (3000 20%) loss : 3.279  accuracy : 36.9 %\n",
      "5m 55s (- 22m 20s) (3100 20%) loss : 3.099  accuracy : 41.1 %\n",
      "6m 6s (- 22m 7s) (3200 21%) loss : 3.165  accuracy : 39.6 %\n",
      "6m 17s (- 21m 56s) (3300 22%) loss : 3.163  accuracy : 38.1 %\n",
      "6m 28s (- 21m 42s) (3400 22%) loss : 3.048  accuracy : 42.2 %\n",
      "6m 38s (- 21m 27s) (3500 23%) loss : 3.033  accuracy : 40.8 %\n",
      "6m 50s (- 21m 17s) (3600 24%) loss : 3.044  accuracy : 41.1 %\n",
      "7m 2s (- 21m 7s) (3700 24%) loss : 3.185  accuracy : 38.7 %\n",
      "7m 14s (- 20m 58s) (3800 25%) loss : 3.028  accuracy : 41.9 %\n",
      "7m 26s (- 20m 48s) (3900 26%) loss : 3.137  accuracy : 39.0 %\n",
      "7m 38s (- 20m 38s) (4000 27%) loss : 3.214  accuracy : 39.5 %\n",
      "7m 51s (- 20m 29s) (4100 27%) loss : 3.157  accuracy : 39.7 %\n",
      "8m 2s (- 20m 17s) (4200 28%) loss : 3.165  accuracy : 39.2 %\n",
      "8m 13s (- 20m 5s) (4300 29%) loss : 3.100  accuracy : 39.3 %\n",
      "8m 24s (- 19m 53s) (4400 29%) loss : 3.242  accuracy : 39.5 %\n",
      "8m 36s (- 19m 43s) (4500 30%) loss : 3.078  accuracy : 39.8 %\n",
      "8m 49s (- 19m 33s) (4600 31%) loss : 3.100  accuracy : 40.3 %\n",
      "9m 1s (- 19m 23s) (4700 31%) loss : 3.157  accuracy : 39.9 %\n",
      "9m 13s (- 19m 12s) (4800 32%) loss : 3.092  accuracy : 39.2 %\n",
      "9m 23s (- 19m 0s) (4900 33%) loss : 3.269  accuracy : 36.9 %\n",
      "9m 34s (- 18m 46s) (5000 33%) loss : 3.075  accuracy : 41.4 %\n",
      "9m 45s (- 18m 34s) (5100 34%) loss : 3.071  accuracy : 42.0 %\n",
      "9m 57s (- 18m 24s) (5200 35%) loss : 3.139  accuracy : 40.0 %\n",
      "10m 9s (- 18m 12s) (5300 35%) loss : 2.979  accuracy : 41.6 %\n",
      "10m 20s (- 18m 0s) (5400 36%) loss : 2.927  accuracy : 42.4 %\n",
      "10m 30s (- 17m 47s) (5500 37%) loss : 3.037  accuracy : 40.8 %\n",
      "10m 43s (- 17m 37s) (5600 37%) loss : 3.113  accuracy : 40.9 %\n",
      "10m 53s (- 17m 24s) (5700 38%) loss : 3.061  accuracy : 39.7 %\n",
      "11m 5s (- 17m 13s) (5800 39%) loss : 3.010  accuracy : 41.8 %\n",
      "11m 17s (- 17m 2s) (5900 39%) loss : 2.977  accuracy : 42.2 %\n",
      "11m 28s (- 16m 50s) (6000 40%) loss : 3.100  accuracy : 40.2 %\n",
      "11m 39s (- 16m 38s) (6100 41%) loss : 3.116  accuracy : 40.9 %\n",
      "11m 52s (- 16m 28s) (6200 41%) loss : 3.115  accuracy : 39.9 %\n",
      "12m 4s (- 16m 17s) (6300 42%) loss : 3.041  accuracy : 41.7 %\n",
      "12m 16s (- 16m 6s) (6400 43%) loss : 3.081  accuracy : 39.9 %\n",
      "12m 27s (- 15m 54s) (6500 43%) loss : 3.104  accuracy : 39.4 %\n",
      "12m 39s (- 15m 44s) (6600 44%) loss : 3.158  accuracy : 40.6 %\n",
      "12m 52s (- 15m 33s) (6700 45%) loss : 3.127  accuracy : 39.7 %\n",
      "13m 2s (- 15m 21s) (6800 45%) loss : 3.035  accuracy : 40.6 %\n",
      "13m 14s (- 15m 10s) (6900 46%) loss : 3.112  accuracy : 39.4 %\n",
      "13m 26s (- 14m 58s) (7000 47%) loss : 3.056  accuracy : 40.9 %\n",
      "13m 38s (- 14m 48s) (7100 47%) loss : 3.024  accuracy : 41.9 %\n",
      "13m 50s (- 14m 36s) (7200 48%) loss : 3.227  accuracy : 36.3 %\n",
      "14m 1s (- 14m 24s) (7300 49%) loss : 3.112  accuracy : 40.8 %\n",
      "14m 14s (- 14m 14s) (7400 49%) loss : 2.994  accuracy : 42.7 %\n",
      "14m 25s (- 14m 3s) (7500 50%) loss : 3.048  accuracy : 41.5 %\n",
      "14m 37s (- 13m 51s) (7600 51%) loss : 3.070  accuracy : 40.0 %\n",
      "14m 49s (- 13m 40s) (7700 52%) loss : 3.171  accuracy : 39.4 %\n",
      "15m 0s (- 13m 28s) (7800 52%) loss : 3.082  accuracy : 40.1 %\n",
      "15m 13s (- 13m 18s) (7900 53%) loss : 3.129  accuracy : 41.0 %\n",
      "15m 26s (- 13m 7s) (8000 54%) loss : 3.193  accuracy : 38.3 %\n",
      "15m 39s (- 12m 57s) (8100 54%) loss : 3.201  accuracy : 38.6 %\n",
      "15m 51s (- 12m 46s) (8200 55%) loss : 3.032  accuracy : 40.8 %\n",
      "16m 2s (- 12m 34s) (8300 56%) loss : 2.922  accuracy : 42.9 %\n",
      "16m 15s (- 12m 23s) (8400 56%) loss : 3.102  accuracy : 40.7 %\n",
      "16m 27s (- 12m 12s) (8500 57%) loss : 2.993  accuracy : 41.3 %\n",
      "16m 40s (- 12m 2s) (8600 58%) loss : 3.099  accuracy : 40.3 %\n",
      "16m 53s (- 11m 50s) (8700 58%) loss : 2.985  accuracy : 41.9 %\n",
      "17m 6s (- 11m 40s) (8800 59%) loss : 3.158  accuracy : 39.1 %\n",
      "17m 19s (- 11m 29s) (8900 60%) loss : 3.066  accuracy : 39.8 %\n",
      "17m 32s (- 11m 18s) (9000 60%) loss : 3.054  accuracy : 41.9 %\n",
      "17m 43s (- 11m 6s) (9100 61%) loss : 2.986  accuracy : 42.4 %\n",
      "17m 56s (- 10m 55s) (9200 62%) loss : 3.077  accuracy : 39.8 %\n",
      "18m 7s (- 10m 44s) (9300 62%) loss : 3.126  accuracy : 38.9 %\n",
      "18m 20s (- 10m 32s) (9400 63%) loss : 3.026  accuracy : 41.4 %\n",
      "18m 31s (- 10m 20s) (9500 64%) loss : 2.933  accuracy : 43.0 %\n",
      "18m 43s (- 10m 9s) (9600 64%) loss : 2.968  accuracy : 42.7 %\n",
      "18m 55s (- 9m 57s) (9700 65%) loss : 3.092  accuracy : 40.5 %\n",
      "19m 7s (- 9m 46s) (9800 66%) loss : 3.132  accuracy : 39.0 %\n",
      "19m 19s (- 9m 34s) (9900 66%) loss : 2.948  accuracy : 43.5 %\n",
      "19m 31s (- 9m 22s) (10000 67%) loss : 3.027  accuracy : 41.6 %\n",
      "19m 44s (- 9m 11s) (10100 68%) loss : 3.064  accuracy : 39.5 %\n",
      "19m 55s (- 8m 59s) (10200 68%) loss : 3.071  accuracy : 40.6 %\n",
      "20m 7s (- 8m 48s) (10300 69%) loss : 3.075  accuracy : 40.8 %\n",
      "20m 18s (- 8m 36s) (10400 70%) loss : 3.123  accuracy : 38.5 %\n",
      "20m 29s (- 8m 24s) (10500 70%) loss : 3.157  accuracy : 39.6 %\n",
      "20m 40s (- 8m 11s) (10600 71%) loss : 2.886  accuracy : 42.5 %\n",
      "20m 51s (- 8m 0s) (10700 72%) loss : 3.086  accuracy : 40.7 %\n",
      "21m 1s (- 7m 47s) (10800 72%) loss : 3.116  accuracy : 39.6 %\n",
      "21m 13s (- 7m 36s) (10900 73%) loss : 2.938  accuracy : 41.8 %\n",
      "21m 25s (- 7m 24s) (11000 74%) loss : 3.013  accuracy : 43.4 %\n",
      "21m 36s (- 7m 12s) (11100 74%) loss : 3.003  accuracy : 41.5 %\n",
      "21m 48s (- 7m 1s) (11200 75%) loss : 3.038  accuracy : 40.5 %\n",
      "21m 58s (- 6m 49s) (11300 76%) loss : 3.060  accuracy : 40.2 %\n",
      "22m 9s (- 6m 37s) (11400 77%) loss : 3.053  accuracy : 39.5 %\n",
      "22m 21s (- 6m 25s) (11500 77%) loss : 3.022  accuracy : 41.4 %\n",
      "22m 33s (- 6m 13s) (11600 78%) loss : 2.954  accuracy : 42.6 %\n",
      "22m 43s (- 6m 1s) (11700 79%) loss : 3.041  accuracy : 40.2 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 54s (- 5m 49s) (11800 79%) loss : 3.018  accuracy : 41.1 %\n",
      "23m 3s (- 5m 37s) (11900 80%) loss : 2.974  accuracy : 41.3 %\n",
      "23m 15s (- 5m 26s) (12000 81%) loss : 3.066  accuracy : 40.2 %\n",
      "23m 26s (- 5m 14s) (12100 81%) loss : 3.020  accuracy : 41.4 %\n",
      "23m 38s (- 5m 2s) (12200 82%) loss : 2.982  accuracy : 42.4 %\n",
      "23m 51s (- 4m 51s) (12300 83%) loss : 2.962  accuracy : 41.8 %\n",
      "24m 3s (- 4m 40s) (12400 83%) loss : 3.025  accuracy : 41.3 %\n",
      "24m 16s (- 4m 28s) (12500 84%) loss : 3.110  accuracy : 38.7 %\n",
      "24m 26s (- 4m 16s) (12600 85%) loss : 3.227  accuracy : 36.7 %\n",
      "24m 39s (- 4m 5s) (12700 85%) loss : 3.025  accuracy : 41.2 %\n",
      "24m 49s (- 3m 53s) (12800 86%) loss : 3.071  accuracy : 41.1 %\n",
      "25m 2s (- 3m 41s) (12900 87%) loss : 3.100  accuracy : 39.8 %\n",
      "25m 13s (- 3m 30s) (13000 87%) loss : 2.919  accuracy : 42.1 %\n",
      "25m 24s (- 3m 18s) (13100 88%) loss : 3.053  accuracy : 39.8 %\n",
      "25m 36s (- 3m 6s) (13200 89%) loss : 3.068  accuracy : 39.4 %\n",
      "25m 47s (- 2m 55s) (13300 89%) loss : 2.972  accuracy : 42.1 %\n",
      "25m 58s (- 2m 43s) (13400 90%) loss : 2.987  accuracy : 42.5 %\n",
      "26m 10s (- 2m 31s) (13500 91%) loss : 3.144  accuracy : 39.6 %\n",
      "26m 20s (- 2m 20s) (13600 91%) loss : 3.029  accuracy : 40.3 %\n",
      "26m 31s (- 2m 8s) (13700 92%) loss : 3.045  accuracy : 40.8 %\n",
      "26m 43s (- 1m 56s) (13800 93%) loss : 3.033  accuracy : 42.1 %\n",
      "26m 54s (- 1m 45s) (13900 93%) loss : 2.978  accuracy : 42.3 %\n",
      "27m 5s (- 1m 33s) (14000 94%) loss : 3.098  accuracy : 40.6 %\n",
      "27m 16s (- 1m 21s) (14100 95%) loss : 3.099  accuracy : 40.6 %\n",
      "27m 28s (- 1m 10s) (14200 95%) loss : 3.013  accuracy : 40.4 %\n",
      "27m 39s (- 0m 58s) (14300 96%) loss : 3.039  accuracy : 41.5 %\n",
      "27m 52s (- 0m 47s) (14400 97%) loss : 2.949  accuracy : 42.1 %\n",
      "28m 4s (- 0m 35s) (14500 97%) loss : 2.991  accuracy : 41.3 %\n",
      "28m 14s (- 0m 23s) (14600 98%) loss : 3.050  accuracy : 40.9 %\n",
      "28m 25s (- 0m 12s) (14700 99%) loss : 3.107  accuracy : 40.8 %\n",
      "28m 36s (- 0m 0s) (14800 99%) loss : 2.988  accuracy : 42.1 %\n",
      "epoch 1\n",
      "0m 11s (- 29m 9s) (100 0%) loss : 2.935  accuracy : 42.0 %\n",
      "0m 22s (- 27m 27s) (200 1%) loss : 3.087  accuracy : 38.9 %\n",
      "0m 34s (- 27m 59s) (300 2%) loss : 2.978  accuracy : 42.2 %\n",
      "0m 45s (- 27m 27s) (400 2%) loss : 3.019  accuracy : 39.6 %\n",
      "0m 57s (- 27m 31s) (500 3%) loss : 2.974  accuracy : 42.7 %\n",
      "1m 9s (- 27m 32s) (600 4%) loss : 3.051  accuracy : 40.3 %\n",
      "1m 21s (- 27m 21s) (700 4%) loss : 2.963  accuracy : 41.0 %\n",
      "1m 33s (- 27m 14s) (800 5%) loss : 2.935  accuracy : 42.0 %\n",
      "1m 45s (- 27m 3s) (900 6%) loss : 2.950  accuracy : 43.0 %\n",
      "1m 57s (- 26m 58s) (1000 6%) loss : 2.966  accuracy : 41.9 %\n",
      "2m 10s (- 27m 0s) (1100 7%) loss : 2.882  accuracy : 42.7 %\n",
      "2m 20s (- 26m 33s) (1200 8%) loss : 2.987  accuracy : 42.0 %\n",
      "2m 32s (- 26m 28s) (1300 8%) loss : 3.001  accuracy : 41.5 %\n",
      "2m 44s (- 26m 16s) (1400 9%) loss : 2.930  accuracy : 40.8 %\n",
      "2m 55s (- 25m 57s) (1500 10%) loss : 2.965  accuracy : 41.6 %\n",
      "3m 7s (- 25m 44s) (1600 10%) loss : 2.883  accuracy : 43.0 %\n",
      "3m 18s (- 25m 30s) (1700 11%) loss : 3.017  accuracy : 41.4 %\n",
      "3m 30s (- 25m 21s) (1800 12%) loss : 3.158  accuracy : 38.6 %\n",
      "3m 43s (- 25m 18s) (1900 12%) loss : 3.020  accuracy : 41.1 %\n",
      "3m 55s (- 25m 6s) (2000 13%) loss : 2.981  accuracy : 41.6 %\n",
      "4m 6s (- 24m 50s) (2100 14%) loss : 3.061  accuracy : 39.3 %\n",
      "4m 16s (- 24m 29s) (2200 14%) loss : 2.945  accuracy : 42.7 %\n",
      "4m 27s (- 24m 12s) (2300 15%) loss : 2.987  accuracy : 41.3 %\n",
      "4m 38s (- 23m 58s) (2400 16%) loss : 2.801  accuracy : 44.9 %\n",
      "4m 49s (- 23m 44s) (2500 16%) loss : 2.905  accuracy : 42.2 %\n",
      "5m 1s (- 23m 34s) (2600 17%) loss : 2.919  accuracy : 43.2 %\n",
      "5m 12s (- 23m 20s) (2700 18%) loss : 3.101  accuracy : 39.0 %\n",
      "5m 25s (- 23m 15s) (2800 18%) loss : 3.007  accuracy : 41.8 %\n",
      "5m 37s (- 23m 7s) (2900 19%) loss : 2.887  accuracy : 42.9 %\n",
      "5m 51s (- 23m 2s) (3000 20%) loss : 2.830  accuracy : 44.1 %\n",
      "6m 3s (- 22m 51s) (3100 20%) loss : 2.844  accuracy : 43.6 %\n",
      "6m 15s (- 22m 40s) (3200 21%) loss : 3.073  accuracy : 39.6 %\n",
      "6m 26s (- 22m 27s) (3300 22%) loss : 2.965  accuracy : 41.5 %\n",
      "6m 36s (- 22m 10s) (3400 22%) loss : 3.009  accuracy : 42.2 %\n",
      "6m 48s (- 22m 0s) (3500 23%) loss : 2.898  accuracy : 42.5 %\n",
      "6m 59s (- 21m 47s) (3600 24%) loss : 2.932  accuracy : 41.0 %\n",
      "7m 10s (- 21m 33s) (3700 24%) loss : 2.938  accuracy : 42.7 %\n",
      "7m 22s (- 21m 20s) (3800 25%) loss : 2.869  accuracy : 43.2 %\n",
      "7m 32s (- 21m 6s) (3900 26%) loss : 2.987  accuracy : 42.0 %\n",
      "7m 44s (- 20m 55s) (4000 27%) loss : 2.811  accuracy : 44.9 %\n",
      "7m 58s (- 20m 48s) (4100 27%) loss : 3.013  accuracy : 41.1 %\n",
      "8m 10s (- 20m 38s) (4200 28%) loss : 3.035  accuracy : 40.1 %\n",
      "8m 23s (- 20m 29s) (4300 29%) loss : 2.987  accuracy : 42.5 %\n",
      "8m 33s (- 20m 14s) (4400 29%) loss : 3.015  accuracy : 39.8 %\n",
      "8m 44s (- 20m 1s) (4500 30%) loss : 2.903  accuracy : 42.6 %\n",
      "8m 55s (- 19m 48s) (4600 31%) loss : 2.993  accuracy : 41.0 %\n",
      "9m 7s (- 19m 36s) (4700 31%) loss : 2.970  accuracy : 41.4 %\n",
      "9m 18s (- 19m 25s) (4800 32%) loss : 2.980  accuracy : 41.8 %\n",
      "9m 29s (- 19m 11s) (4900 33%) loss : 3.060  accuracy : 38.9 %\n",
      "9m 39s (- 18m 57s) (5000 33%) loss : 2.951  accuracy : 41.7 %\n",
      "9m 51s (- 18m 45s) (5100 34%) loss : 2.891  accuracy : 43.4 %\n",
      "10m 1s (- 18m 31s) (5200 35%) loss : 2.834  accuracy : 43.0 %\n",
      "10m 12s (- 18m 19s) (5300 35%) loss : 2.985  accuracy : 41.2 %\n",
      "10m 23s (- 18m 5s) (5400 36%) loss : 2.965  accuracy : 41.8 %\n",
      "10m 36s (- 17m 56s) (5500 37%) loss : 2.904  accuracy : 43.0 %\n",
      "10m 48s (- 17m 46s) (5600 37%) loss : 2.933  accuracy : 43.7 %\n",
      "10m 59s (- 17m 33s) (5700 38%) loss : 2.957  accuracy : 42.1 %\n",
      "11m 10s (- 17m 20s) (5800 39%) loss : 2.835  accuracy : 45.4 %\n",
      "11m 21s (- 17m 9s) (5900 39%) loss : 2.898  accuracy : 43.7 %\n",
      "11m 35s (- 17m 0s) (6000 40%) loss : 2.973  accuracy : 41.8 %\n",
      "11m 46s (- 16m 47s) (6100 41%) loss : 3.016  accuracy : 42.2 %\n",
      "11m 57s (- 16m 36s) (6200 41%) loss : 3.033  accuracy : 40.8 %\n",
      "12m 8s (- 16m 23s) (6300 42%) loss : 2.941  accuracy : 42.5 %\n",
      "12m 20s (- 16m 12s) (6400 43%) loss : 2.739  accuracy : 47.1 %\n",
      "12m 31s (- 16m 0s) (6500 43%) loss : 2.879  accuracy : 43.0 %\n",
      "12m 42s (- 15m 47s) (6600 44%) loss : 3.020  accuracy : 40.8 %\n",
      "12m 52s (- 15m 34s) (6700 45%) loss : 2.882  accuracy : 43.4 %\n",
      "13m 5s (- 15m 24s) (6800 45%) loss : 2.984  accuracy : 41.6 %\n",
      "13m 16s (- 15m 12s) (6900 46%) loss : 3.006  accuracy : 41.3 %\n",
      "13m 28s (- 15m 0s) (7000 47%) loss : 2.889  accuracy : 42.1 %\n",
      "13m 39s (- 14m 48s) (7100 47%) loss : 2.930  accuracy : 43.1 %\n",
      "13m 51s (- 14m 38s) (7200 48%) loss : 3.056  accuracy : 40.1 %\n",
      "14m 1s (- 14m 25s) (7300 49%) loss : 2.947  accuracy : 42.4 %\n",
      "14m 12s (- 14m 13s) (7400 49%) loss : 2.905  accuracy : 40.9 %\n",
      "14m 23s (- 14m 0s) (7500 50%) loss : 2.842  accuracy : 44.3 %\n",
      "14m 34s (- 13m 48s) (7600 51%) loss : 2.971  accuracy : 42.7 %\n",
      "14m 47s (- 13m 39s) (7700 52%) loss : 2.855  accuracy : 43.9 %\n",
      "15m 1s (- 13m 29s) (7800 52%) loss : 2.921  accuracy : 43.3 %\n",
      "15m 12s (- 13m 17s) (7900 53%) loss : 2.876  accuracy : 42.9 %\n",
      "15m 23s (- 13m 5s) (8000 54%) loss : 3.013  accuracy : 40.8 %\n",
      "15m 34s (- 12m 53s) (8100 54%) loss : 2.787  accuracy : 45.3 %\n",
      "15m 46s (- 12m 42s) (8200 55%) loss : 3.044  accuracy : 42.0 %\n",
      "15m 57s (- 12m 30s) (8300 56%) loss : 2.899  accuracy : 43.5 %\n",
      "16m 10s (- 12m 19s) (8400 56%) loss : 2.989  accuracy : 41.9 %\n",
      "16m 21s (- 12m 7s) (8500 57%) loss : 2.948  accuracy : 42.0 %\n",
      "16m 33s (- 11m 56s) (8600 58%) loss : 2.945  accuracy : 41.6 %\n",
      "16m 44s (- 11m 44s) (8700 58%) loss : 2.819  accuracy : 44.1 %\n",
      "16m 55s (- 11m 33s) (8800 59%) loss : 3.019  accuracy : 41.0 %\n",
      "17m 7s (- 11m 21s) (8900 60%) loss : 2.751  accuracy : 45.6 %\n",
      "17m 18s (- 11m 9s) (9000 60%) loss : 2.875  accuracy : 43.7 %\n",
      "17m 28s (- 10m 57s) (9100 61%) loss : 2.884  accuracy : 44.4 %\n",
      "17m 39s (- 10m 45s) (9200 62%) loss : 2.997  accuracy : 42.9 %\n",
      "17m 50s (- 10m 33s) (9300 62%) loss : 3.017  accuracy : 42.2 %\n",
      "18m 1s (- 10m 22s) (9400 63%) loss : 2.818  accuracy : 44.9 %\n",
      "18m 12s (- 10m 10s) (9500 64%) loss : 2.877  accuracy : 43.8 %\n",
      "18m 23s (- 9m 58s) (9600 64%) loss : 2.840  accuracy : 44.0 %\n",
      "18m 33s (- 9m 46s) (9700 65%) loss : 3.009  accuracy : 42.3 %\n",
      "18m 45s (- 9m 34s) (9800 66%) loss : 2.830  accuracy : 43.5 %\n",
      "18m 55s (- 9m 22s) (9900 66%) loss : 3.029  accuracy : 40.3 %\n",
      "19m 6s (- 9m 10s) (10000 67%) loss : 2.941  accuracy : 41.6 %\n",
      "19m 18s (- 8m 59s) (10100 68%) loss : 2.965  accuracy : 42.4 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19m 28s (- 8m 47s) (10200 68%) loss : 2.963  accuracy : 42.4 %\n",
      "19m 39s (- 8m 35s) (10300 69%) loss : 2.875  accuracy : 42.4 %\n",
      "19m 51s (- 8m 24s) (10400 70%) loss : 2.830  accuracy : 43.7 %\n",
      "20m 3s (- 8m 13s) (10500 70%) loss : 3.074  accuracy : 39.9 %\n",
      "20m 15s (- 8m 2s) (10600 71%) loss : 2.888  accuracy : 43.4 %\n",
      "20m 26s (- 7m 50s) (10700 72%) loss : 2.961  accuracy : 42.0 %\n",
      "20m 38s (- 7m 39s) (10800 72%) loss : 3.002  accuracy : 41.5 %\n",
      "20m 51s (- 7m 28s) (10900 73%) loss : 2.940  accuracy : 42.0 %\n",
      "21m 3s (- 7m 17s) (11000 74%) loss : 2.965  accuracy : 42.0 %\n",
      "21m 15s (- 7m 5s) (11100 74%) loss : 2.980  accuracy : 41.9 %\n",
      "21m 27s (- 6m 54s) (11200 75%) loss : 2.913  accuracy : 42.8 %\n",
      "21m 38s (- 6m 42s) (11300 76%) loss : 2.983  accuracy : 41.4 %\n",
      "21m 50s (- 6m 31s) (11400 77%) loss : 2.940  accuracy : 41.9 %\n",
      "22m 1s (- 6m 19s) (11500 77%) loss : 2.957  accuracy : 42.4 %\n",
      "22m 13s (- 6m 8s) (11600 78%) loss : 2.975  accuracy : 41.8 %\n",
      "22m 25s (- 5m 57s) (11700 79%) loss : 3.070  accuracy : 40.0 %\n",
      "22m 38s (- 5m 45s) (11800 79%) loss : 2.857  accuracy : 42.4 %\n",
      "22m 51s (- 5m 34s) (11900 80%) loss : 2.948  accuracy : 41.9 %\n",
      "23m 4s (- 5m 23s) (12000 81%) loss : 3.048  accuracy : 39.7 %\n",
      "23m 16s (- 5m 12s) (12100 81%) loss : 2.925  accuracy : 42.5 %\n",
      "23m 28s (- 5m 0s) (12200 82%) loss : 2.870  accuracy : 43.7 %\n",
      "23m 38s (- 4m 48s) (12300 83%) loss : 2.839  accuracy : 44.0 %\n",
      "23m 50s (- 4m 37s) (12400 83%) loss : 2.879  accuracy : 43.9 %\n",
      "24m 1s (- 4m 25s) (12500 84%) loss : 2.989  accuracy : 41.3 %\n",
      "24m 13s (- 4m 14s) (12600 85%) loss : 2.807  accuracy : 46.6 %\n",
      "24m 24s (- 4m 2s) (12700 85%) loss : 3.018  accuracy : 41.2 %\n",
      "24m 35s (- 3m 51s) (12800 86%) loss : 2.868  accuracy : 43.1 %\n",
      "24m 46s (- 3m 39s) (12900 87%) loss : 3.007  accuracy : 42.5 %\n",
      "24m 58s (- 3m 28s) (13000 87%) loss : 2.947  accuracy : 41.5 %\n",
      "25m 9s (- 3m 16s) (13100 88%) loss : 2.775  accuracy : 45.4 %\n",
      "25m 22s (- 3m 5s) (13200 89%) loss : 2.968  accuracy : 42.3 %\n",
      "25m 34s (- 2m 53s) (13300 89%) loss : 2.983  accuracy : 41.9 %\n",
      "25m 45s (- 2m 42s) (13400 90%) loss : 2.889  accuracy : 42.8 %\n",
      "25m 56s (- 2m 30s) (13500 91%) loss : 2.922  accuracy : 42.2 %\n",
      "26m 7s (- 2m 18s) (13600 91%) loss : 2.995  accuracy : 40.3 %\n",
      "26m 20s (- 2m 7s) (13700 92%) loss : 3.000  accuracy : 39.7 %\n",
      "26m 33s (- 1m 56s) (13800 93%) loss : 3.021  accuracy : 41.2 %\n",
      "26m 46s (- 1m 44s) (13900 93%) loss : 2.916  accuracy : 42.3 %\n",
      "26m 57s (- 1m 32s) (14000 94%) loss : 2.922  accuracy : 42.2 %\n",
      "27m 9s (- 1m 21s) (14100 95%) loss : 2.870  accuracy : 43.5 %\n",
      "27m 25s (- 1m 10s) (14200 95%) loss : 2.885  accuracy : 43.0 %\n",
      "27m 40s (- 0m 58s) (14300 96%) loss : 2.910  accuracy : 42.7 %\n",
      "27m 53s (- 0m 47s) (14400 97%) loss : 2.836  accuracy : 43.9 %\n",
      "28m 6s (- 0m 35s) (14500 97%) loss : 2.855  accuracy : 43.4 %\n",
      "28m 21s (- 0m 23s) (14600 98%) loss : 3.056  accuracy : 39.9 %\n",
      "28m 36s (- 0m 12s) (14700 99%) loss : 2.860  accuracy : 44.0 %\n",
      "28m 50s (- 0m 0s) (14800 99%) loss : 2.934  accuracy : 42.3 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches, epochs = 1, lr = 0.01,   print_every = 100)\n",
    "denoiser.fit(batches, epochs = 1, lr = 0.0025, print_every = 100)\n",
    "denoiser.fit(batches, epochs = 1, lr = 0.0005, print_every = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". principle . percent adsorption of the hepatitis b antigen onto the alum adjuvant is determined using a \" sandwich \" type enzyme linked immunosorbent assay ( elisa ) by measuring the antigen concentration in the supernatant of a sample compared to the antigen concentration in the total test sample . equipment . standard laboratory equipment in addition to . centrifuge . microplate spectrofluorimeter automated liquid handling workstation plate washer . reagents . standard laboratory reagents in addition to . mouse anti hepatitis b b4 antibody ( elisa ) mouse anti hepatitis b h35c16 antibody ( elisa ) . rat anti mouse kappa alkaline phosphatase conjugate antibody ( elisa ) hepatitis b reference standard ( elisa ) . blocking buffer ( tbs1 / % bsa / 0.05 % tween ) wash buffer ( tbs / 0.05 % tween ) . citrate buffer ( mm sodium citrate mm boric acid % bsa ph 9.0 ) mup ( methylumbelliferyl phosphate ) cocktail diluent . 1 tbs is mm tris and 0.9 % nacl . 4 mup concentrate 1m . preparation of standard . the reference standard monovalent hepatitis b vaccine recombinant is diluted to µg / ml and then pre treated with a solution of mm sodium citrate mm boric acid % bsa ph 9.0 ( citrate buffer ) to dissolve the alum . 125 µl of reference standard is combined with µl of the citrate buffer ( : dilution ) and heated at °c to °c for to min . after citration reference standard is diluted further with citrate buffer before loading onto an assay plate for a point serial dilution . preparation of samples . the hepatitis b protein concentration in pr5i test samples is µg / ml . for percent adsorption testing the total test samples are diluted volume to volume with mm tris 0.9 % nacl 0.05 % tween % bovine serum albumin ( blocking buffer ) to the hepatitis b protein concentration of µg / ml . then the total test samples are pre treated with citrate buffer the same way as the standard ( : dilution ) to dissolve the alum . 125 µl of pre diluted test sample are combined with µl of the citrate buffer ( : dilution ) and heated at °c to °c for to min . after pre \u001b[48;2;255;229;217m the plates are incubated at room temperature . \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(denoiser.state_dict(), path_to_NLP + '\\\\saves\\\\DL4NLP_I4a_sentence_denoiser.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
