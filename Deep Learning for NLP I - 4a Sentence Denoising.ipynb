{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 4a <br><br><br>\n",
    "  Sentence Denoising\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=red>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "We consider as Sequence labelling task a **Sentence Denoising** problem, which consists in transforming a noisy sequence of words into a correctly formed sentence.<br> Training follows a denoising objective known as _Cloze task_ , which is used :\n",
    "\n",
    "- For the BERT model in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\DL4NLP lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importWords(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importAllWords(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importWords(file_name))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def importSentences(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        sentences = []\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                sentences.append(cleanSentence(current_tile))\n",
    "                title = current_tile\n",
    "            sentences.append(cleanSentence(str(pair[1]).split(' | ')[-1]))\n",
    "        return sentences\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    sentences = cleanDatabase(db)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def importAllSentences(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus += importSentences(file_name)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importAllWords(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31574"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = importAllSentences(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'test',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'assessing',\n",
       " 'the',\n",
       " 'dissolution',\n",
       " 'time',\n",
       " 'of',\n",
       " 'the',\n",
       " 'freeze',\n",
       " 'dried',\n",
       " 'yellow',\n",
       " 'fever',\n",
       " 'vaccine',\n",
       " 'after',\n",
       " 'adding',\n",
       " 'the',\n",
       " 'suitable',\n",
       " 'diluent',\n",
       " 'directly',\n",
       " 'into',\n",
       " 'the',\n",
       " 'original',\n",
       " 'container']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = FastText(size = 75, \n",
    "                    window = 5, \n",
    "                    min_count = 3, \n",
    "                    negative = 20,\n",
    "                    sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4662"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.train(sentences = sentences, \n",
    "               epochs = 50,\n",
    "               total_examples = word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#word2vec.save(get_tmpfile(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_word2vec.model'))\n",
    "\n",
    "# load trained model\n",
    "#word2vec = FastText.load(get_tmpfile(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_word2vec.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentence denoising Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDenoiser(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SentenceDenoiser, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, lang_size)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length, lang_size)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        new_ws = [self.word2vec.lang.index2word[ind] for ind in inds]\n",
    "        print(' '.join([addColor(w1, w2, color) for w1, w2 in zip(words, new_ws)]))\n",
    "        return\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0.15,\n",
    "                                max_sentence_length = 50,\n",
    "                                tol = 10,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index\n",
    "        \n",
    "        def splitLongs(words, threshold = 50, tol = 10):\n",
    "            news = []\n",
    "            for i in range(0, len(words), threshold) :\n",
    "                if len(words)-i-threshold > tol : \n",
    "                    news.append(words[i : i + threshold])\n",
    "                else : \n",
    "                    news.append(words[i:])\n",
    "                    break\n",
    "            return news\n",
    "        \n",
    "        packed_data = []\n",
    "        random.seed(seed)\n",
    "        # prepare sentences\n",
    "        #sentences = [self.tokenizer(s) for s in sentences]\n",
    "        sentences = [[self.word2vec.lang.getIndex(w) for w in s] for s in sentences]\n",
    "        sentences = [[w for w in words if w is not None] for words in sentences]\n",
    "        sentences = [s for S in sentences for s in splitLongs(S, max_sentence_length, tol) if len([w for w in s if w != self.word2vec.lang.getIndex('UNK')]) > 1]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        # collect packs\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            # prepare mask\n",
    "            mask_xl = [[i for i, w in enumerate(p) if w != self.word2vec.lang.getIndex('UNK')] for p in pack]\n",
    "            mask_xs = [random.sample(m, k = int(mask_ratio*len(m) +1)) for m in mask_xl]\n",
    "            # prepare input and target packs\n",
    "            pack0    = [[ maskInput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask_xs)]\n",
    "            pack1_xs = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask_xs)]\n",
    "            pack1_xl = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask_xl)]\n",
    "            lengths  = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            # padd\n",
    "            pack0    = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index)) \n",
    "            pack1_xs = list(itertools.zip_longest(*pack1_xs, fillvalue = self.ignore_index))\n",
    "            pack1_xl = list(itertools.zip_longest(*pack1_xl, fillvalue = self.ignore_index))\n",
    "            # turn into torch variables\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length)\n",
    "            pack1_xs = Variable(torch.LongTensor(pack1_xs).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            pack1_xl = Variable(torch.LongTensor(pack1_xl).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            # store pack\n",
    "            packed_data.append([[pack0, lengths], [pack1_xs, pack1_xl]])\n",
    "        return packed_data\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, \n",
    "            iters = None, epochs = None, lr = 0.025, unmasked_ratio = 0,\n",
    "            random_state = 42, print_every = 10, compute_accuracy = 'xs'):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index)\n",
    "            success = sum([self.ignore_index != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, unmasked_ratio, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs  = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets_xs = batch[1][0].to(self.device)               # dim = (batch_size, input_length)\n",
    "            targets_xl = batch[1][1].to(self.device)               # dim = (batch_size, input_length)\n",
    "            loss       = (1-unmasked_ratio)*self.criterion(log_probs, targets_xs) \\\n",
    "                       + unmasked_ratio    *self.criterion(log_probs, targets_xl)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            if compute_accuracy == 'xs': \n",
    "                accuracy = computeAccuracy(log_probs, targets_xs)\n",
    "                error = float(loss.item() / np.sum(targets_xs.data.cpu().numpy() != self.ignore_index))\n",
    "            elif compute_accuracy == 'xl': \n",
    "                accuracy = computeAccuracy(log_probs, targets_xl)\n",
    "                error = float(loss.item() / np.sum(targets_xl.data.cpu().numpy() != self.ignore_index))\n",
    "            else : \n",
    "                accuracy = 0\n",
    "                error = float(loss.item() / np.sum(targets_xs.data.cpu().numpy() != self.ignore_index))\n",
    "            return error, accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, unmasked_ratio, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer,unmasked_ratio, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627164"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser = SentenceDenoiser(device = torch.device('cpu'), # device\n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = Word2VecConnector(word2vec),\n",
    "                            hidden_dim = 75, \n",
    "                            n_layers = 3, \n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.AdamW)\n",
    "\n",
    "denoiser.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n",
      "9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for seed in [42, 854, 7956, 657, 124] :\n",
    "    batches += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 14s (- 22m 18s) (100 1%) loss : 10.937  accuracy : 6.3 %\n",
      "0m 27s (- 20m 35s) (200 2%) loss : 9.902  accuracy : 6.3 %\n",
      "0m 42s (- 20m 39s) (300 3%) loss : 9.919  accuracy : 7.3 %\n",
      "0m 55s (- 20m 22s) (400 4%) loss : 9.278  accuracy : 10.1 %\n",
      "1m 11s (- 20m 31s) (500 5%) loss : 8.866  accuracy : 11.5 %\n",
      "1m 24s (- 20m 2s) (600 6%) loss : 8.299  accuracy : 13.9 %\n",
      "1m 38s (- 19m 50s) (700 7%) loss : 7.937  accuracy : 15.7 %\n",
      "1m 52s (- 19m 36s) (800 8%) loss : 7.376  accuracy : 18.4 %\n",
      "2m 8s (- 19m 33s) (900 9%) loss : 7.531  accuracy : 17.2 %\n",
      "2m 21s (- 19m 14s) (1000 10%) loss : 6.964  accuracy : 19.8 %\n",
      "2m 37s (- 19m 9s) (1100 12%) loss : 6.866  accuracy : 19.4 %\n",
      "2m 50s (- 18m 50s) (1200 13%) loss : 6.621  accuracy : 20.5 %\n",
      "3m 4s (- 18m 35s) (1300 14%) loss : 6.507  accuracy : 19.5 %\n",
      "3m 19s (- 18m 22s) (1400 15%) loss : 6.226  accuracy : 20.9 %\n",
      "3m 32s (- 18m 4s) (1500 16%) loss : 6.063  accuracy : 22.4 %\n",
      "3m 46s (- 17m 50s) (1600 17%) loss : 5.990  accuracy : 23.8 %\n",
      "4m 1s (- 17m 36s) (1700 18%) loss : 5.913  accuracy : 23.4 %\n",
      "4m 15s (- 17m 22s) (1800 19%) loss : 5.812  accuracy : 22.9 %\n",
      "4m 28s (- 17m 5s) (1900 20%) loss : 5.773  accuracy : 24.6 %\n",
      "4m 42s (- 16m 50s) (2000 21%) loss : 5.630  accuracy : 25.0 %\n",
      "4m 58s (- 16m 43s) (2100 22%) loss : 5.575  accuracy : 25.6 %\n",
      "5m 15s (- 16m 37s) (2200 24%) loss : 5.313  accuracy : 27.5 %\n",
      "5m 31s (- 16m 26s) (2300 25%) loss : 5.326  accuracy : 26.9 %\n",
      "5m 47s (- 16m 16s) (2400 26%) loss : 5.131  accuracy : 27.6 %\n",
      "6m 0s (- 15m 59s) (2500 27%) loss : 5.312  accuracy : 26.9 %\n",
      "6m 14s (- 15m 43s) (2600 28%) loss : 5.242  accuracy : 26.3 %\n",
      "6m 28s (- 15m 29s) (2700 29%) loss : 5.091  accuracy : 28.6 %\n",
      "6m 42s (- 15m 13s) (2800 30%) loss : 5.060  accuracy : 28.4 %\n",
      "6m 55s (- 14m 56s) (2900 31%) loss : 5.112  accuracy : 28.0 %\n",
      "7m 10s (- 14m 42s) (3000 32%) loss : 5.110  accuracy : 27.4 %\n",
      "7m 24s (- 14m 26s) (3100 33%) loss : 4.988  accuracy : 28.8 %\n",
      "7m 38s (- 14m 12s) (3200 34%) loss : 4.946  accuracy : 29.1 %\n",
      "7m 51s (- 13m 56s) (3300 36%) loss : 4.745  accuracy : 29.8 %\n",
      "8m 6s (- 13m 41s) (3400 37%) loss : 4.779  accuracy : 30.2 %\n",
      "8m 20s (- 13m 27s) (3500 38%) loss : 4.740  accuracy : 30.5 %\n",
      "8m 34s (- 13m 13s) (3600 39%) loss : 4.771  accuracy : 29.4 %\n",
      "8m 47s (- 12m 57s) (3700 40%) loss : 4.646  accuracy : 30.6 %\n",
      "9m 2s (- 12m 43s) (3800 41%) loss : 4.693  accuracy : 30.1 %\n",
      "9m 16s (- 12m 29s) (3900 42%) loss : 4.660  accuracy : 30.4 %\n",
      "9m 29s (- 12m 13s) (4000 43%) loss : 4.747  accuracy : 29.5 %\n",
      "9m 41s (- 11m 56s) (4100 44%) loss : 4.595  accuracy : 30.9 %\n",
      "9m 55s (- 11m 41s) (4200 45%) loss : 4.567  accuracy : 31.3 %\n",
      "10m 10s (- 11m 28s) (4300 46%) loss : 4.406  accuracy : 31.1 %\n",
      "10m 24s (- 11m 13s) (4400 48%) loss : 4.317  accuracy : 32.9 %\n",
      "10m 37s (- 10m 58s) (4500 49%) loss : 4.479  accuracy : 32.2 %\n",
      "10m 51s (- 10m 43s) (4600 50%) loss : 4.532  accuracy : 31.1 %\n",
      "11m 5s (- 10m 29s) (4700 51%) loss : 4.307  accuracy : 32.7 %\n",
      "11m 18s (- 10m 14s) (4800 52%) loss : 4.390  accuracy : 32.9 %\n",
      "11m 33s (- 10m 1s) (4900 53%) loss : 4.391  accuracy : 31.4 %\n",
      "11m 47s (- 9m 46s) (5000 54%) loss : 4.335  accuracy : 33.2 %\n",
      "12m 2s (- 9m 33s) (5100 55%) loss : 4.515  accuracy : 30.6 %\n",
      "12m 15s (- 9m 18s) (5200 56%) loss : 4.334  accuracy : 33.8 %\n",
      "12m 30s (- 9m 4s) (5300 57%) loss : 4.309  accuracy : 33.3 %\n",
      "12m 43s (- 8m 50s) (5400 59%) loss : 4.284  accuracy : 33.0 %\n",
      "12m 57s (- 8m 35s) (5500 60%) loss : 4.044  accuracy : 36.0 %\n",
      "13m 10s (- 8m 21s) (5600 61%) loss : 4.180  accuracy : 34.4 %\n",
      "13m 25s (- 8m 7s) (5700 62%) loss : 4.129  accuracy : 35.9 %\n",
      "13m 39s (- 7m 53s) (5800 63%) loss : 4.295  accuracy : 33.1 %\n",
      "13m 52s (- 7m 38s) (5900 64%) loss : 4.234  accuracy : 32.8 %\n",
      "14m 6s (- 7m 24s) (6000 65%) loss : 4.164  accuracy : 34.3 %\n",
      "14m 20s (- 7m 10s) (6100 66%) loss : 4.029  accuracy : 35.6 %\n",
      "14m 33s (- 6m 55s) (6200 67%) loss : 3.973  accuracy : 36.5 %\n",
      "14m 46s (- 6m 41s) (6300 68%) loss : 4.057  accuracy : 34.4 %\n",
      "15m 1s (- 6m 27s) (6400 69%) loss : 4.114  accuracy : 36.2 %\n",
      "15m 15s (- 6m 13s) (6500 71%) loss : 3.958  accuracy : 35.3 %\n",
      "15m 29s (- 5m 59s) (6600 72%) loss : 3.965  accuracy : 35.7 %\n",
      "15m 44s (- 5m 45s) (6700 73%) loss : 3.994  accuracy : 35.0 %\n",
      "15m 57s (- 5m 30s) (6800 74%) loss : 3.932  accuracy : 35.9 %\n",
      "16m 10s (- 5m 16s) (6900 75%) loss : 4.061  accuracy : 35.0 %\n",
      "16m 25s (- 5m 2s) (7000 76%) loss : 3.968  accuracy : 35.6 %\n",
      "16m 38s (- 4m 48s) (7100 77%) loss : 3.974  accuracy : 35.5 %\n",
      "16m 52s (- 4m 34s) (7200 78%) loss : 3.986  accuracy : 35.7 %\n",
      "17m 7s (- 4m 20s) (7300 79%) loss : 4.067  accuracy : 34.4 %\n",
      "17m 23s (- 4m 6s) (7400 80%) loss : 3.986  accuracy : 34.8 %\n",
      "17m 37s (- 3m 52s) (7500 81%) loss : 4.067  accuracy : 34.8 %\n",
      "17m 50s (- 3m 38s) (7600 83%) loss : 3.804  accuracy : 37.4 %\n",
      "18m 3s (- 3m 23s) (7700 84%) loss : 3.792  accuracy : 37.7 %\n",
      "18m 19s (- 3m 10s) (7800 85%) loss : 3.781  accuracy : 37.5 %\n",
      "18m 32s (- 2m 56s) (7900 86%) loss : 3.813  accuracy : 37.5 %\n",
      "18m 46s (- 2m 41s) (8000 87%) loss : 3.884  accuracy : 36.6 %\n",
      "19m 1s (- 2m 28s) (8100 88%) loss : 3.876  accuracy : 35.5 %\n",
      "19m 15s (- 2m 13s) (8200 89%) loss : 3.911  accuracy : 36.5 %\n",
      "19m 28s (- 1m 59s) (8300 90%) loss : 3.797  accuracy : 36.5 %\n",
      "19m 44s (- 1m 45s) (8400 91%) loss : 3.862  accuracy : 36.9 %\n",
      "19m 58s (- 1m 31s) (8500 92%) loss : 3.757  accuracy : 39.2 %\n",
      "20m 12s (- 1m 17s) (8600 93%) loss : 3.910  accuracy : 36.0 %\n",
      "20m 25s (- 1m 3s) (8700 95%) loss : 3.734  accuracy : 38.4 %\n",
      "20m 40s (- 0m 49s) (8800 96%) loss : 3.822  accuracy : 37.5 %\n",
      "20m 54s (- 0m 35s) (8900 97%) loss : 3.758  accuracy : 37.1 %\n",
      "21m 9s (- 0m 21s) (9000 98%) loss : 3.857  accuracy : 36.4 %\n",
      "21m 23s (- 0m 7s) (9100 99%) loss : 3.734  accuracy : 37.9 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches, epochs = 1, lr = 0.001, unmasked_ratio = 0.15, compute_accuracy = 'xs', print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n",
      "9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches2 = []\n",
    "for seed in [777, 5821, 12, 693, 45824] :\n",
    "    batches2 += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches2))\n",
    "len(batches2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 14s (- 21m 20s) (100 1%) loss : 3.676  accuracy : 38.4 %\n",
      "0m 26s (- 19m 44s) (200 2%) loss : 3.784  accuracy : 37.8 %\n",
      "0m 41s (- 20m 21s) (300 3%) loss : 3.828  accuracy : 37.3 %\n",
      "0m 55s (- 20m 12s) (400 4%) loss : 3.848  accuracy : 38.0 %\n",
      "1m 7s (- 19m 32s) (500 5%) loss : 3.914  accuracy : 36.7 %\n",
      "1m 21s (- 19m 24s) (600 6%) loss : 3.530  accuracy : 40.8 %\n",
      "1m 36s (- 19m 20s) (700 7%) loss : 3.712  accuracy : 39.3 %\n",
      "1m 50s (- 19m 16s) (800 8%) loss : 3.839  accuracy : 37.2 %\n",
      "2m 6s (- 19m 19s) (900 9%) loss : 3.756  accuracy : 38.1 %\n",
      "2m 20s (- 19m 1s) (1000 10%) loss : 3.843  accuracy : 37.3 %\n",
      "2m 33s (- 18m 45s) (1100 12%) loss : 3.650  accuracy : 39.2 %\n",
      "2m 48s (- 18m 35s) (1200 13%) loss : 3.868  accuracy : 38.0 %\n",
      "3m 0s (- 18m 8s) (1300 14%) loss : 3.574  accuracy : 40.6 %\n",
      "3m 13s (- 17m 51s) (1400 15%) loss : 3.746  accuracy : 37.5 %\n",
      "3m 26s (- 17m 34s) (1500 16%) loss : 3.936  accuracy : 35.4 %\n",
      "3m 41s (- 17m 27s) (1600 17%) loss : 3.684  accuracy : 38.2 %\n",
      "3m 56s (- 17m 15s) (1700 18%) loss : 3.468  accuracy : 42.0 %\n",
      "4m 10s (- 17m 3s) (1800 19%) loss : 3.524  accuracy : 40.8 %\n",
      "4m 26s (- 16m 57s) (1900 20%) loss : 3.758  accuracy : 37.3 %\n",
      "4m 42s (- 16m 51s) (2000 21%) loss : 3.759  accuracy : 37.3 %\n",
      "4m 58s (- 16m 43s) (2100 22%) loss : 3.506  accuracy : 40.6 %\n",
      "5m 13s (- 16m 30s) (2200 24%) loss : 3.647  accuracy : 39.4 %\n",
      "5m 27s (- 16m 14s) (2300 25%) loss : 3.739  accuracy : 38.8 %\n",
      "5m 41s (- 16m 1s) (2400 26%) loss : 3.538  accuracy : 39.9 %\n",
      "5m 56s (- 15m 48s) (2500 27%) loss : 3.696  accuracy : 38.9 %\n",
      "6m 10s (- 15m 33s) (2600 28%) loss : 3.673  accuracy : 39.1 %\n",
      "6m 22s (- 15m 14s) (2700 29%) loss : 3.668  accuracy : 38.8 %\n",
      "6m 37s (- 15m 1s) (2800 30%) loss : 3.511  accuracy : 40.4 %\n",
      "6m 51s (- 14m 46s) (2900 31%) loss : 3.470  accuracy : 41.3 %\n",
      "7m 5s (- 14m 31s) (3000 32%) loss : 3.600  accuracy : 39.9 %\n",
      "7m 19s (- 14m 17s) (3100 33%) loss : 3.604  accuracy : 39.0 %\n",
      "7m 32s (- 14m 2s) (3200 34%) loss : 3.349  accuracy : 43.3 %\n",
      "7m 46s (- 13m 46s) (3300 36%) loss : 3.524  accuracy : 39.7 %\n",
      "8m 2s (- 13m 35s) (3400 37%) loss : 3.514  accuracy : 39.8 %\n",
      "8m 16s (- 13m 21s) (3500 38%) loss : 3.616  accuracy : 39.0 %\n",
      "8m 31s (- 13m 9s) (3600 39%) loss : 3.660  accuracy : 38.4 %\n",
      "8m 45s (- 12m 54s) (3700 40%) loss : 3.510  accuracy : 39.9 %\n",
      "9m 1s (- 12m 42s) (3800 41%) loss : 3.425  accuracy : 41.4 %\n",
      "9m 15s (- 12m 27s) (3900 42%) loss : 3.498  accuracy : 40.3 %\n",
      "9m 27s (- 12m 11s) (4000 43%) loss : 3.469  accuracy : 41.2 %\n",
      "9m 41s (- 11m 56s) (4100 44%) loss : 3.422  accuracy : 41.8 %\n",
      "9m 54s (- 11m 40s) (4200 45%) loss : 3.363  accuracy : 42.0 %\n",
      "10m 9s (- 11m 27s) (4300 46%) loss : 3.477  accuracy : 41.2 %\n",
      "10m 22s (- 11m 11s) (4400 48%) loss : 3.444  accuracy : 40.6 %\n",
      "10m 36s (- 10m 57s) (4500 49%) loss : 3.536  accuracy : 40.4 %\n",
      "10m 50s (- 10m 43s) (4600 50%) loss : 3.488  accuracy : 40.4 %\n",
      "11m 2s (- 10m 27s) (4700 51%) loss : 3.223  accuracy : 44.4 %\n",
      "11m 16s (- 10m 13s) (4800 52%) loss : 3.554  accuracy : 40.2 %\n",
      "11m 30s (- 9m 59s) (4900 53%) loss : 3.433  accuracy : 41.6 %\n",
      "11m 44s (- 9m 44s) (5000 54%) loss : 3.507  accuracy : 39.8 %\n",
      "11m 58s (- 9m 30s) (5100 55%) loss : 3.449  accuracy : 40.2 %\n",
      "12m 12s (- 9m 16s) (5200 56%) loss : 3.594  accuracy : 39.6 %\n",
      "12m 25s (- 9m 1s) (5300 57%) loss : 3.540  accuracy : 39.4 %\n",
      "12m 41s (- 8m 48s) (5400 59%) loss : 3.459  accuracy : 41.3 %\n",
      "12m 55s (- 8m 34s) (5500 60%) loss : 3.474  accuracy : 40.1 %\n",
      "13m 10s (- 8m 21s) (5600 61%) loss : 3.487  accuracy : 41.0 %\n",
      "13m 26s (- 8m 8s) (5700 62%) loss : 3.359  accuracy : 42.5 %\n",
      "13m 40s (- 7m 54s) (5800 63%) loss : 3.394  accuracy : 42.2 %\n",
      "13m 55s (- 7m 39s) (5900 64%) loss : 3.616  accuracy : 37.7 %\n",
      "14m 9s (- 7m 25s) (6000 65%) loss : 3.347  accuracy : 41.7 %\n",
      "14m 23s (- 7m 11s) (6100 66%) loss : 3.443  accuracy : 39.6 %\n",
      "14m 36s (- 6m 57s) (6200 67%) loss : 3.441  accuracy : 40.6 %\n",
      "14m 52s (- 6m 43s) (6300 68%) loss : 3.436  accuracy : 41.0 %\n",
      "15m 5s (- 6m 29s) (6400 69%) loss : 3.584  accuracy : 39.3 %\n",
      "15m 19s (- 6m 14s) (6500 71%) loss : 3.422  accuracy : 40.8 %\n",
      "15m 33s (- 6m 0s) (6600 72%) loss : 3.297  accuracy : 43.8 %\n",
      "15m 47s (- 5m 46s) (6700 73%) loss : 3.387  accuracy : 40.9 %\n",
      "16m 1s (- 5m 32s) (6800 74%) loss : 3.292  accuracy : 42.2 %\n",
      "16m 14s (- 5m 17s) (6900 75%) loss : 3.456  accuracy : 40.8 %\n",
      "16m 28s (- 5m 3s) (7000 76%) loss : 3.211  accuracy : 43.2 %\n",
      "16m 41s (- 4m 49s) (7100 77%) loss : 3.305  accuracy : 42.0 %\n",
      "16m 54s (- 4m 34s) (7200 78%) loss : 3.343  accuracy : 42.3 %\n",
      "17m 8s (- 4m 20s) (7300 79%) loss : 3.590  accuracy : 39.1 %\n",
      "17m 23s (- 4m 6s) (7400 80%) loss : 3.371  accuracy : 41.5 %\n",
      "17m 36s (- 3m 52s) (7500 81%) loss : 3.424  accuracy : 40.1 %\n",
      "17m 50s (- 3m 38s) (7600 83%) loss : 3.341  accuracy : 42.7 %\n",
      "18m 3s (- 3m 24s) (7700 84%) loss : 3.362  accuracy : 41.9 %\n",
      "18m 17s (- 3m 9s) (7800 85%) loss : 3.412  accuracy : 41.1 %\n",
      "18m 31s (- 2m 55s) (7900 86%) loss : 3.251  accuracy : 43.1 %\n",
      "18m 44s (- 2m 41s) (8000 87%) loss : 3.320  accuracy : 42.8 %\n",
      "18m 59s (- 2m 27s) (8100 88%) loss : 3.495  accuracy : 40.1 %\n",
      "19m 13s (- 2m 13s) (8200 89%) loss : 3.324  accuracy : 42.1 %\n",
      "19m 26s (- 1m 59s) (8300 90%) loss : 3.331  accuracy : 41.1 %\n",
      "19m 40s (- 1m 45s) (8400 91%) loss : 3.324  accuracy : 41.9 %\n",
      "19m 54s (- 1m 31s) (8500 92%) loss : 3.512  accuracy : 38.9 %\n",
      "20m 9s (- 1m 17s) (8600 93%) loss : 3.281  accuracy : 42.3 %\n",
      "20m 24s (- 1m 3s) (8700 95%) loss : 3.275  accuracy : 42.6 %\n",
      "20m 37s (- 0m 49s) (8800 96%) loss : 3.315  accuracy : 40.8 %\n",
      "20m 50s (- 0m 35s) (8900 97%) loss : 3.214  accuracy : 43.1 %\n",
      "21m 6s (- 0m 21s) (9000 98%) loss : 3.294  accuracy : 42.7 %\n",
      "21m 21s (- 0m 7s) (9100 99%) loss : 3.286  accuracy : 41.9 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches2, epochs = 1, lr = 0.0005, unmasked_ratio = 0.15, compute_accuracy = 'xs', print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7320"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches3 = []\n",
    "for seed in [124, 5864, 13, 475] :\n",
    "    batches3 += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches3))\n",
    "len(batches3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 15s (- 18m 33s) (100 1%) loss : 3.352  accuracy : 41.6 %\n",
      "0m 29s (- 17m 18s) (200 2%) loss : 3.182  accuracy : 43.5 %\n",
      "0m 44s (- 17m 28s) (300 4%) loss : 3.262  accuracy : 41.3 %\n",
      "0m 58s (- 17m 0s) (400 5%) loss : 3.244  accuracy : 43.1 %\n",
      "1m 14s (- 16m 54s) (500 6%) loss : 3.125  accuracy : 44.5 %\n",
      "1m 28s (- 16m 31s) (600 8%) loss : 3.163  accuracy : 44.0 %\n",
      "1m 42s (- 16m 7s) (700 9%) loss : 3.190  accuracy : 43.4 %\n",
      "1m 56s (- 15m 52s) (800 10%) loss : 3.329  accuracy : 42.3 %\n",
      "2m 10s (- 15m 28s) (900 12%) loss : 3.311  accuracy : 41.5 %\n",
      "2m 25s (- 15m 22s) (1000 13%) loss : 3.192  accuracy : 43.9 %\n",
      "2m 39s (- 15m 0s) (1100 15%) loss : 3.121  accuracy : 44.5 %\n",
      "2m 54s (- 14m 48s) (1200 16%) loss : 3.211  accuracy : 42.8 %\n",
      "3m 8s (- 14m 33s) (1300 17%) loss : 3.232  accuracy : 44.0 %\n",
      "3m 23s (- 14m 22s) (1400 19%) loss : 3.281  accuracy : 41.6 %\n",
      "3m 39s (- 14m 9s) (1500 20%) loss : 3.291  accuracy : 42.1 %\n",
      "3m 53s (- 13m 53s) (1600 21%) loss : 3.226  accuracy : 42.0 %\n",
      "4m 7s (- 13m 37s) (1700 23%) loss : 3.205  accuracy : 42.4 %\n",
      "4m 21s (- 13m 21s) (1800 24%) loss : 3.262  accuracy : 42.7 %\n",
      "4m 34s (- 13m 1s) (1900 25%) loss : 3.205  accuracy : 43.5 %\n",
      "4m 48s (- 12m 48s) (2000 27%) loss : 3.353  accuracy : 41.3 %\n",
      "5m 2s (- 12m 30s) (2100 28%) loss : 3.159  accuracy : 43.8 %\n",
      "5m 16s (- 12m 15s) (2200 30%) loss : 3.315  accuracy : 41.9 %\n",
      "5m 29s (- 11m 58s) (2300 31%) loss : 3.243  accuracy : 41.2 %\n",
      "5m 43s (- 11m 44s) (2400 32%) loss : 3.190  accuracy : 44.3 %\n",
      "5m 57s (- 11m 29s) (2500 34%) loss : 3.260  accuracy : 43.1 %\n",
      "6m 12s (- 11m 15s) (2600 35%) loss : 3.262  accuracy : 43.5 %\n",
      "6m 25s (- 10m 59s) (2700 36%) loss : 3.194  accuracy : 44.4 %\n",
      "6m 40s (- 10m 45s) (2800 38%) loss : 3.423  accuracy : 41.6 %\n",
      "6m 54s (- 10m 31s) (2900 39%) loss : 3.207  accuracy : 42.7 %\n",
      "7m 7s (- 10m 15s) (3000 40%) loss : 3.525  accuracy : 39.4 %\n",
      "7m 21s (- 10m 0s) (3100 42%) loss : 3.039  accuracy : 44.9 %\n",
      "7m 36s (- 9m 48s) (3200 43%) loss : 3.195  accuracy : 42.6 %\n",
      "7m 51s (- 9m 33s) (3300 45%) loss : 3.268  accuracy : 42.7 %\n",
      "8m 4s (- 9m 19s) (3400 46%) loss : 3.303  accuracy : 41.9 %\n",
      "8m 18s (- 9m 3s) (3500 47%) loss : 3.028  accuracy : 45.9 %\n",
      "8m 31s (- 8m 48s) (3600 49%) loss : 3.254  accuracy : 42.6 %\n",
      "8m 43s (- 8m 31s) (3700 50%) loss : 3.138  accuracy : 45.1 %\n",
      "8m 55s (- 8m 15s) (3800 51%) loss : 3.205  accuracy : 42.9 %\n",
      "9m 9s (- 8m 2s) (3900 53%) loss : 3.149  accuracy : 44.6 %\n",
      "9m 23s (- 7m 47s) (4000 54%) loss : 3.319  accuracy : 42.8 %\n",
      "9m 39s (- 7m 34s) (4100 56%) loss : 3.201  accuracy : 43.4 %\n",
      "9m 54s (- 7m 21s) (4200 57%) loss : 3.284  accuracy : 42.5 %\n",
      "10m 9s (- 7m 8s) (4300 58%) loss : 3.255  accuracy : 42.1 %\n",
      "10m 23s (- 6m 53s) (4400 60%) loss : 3.115  accuracy : 45.6 %\n",
      "10m 37s (- 6m 39s) (4500 61%) loss : 3.132  accuracy : 43.9 %\n",
      "10m 52s (- 6m 25s) (4600 62%) loss : 3.232  accuracy : 43.3 %\n",
      "11m 7s (- 6m 11s) (4700 64%) loss : 3.163  accuracy : 43.8 %\n",
      "11m 20s (- 5m 57s) (4800 65%) loss : 3.215  accuracy : 42.2 %\n",
      "11m 35s (- 5m 43s) (4900 66%) loss : 3.070  accuracy : 45.4 %\n",
      "11m 49s (- 5m 29s) (5000 68%) loss : 3.261  accuracy : 42.8 %\n",
      "12m 3s (- 5m 14s) (5100 69%) loss : 3.290  accuracy : 43.2 %\n",
      "12m 17s (- 5m 0s) (5200 71%) loss : 3.278  accuracy : 42.1 %\n",
      "12m 31s (- 4m 46s) (5300 72%) loss : 3.252  accuracy : 42.2 %\n",
      "12m 45s (- 4m 32s) (5400 73%) loss : 3.267  accuracy : 42.9 %\n",
      "12m 59s (- 4m 18s) (5500 75%) loss : 3.157  accuracy : 44.3 %\n",
      "13m 14s (- 4m 3s) (5600 76%) loss : 3.172  accuracy : 42.9 %\n",
      "13m 28s (- 3m 49s) (5700 77%) loss : 3.128  accuracy : 43.9 %\n",
      "13m 42s (- 3m 35s) (5800 79%) loss : 3.322  accuracy : 40.9 %\n",
      "13m 57s (- 3m 21s) (5900 80%) loss : 3.258  accuracy : 43.1 %\n",
      "14m 9s (- 3m 6s) (6000 81%) loss : 3.103  accuracy : 45.8 %\n",
      "14m 24s (- 2m 52s) (6100 83%) loss : 3.318  accuracy : 42.3 %\n",
      "14m 39s (- 2m 38s) (6200 84%) loss : 3.214  accuracy : 44.4 %\n",
      "14m 54s (- 2m 24s) (6300 86%) loss : 3.132  accuracy : 44.3 %\n",
      "15m 9s (- 2m 10s) (6400 87%) loss : 3.188  accuracy : 43.7 %\n",
      "15m 23s (- 1m 56s) (6500 88%) loss : 3.280  accuracy : 43.0 %\n",
      "15m 36s (- 1m 42s) (6600 90%) loss : 3.161  accuracy : 44.4 %\n",
      "15m 49s (- 1m 27s) (6700 91%) loss : 3.236  accuracy : 42.7 %\n",
      "16m 2s (- 1m 13s) (6800 92%) loss : 3.201  accuracy : 43.3 %\n",
      "16m 16s (- 0m 59s) (6900 94%) loss : 3.192  accuracy : 43.5 %\n",
      "16m 30s (- 0m 45s) (7000 95%) loss : 3.085  accuracy : 44.8 %\n",
      "16m 45s (- 0m 31s) (7100 96%) loss : 3.141  accuracy : 44.3 %\n",
      "16m 58s (- 0m 16s) (7200 98%) loss : 3.235  accuracy : 42.6 %\n",
      "17m 12s (- 0m 2s) (7300 99%) loss : 3.143  accuracy : 43.8 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches3, epochs = 1, lr = 0.0001, unmasked_ratio = 0.15, compute_accuracy = 'xs', print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(denoiser.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth')\n",
    "\n",
    "# load\n",
    "#denoiser.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstitution diluent specific of the product to be analysed\n",
      "\n",
      "\n",
      "reconstitution diluent \u001b[93mvolume\u001b[0m of the product to be \u001b[93manalyzed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "denoiser.eval()\n",
    "sentence = ' '.join(sentences[14]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "denoiser(sentence, color = '\\033[93m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "through the vial stopper add the appropriate volume of reconstitution solvent using a syringe fitted with a needle ( diameter smaller than or equal to 0.6 mm )\n",
      "\n",
      "\n",
      "through the vial stopper add the appropriate volume of reconstitution solvent using a syringe fitted with a needle ( \u001b[93mis\u001b[0m \u001b[93mgreater\u001b[0m than or equal to \u001b[93mmm\u001b[0m mm )\n"
     ]
    }
   ],
   "source": [
    "denoiser.eval()\n",
    "sentence = ' '.join(sentences[21]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "denoiser(sentence, color = '\\033[93m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctCorpus(word2vec, corpus, threshold = 0.9) :\n",
    "    new_sentences = []\n",
    "    corrections = []\n",
    "    for s in corpus :\n",
    "        new_s = []\n",
    "        for word in s :\n",
    "            try : #fasttext raises an error if no character ngram seen during training appears in the word\n",
    "                if (word not in word2vec.wv.vocab and word2vec.most_similar(word)[0][1] >= threshold) :\n",
    "                    new_word = word2vec.most_similar(word)[0][0]\n",
    "                    new_s.append(new_word)\n",
    "                    corrections.append([word, new_word])\n",
    "                else :\n",
    "                    new_s.append(word)\n",
    "            except : \n",
    "                new_s.append(word)\n",
    "        new_sentences.append(new_s)\n",
    "    return new_sentences, corrections\n",
    "\n",
    "\n",
    "def contextualCorrectCorpus(denoiser, corpus, threshold = 0.9, print_every = 100) :\n",
    "    word2vec = denoiser.word2vec.word2vec\n",
    "    new_sentences = []\n",
    "    corrections = []\n",
    "    corpus = [s for s in corpus if len(s) > 0]\n",
    "    for ws in corpus : \n",
    "        probs = denoiser.predict_proba(ws).squeeze(0) # size = (input_length, lang_size)\n",
    "        new_s = []\n",
    "        for i, word in enumerate(ws) :\n",
    "            try : #fasttext raises an error if no character ngram seen during training appears in the word\n",
    "                candidates = [wp[0] for wp in word2vec.most_similar(word) if wp[1] >= threshold]\n",
    "                if (word not in word2vec.wv.vocab and candidates != []) :\n",
    "                    indices  = [denoiser.word2vec.lang.getIndex(w) for w in candidates]\n",
    "                    probsi   = [probs[i, j].item() for j in indices]\n",
    "                    wps      = [[w, p] for w, p in zip(candidates, probsi)]\n",
    "                    wps.sort(key = lambda wp : wp[1], reverse = True)\n",
    "                    new_word = wps[0][0]\n",
    "                    new_s.append(new_word)\n",
    "                    corrections.append([word, new_word])\n",
    "                else :\n",
    "                    new_s.append(word)\n",
    "            except : \n",
    "                new_s.append(word)\n",
    "        new_sentences.append(new_s)\n",
    "        if len(new_sentences)+1 % print_every == 0 : print(len(new_sentences)+1)\n",
    "    return new_sentences, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_corpus, corrections = correctCorpus(word2vec, corpus, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_corrected_corpus, contextual_corrections = contextualCorrectCorpus(denoiser, corpus[:200], threshold = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riboflavin -> ribosyl  |  ribosyl\n",
      "22.5cm -> 22.9  |  22.9\n",
      "return -> returned  |  returned\n",
      "atmospheric -> atmosphere  |  atmosphere\n",
      "tracer -> trace  |  trace\n",
      "2.1.6 -> 2.1.1.1  |  2.1.1.1\n",
      "duplicated -> duplicate  |  duplicate\n",
      "interpreting -> interpretation  |  interpretation\n",
      "pseudoalcaligenes -> pseudomonas  |  \u001b[93msporogenes\u001b[0m\n",
      "coagulogen -> coagulase  |  coagulase\n",
      "coagulin -> coagulase  |  coagulase\n",
      "0.0625 -> 0.06  |  0.06\n",
      "inhibitor -> inhibitory  |  inhibitory\n",
      "fertilized -> utilized  |  utilized\n",
      "fulfil -> fulfilled  |  fulfilled\n",
      "872 -> 72  |  72\n",
      "872 -> 72  |  72\n",
      "pharmacovigilance -> pharmacopoeias  |  \u001b[93mpharmacopoeia\u001b[0m\n",
      "104.1 -> 104.6  |  104.6\n",
      "riboflavin -> ribosyl  |  ribosyl\n",
      "22.5cm -> 22.9  |  22.9\n",
      "return -> returned  |  returned\n",
      "atmospheric -> atmosphere  |  atmosphere\n",
      "tracer -> trace  |  trace\n",
      "2.1.6 -> 2.1.1.1  |  2.1.1.1\n",
      "duplicated -> duplicate  |  duplicate\n",
      "interpreting -> interpretation  |  interpretation\n",
      "lureus -> aureus  |  aureus\n",
      "pseudoalcaligenes -> pseudomonas  |  \u001b[93msporogenes\u001b[0m\n",
      "coagulogen -> coagulase  |  coagulase\n",
      "coagulin -> coagulase  |  coagulase\n",
      "threebatches -> batches  |  batches\n",
      "fertilized -> utilized  |  utilized\n",
      "pharmacovigilance -> pharmacopoeias  |  \u001b[93mpharmacopoeia\u001b[0m\n",
      "104.0 -> 104.6  |  104.6\n",
      "104.1 -> 104.6  |  104.6\n",
      "104.5 -> 104.6  |  104.6\n",
      "unheated -> heated  |  heated\n",
      "eol -> eosin  |  eosin\n",
      "alkalinity -> alkaline  |  alkaline\n",
      "chlorides -> chloride  |  chloride\n",
      "sulfates -> sulfate  |  sulfate\n",
      "extractables -> extractable  |  extractable\n",
      "extractables -> extractable  |  extractable\n",
      "functionality -> functional  |  functional\n",
      "functionality -> functional  |  functional\n",
      "3.2.p.7 -> 3.2.p.2.2  |  \u001b[93m3.2.p.2.3\u001b[0m\n",
      "associates -> associated  |  associated\n",
      "cooling -> pooling  |  pooling\n",
      "walls -> falls  |  falls\n",
      "resembles -> assembled  |  assembled\n",
      "51.0 -> 51.7  |  51.7\n",
      "crosslinked -> linked  |  linked\n",
      "polyethylen -> ethylenediamine  |  ethylenediamine\n",
      "calibrate -> calibrated  |  calibrated\n",
      "phenoxyeth -> phenoxyethanol  |  phenoxyethanol\n",
      "phenoxyeth -> phenoxyethanol  |  phenoxyethanol\n",
      "hestrin's -> hestrin  |  hestrin\n",
      "tbd -> tbs  |  tbs\n",
      "3181.68 -> 181.68  |  181.68\n",
      "0.9960 -> 0.9996  |  \u001b[93m0.9993\u001b[0m\n",
      "51.74 -> 51.7  |  51.7\n",
      "51.74 -> 51.7  |  51.7\n",
      "0.9953 -> 0.995  |  \u001b[93m0.990\u001b[0m\n",
      "52.6 -> 52.8  |  52.8\n",
      "52.6 -> 52.8  |  52.8\n",
      "0.0375 -> 0.03  |  0.03\n",
      "0.0106 -> 0.010  |  0.010\n",
      "1.0365 -> 1.03  |  1.03\n",
      "0.0228 -> 0.021  |  0.021\n",
      "0.0375 -> 0.03  |  0.03\n",
      "0.0106 -> 0.010  |  0.010\n",
      "1.0365 -> 1.03  |  1.03\n",
      "0.0228 -> 0.021  |  0.021\n",
      "57.8 -> 7.8  |  7.8\n",
      "millilitre -> milliliter  |  milliliter\n",
      "conventionally -> conventional  |  conventional\n",
      "2.158 -> 2.15  |  2.15\n",
      "0.278 -> 0.27  |  0.27\n",
      "0.069 -> 0.06  |  0.06\n",
      "57.8 -> 7.8  |  7.8\n",
      "hepatitisa -> hepatitis  |  hepatitis\n",
      "surrounds -> surrounding  |  surrounding\n",
      "underestimation -> underestimated  |  underestimated\n",
      "3.2.s.4.5 -> 3.2.s.4.3  |  \u001b[93m3.2.s.4.2\u001b[0m\n",
      "0.106 -> 0.100  |  0.100\n",
      "0.085 -> 0.081  |  \u001b[93m0.020\u001b[0m\n",
      "immersing -> immersion  |  immersion\n",
      "dehydrated -> rehydrated  |  rehydrated\n",
      "antivenimous -> antivenomous  |  antivenomous\n",
      "ethylene -> ethylenediamine  |  \u001b[93methylenediaminetetraacetic\u001b[0m\n",
      "na2 -> nahc03  |  \u001b[93mna2c03\u001b[0m\n",
      "2h2o -> h2o  |  h2o\n",
      "monohydrate -> pentahydrate  |  \u001b[93mdecahydrate\u001b[0m\n",
      "nah2 -> nahc03  |  \u001b[93mna2c03\u001b[0m\n",
      "na2 -> nahc03  |  \u001b[93mna2c03\u001b[0m\n",
      "2h2o -> h2o  |  h2o\n",
      "monohydrate -> pentahydrate  |  \u001b[93mdecahydrate\u001b[0m\n",
      "nah2 -> nahc03  |  \u001b[93mna2c03\u001b[0m\n",
      "11.688 -> 181.68  |  181.68\n",
      "fab -> fab'  |  fab'\n",
      "derivate -> derivatization  |  \u001b[93mderivatized\u001b[0m\n",
      "evaporate -> evaporation  |  evaporation\n",
      "polysorbate -> polysorbate  |  polysorbate\n",
      "microwell -> microwells  |  microwells\n",
      "microwell -> microwells  |  microwells\n",
      "phenylenediamine -> ethylenediamine  |  ethylenediamine\n",
      "h2o2 -> h2o  |  h2o\n",
      "h2so4 -> cuso4  |  cuso4\n",
      "decomposed -> composed  |  composed\n",
      "liberate -> deliberate  |  deliberate\n",
      "uncolored -> colored  |  colored\n",
      "distill -> distilled  |  distilled\n",
      "v2 -> v2  |  v2\n",
      "14.01 -> 14.01.103  |  \u001b[93mthcl\u001b[0m\n",
      "14.01 -> 14.01.103  |  14.01.103\n",
      "v3 -> ipv3  |  \u001b[93mipv2\u001b[0m\n",
      "14.01 -> 14.01.103  |  \u001b[93mthcl\u001b[0m\n",
      "v3 -> ipv3  |  \u001b[93mipv2\u001b[0m\n",
      "1970 -> 197.333  |  \u001b[93m183.667\u001b[0m\n",
      "bacteriophage -> bacteria  |  bacteria\n",
      "685 -> 687a  |  \u001b[93m5685a\u001b[0m\n",
      "1976 -> 197.333  |  \u001b[93m183.667\u001b[0m\n",
      "pepsin -> trypsin  |  trypsin\n",
      "horizontal -> horizontally  |  horizontally\n",
      "polypeptides -> peptides  |  peptides\n",
      "volts -> voltage  |  voltage\n",
      "bridge -> bridging  |  bridging\n",
      "polypeptide -> peptide  |  \u001b[93mpeptides\u001b[0m\n",
      "rf -> rfu  |  rfu\n",
      "satisfies -> satisfied  |  satisfied\n",
      "1.271 -> 1.27  |  1.27\n",
      "electophoretic -> electrophoretic  |  electrophoretic\n",
      "55987 -> k1387  |  \u001b[93mk7229\u001b[0m\n",
      "500 -> 2.500  |  \u001b[93m3.500\u001b[0m\n",
      "electophoretic -> electrophoretic  |  electrophoretic\n",
      "deposites -> deposited  |  \u001b[93mdeposit\u001b[0m\n",
      "37f9326 -> 68  |  \u001b[93m34\u001b[0m\n",
      "37f9326 -> 68  |  \u001b[93m34\u001b[0m\n",
      "mention -> mentioned  |  mentioned\n",
      "promotes -> promoting  |  promoting\n",
      "identificationbovine -> identification  |  identification\n",
      "identificationviper -> identification  |  identification\n",
      "bulkfilled -> filled  |  \u001b[93mbulk\u001b[0m\n",
      "productporcine -> porcine  |  porcine\n",
      "identificationovine -> identification  |  identification\n",
      "identificationequine -> identification  |  identification\n",
      "identificationbovine -> identification  |  identification\n",
      "identificationviper -> identification  |  identification\n",
      "bulkfilled -> filled  |  \u001b[93mbulk\u001b[0m\n",
      "productporcine -> porcine  |  porcine\n",
      "identificationovine -> identification  |  identification\n",
      "160 -> 1160  |  1160\n",
      "160 -> 1160  |  1160\n",
      "vipervaf -> vipera  |  vipera\n",
      "invalided -> invalid  |  invalid\n",
      "4.460 -> 4.4  |  4.4\n",
      "0.955 -> 0.95  |  0.95\n",
      "0.056 -> 0.05  |  0.05\n",
      "0.9939 -> 0.993  |  0.993\n",
      "polysorbarte -> polysorbate  |  polysorbate\n",
      "diltheoretical -> theoretical  |  theoretical\n",
      "dilobserved -> observed  |  observed\n",
      "diltheoretical -> theoretical  |  theoretical\n",
      "0.979 -> 0.97  |  0.97\n",
      "0.008 -> 0.006  |  \u001b[93m0.009\u001b[0m\n",
      "0.9999 -> 0.999  |  \u001b[93m0.9993\u001b[0m\n",
      "0.191 -> 0.1509  |  \u001b[93m0.1295\u001b[0m\n",
      "0.012 -> 0.010  |  \u001b[93m0.014\u001b[0m\n",
      "0.9998 -> 0.9992  |  \u001b[93m0.9993\u001b[0m\n",
      "anionic -> anion  |  \u001b[93mexchange\u001b[0m\n",
      "pepsin -> trypsin  |  trypsin\n",
      "actually -> actual  |  actual\n",
      "polypeptide -> peptide  |  \u001b[93mpeptides\u001b[0m\n",
      "appeared -> appear  |  appear\n",
      "0.084 -> 0.081  |  \u001b[93m0.048\u001b[0m\n",
      "0.086 -> 0.081  |  0.081\n",
      "5.34 -> 5.3  |  5.3\n",
      "5.34 -> 5.3  |  5.3\n",
      "2006 -> cpmrs2006  |  cpmrs2006\n",
      "4.2 -> 24.2  |  24.2\n",
      "4.2 -> 24.2  |  24.2\n",
      "ramp -> ramps  |  ramps\n",
      "evolved -> involved  |  involved\n",
      "disappears -> appears  |  appears\n",
      "neutralise -> neutral  |  neutral\n",
      "3.5diacetyl -> diacetyl  |  diacetyl\n",
      "1.4dihydrolutidine -> dihydrolutidine  |  dihydrolutidine\n",
      "crossed -> cross  |  cross\n",
      "mustafa -> must  |  must\n",
      "1978 -> 197.333  |  197.333\n",
      "november -> december  |  december\n",
      "hydroxylammonium -> hydroxylamine  |  hydroxylamine\n",
      "consumed -> assumed  |  assumed\n",
      "analyser -> analysed  |  \u001b[93manalyses\u001b[0m\n",
      "hydroxylammonium -> hydroxylamine  |  hydroxylamine\n",
      "digested -> digest  |  digest\n",
      "analyse -> analysed  |  analysed\n",
      "0.149 -> 0.14  |  0.14\n",
      "1.274 -> 1.27  |  1.27\n",
      "0.149 -> 0.14  |  0.14\n",
      "1.274 -> 1.27  |  1.27\n",
      "aspergillus -> bacillus  |  bacillus\n",
      "proteus -> protected  |  protected\n",
      "060947 -> fdnc1947  |  \u001b[93mfdnc1948\u001b[0m\n",
      "99.5 -> 99.4  |  99.4\n",
      "0.080 -> 0.081  |  0.081\n",
      "0.080 -> 0.081  |  0.081\n",
      "3.54 -> 3.5  |  3.5\n",
      "42.8 -> 2.8  |  2.8\n",
      "100.6 -> 100.3  |  100.3\n",
      "99.9 -> 99.4  |  \u001b[93m99.6\u001b[0m\n",
      "100.6 -> 100.3  |  100.3\n",
      "17.50 -> 17.5  |  17.5\n",
      "70.66 -> 0.66  |  0.66\n",
      "100.1 -> 100.3  |  100.3\n",
      "100.1 -> 100.3  |  100.3\n",
      "symptom -> asymptotic  |  asymptotic\n",
      "0.450.85 -> 0.85  |  0.85\n",
      "''vaccines -> vaccines  |  vaccines\n",
      "complexes -> complexed  |  complexed\n",
      "depicted -> depicts  |  depicts\n",
      "c3 -> c3146  |  \u001b[93mc3147\u001b[0m\n",
      "1h00 -> 100  |  100\n",
      "washings -> washing  |  washing\n",
      "1h00 -> 100  |  100\n",
      "washings -> washing  |  washing\n",
      "polynomial -> binomial  |  binomial\n",
      "0.070 -> 0.07  |  0.07\n",
      "odc0 -> odc  |  \u001b[93modt\u001b[0m\n",
      "0.070 -> 0.07  |  \u001b[93m0.009\u001b[0m\n",
      "0.400 -> 0.40  |  0.40\n",
      "odc2 -> odc  |  \u001b[93modt\u001b[0m\n",
      "0.400 -> 0.40  |  0.40\n",
      "odc0 -> odc  |  \u001b[93modt\u001b[0m\n",
      "c3 -> c3146  |  \u001b[93mc3886\u001b[0m\n",
      "odc3 -> odc  |  \u001b[93modt\u001b[0m\n",
      "fractioning -> fraction  |  \u001b[93mfractions\u001b[0m\n",
      "property -> properties  |  properties\n",
      "become -> becomes  |  becomes\n",
      "charged -> charge  |  charge\n",
      "dispose -> disposable  |  disposable\n",
      "diffused -> diffuse  |  diffuse\n",
      "hydragel -> hydrasys  |  hydrasys\n",
      "alternately -> alternate  |  alternate\n",
      "visualize -> visualized  |  visualized\n",
      "sheets -> sheet  |  sheet\n",
      "papers -> paper  |  paper\n",
      "r4 -> pr4  |  pr4\n",
      "dixon -> wilcoxon  |  wilcoxon\n",
      "22.55 -> 4.55  |  \u001b[93m22.9\u001b[0m\n",
      "theo -> ctheo  |  ctheo\n",
      "theo -> ctheo  |  ctheo\n",
      "xvolume -> volume  |  volume\n",
      "xvolume -> volume  |  volume\n",
      "132 -> 132v  |  132v\n",
      "0.860 -> 0.86  |  0.86\n",
      "0.860 -> 0.86  |  0.86\n",
      "0.140 -> 0.14  |  0.14\n",
      "returns -> returned  |  returned\n",
      "decimals -> decimal  |  decimal\n",
      "returns -> returned  |  returned\n",
      "97.6 -> 7.6  |  7.6\n",
      "immunoglobulinbatch -> immunoglobulin  |  immunoglobulin\n",
      "194 -> 194194  |  194194\n",
      "recommandations -> recommendations  |  recommendations\n",
      "derivatives -> derivatized  |  \u001b[93mderivatization\u001b[0m\n",
      "aims -> aim  |  aim\n",
      "2.2.22 -> 2.2.23  |  \u001b[93m2.2.29\u001b[0m\n",
      "convention -> conventional  |  conventional\n",
      "immunoelectrophoretic -> immunoelectrophoresis  |  immunoelectrophoresis\n",
      "biotechnology -> biotechnological  |  biotechnological\n",
      "reasonable -> reason  |  reason\n",
      "clearance -> appearance  |  appearance\n",
      "instituted -> institute  |  institute\n",
      "undergone -> undergo  |  \u001b[93mundergoes\u001b[0m\n",
      "clearance -> appearance  |  appearance\n",
      "ch3coona -> ch3cho  |  ch3cho\n",
      "ch3coonh4 -> ch3cho  |  ch3cho\n",
      "ch3cooh -> ch3cho  |  ch3cho\n",
      "cooling -> pooling  |  pooling\n",
      "walls -> falls  |  falls\n",
      "resembles -> assembled  |  assembled\n",
      "trouble -> double  |  double\n",
      "186 -> k7186  |  \u001b[93mk7192\u001b[0m\n",
      "h2so4 -> cuso4  |  cuso4\n",
      "odblank -> blank  |  blank\n",
      "crosslinked -> linked  |  linked\n",
      "polyethylen -> ethylenediamine  |  ethylenediamine\n",
      "calibrate -> calibrated  |  calibrated\n",
      "0.9960 -> 0.9996  |  \u001b[93m0.9993\u001b[0m\n",
      "10.17 -> 0.17  |  0.17\n",
      "0.56 -> 0.563  |  0.563\n",
      "10.17 -> 0.17  |  0.17\n",
      "13.46 -> 3.46  |  3.46\n",
      "3.50 -> 3.500  |  3.500\n",
      "0.9174 -> 0.91  |  0.91\n",
      "1m01and -> 1m03  |  1m03\n",
      "reveals -> revealed  |  revealed\n",
      "excipient1 -> excipient  |  excipient\n",
      "0.975 -> 0.97  |  0.97\n",
      "doubling -> double  |  double\n",
      "aspired -> aspirated  |  aspirated\n",
      "multiply -> multiplying  |  multiplying\n",
      "mdv -> udv  |  udv\n",
      "cellular -> acellular  |  acellular\n",
      "immunoenzymatic -> enzymatic  |  enzymatic\n",
      "doubling -> double  |  double\n",
      "seeding -> bleeding  |  bleeding\n",
      "dispenser -> dispense  |  dispense\n",
      "aspirate -> aspirated  |  aspirated\n",
      "multiply -> multiplying  |  multiplying\n",
      "105.00 -> 5.00  |  5.00\n",
      "dicc50 -> 10dicc50  |  10dicc50\n",
      "dicc50 -> 10dicc50  |  10dicc50\n",
      "monolayers -> layers  |  layers\n",
      "earle -> earlier  |  earlier\n",
      "nahco3 -> nahc03  |  \u001b[93mna2c03\u001b[0m\n",
      "2.54 -> 2.5.33  |  2.5.33\n",
      "dihydrostreptomycin -> streptomycin  |  streptomycin\n",
      "isothiocyanate -> thiocyanate  |  thiocyanate\n",
      "microscopic -> macroscopic  |  macroscopic\n",
      "cm² -> cm  |  cm\n",
      "weekly -> weeks  |  weeks\n",
      "weekly -> weeks  |  weeks\n",
      "resuspended -> resuspend  |  resuspend\n",
      "thawed -> thaw  |  thaw\n",
      "glasses -> glass  |  glass\n",
      "isothiocyanate -> thiocyanate  |  thiocyanate\n",
      "incubations -> incubation  |  incubation\n",
      "microscope -> macroscopic  |  macroscopic\n",
      "feeding -> bleeding  |  bleeding\n",
      "microscope -> macroscopic  |  macroscopic\n",
      "inclusions -> inclusion  |  inclusion\n",
      "0.55 -> 4.55  |  4.55\n",
      "78.8 -> 8.8  |  8.8\n",
      "0.000942 -> 0.0001  |  0.0001\n",
      "0.069 -> 0.06  |  \u001b[93m0.064\u001b[0m\n",
      "0.975 -> 0.97  |  0.97\n",
      "4series -> series  |  series\n",
      "11.033 -> 1.03  |  1.03\n",
      "11.033 -> 1.03  |  1.03\n",
      "11.034 -> 1.03  |  1.03\n",
      "107.15 -> 7.15  |  7.15\n",
      "107.15 -> 7.15  |  7.15\n",
      "amplifications -> amplification  |  amplification\n",
      "characters -> characterized  |  characterized\n",
      "campaigns -> campaign  |  campaign\n",
      "importance -> important  |  important\n",
      "survey -> surveillance  |  surveillance\n",
      "0.2049 -> 0.20  |  0.20\n",
      "isolated -> isolate  |  isolate\n",
      "bottles -> bottle  |  bottle\n",
      "bottles -> bottle  |  bottle\n",
      "sorbitol -> ribitol  |  \u001b[93mpolyribosyl\u001b[0m\n",
      "sorbitol -> ribitol  |  \u001b[93mpolyribosyl\u001b[0m\n",
      "certificates -> certificate  |  certificate\n",
      "documentation -> document  |  document\n",
      "inconclusive -> conclusion  |  conclusion\n",
      "alternatively -> alternative  |  alternative\n",
      "0.0025 -> 0.002  |  0.002\n",
      "funnels -> funnel  |  funnel\n",
      "combine -> combines  |  combines\n",
      "mictroplate -> microplate  |  microplate\n",
      "14.0 -> 14.01.103  |  14.01.103\n",
      "vortexer -> vortexed  |  \u001b[93mvortex\u001b[0m\n",
      "carbonizes -> carbonate  |  carbonate\n",
      "digesting -> digestion  |  \u001b[93mdigest\u001b[0m\n",
      "pinkish -> pink  |  pink\n",
      "liberated -> deliberate  |  deliberate\n",
      "involve -> involves  |  involves\n",
      "6.2and -> 6.25  |  6.25\n",
      "200.000 -> 40.000  |  \u001b[93m0.000\u001b[0m\n",
      "37.0 -> 7.0  |  7.0\n",
      "covalent -> monovalent  |  monovalent\n",
      "distinguishing -> distinguish  |  distinguish\n",
      "ttbs -> tbs  |  tbs\n",
      "ttbs -> tbs  |  tbs\n",
      "naphthol -> pyridylazonaphthol  |  pyridylazonaphthol\n",
      "figure1 -> figure  |  figure\n",
      "traceable -> trace  |  trace\n",
      "bridge -> bridging  |  bridging\n",
      "mathematically -> mathematical  |  mathematical\n",
      "tissue -> issued  |  issued\n",
      "lotsresponses -> responses  |  responses\n",
      "lotsfigure -> figure  |  figure\n",
      "vaccinelots -> vaccinea  |  vaccinea\n",
      "mice100lot# -> 5lot#  |  \u001b[93mlot#\u001b[0m\n",
      "710lot# -> lot#  |  lot#\n",
      "vaccinelots -> vaccinea  |  vaccinea\n",
      "mice100lot# -> 5lot#  |  \u001b[93mlot#\u001b[0m\n",
      "710lot# -> lot#  |  lot#\n",
      "lotsresponses -> responses  |  responses\n",
      "lotsgeomean -> geomean  |  geomean\n",
      "determinationsa -> determination  |  determination\n",
      "0.99974 -> 0.9996  |  0.9996\n",
      "0.99850 -> 0.998  |  0.998\n",
      "0.99974 -> 0.9996  |  0.9996\n",
      "0.99826 -> 0.998  |  0.998\n",
      "displays -> display  |  display\n",
      "0.99996 -> 0.9996  |  0.9996\n",
      "tdapa -> tdap  |  tdap\n",
      "c2883a -> c2863a  |  c2863a\n",
      "1.1.1 -> 2.1.1.1  |  2.1.1.1\n",
      "c2883a -> c2863a  |  c2863a\n",
      "tighter -> tightened  |  tightened\n",
      "studywas -> study  |  study\n",
      "optimize -> optimized  |  optimized\n",
      "thirteen -> thirty  |  thirty\n",
      "reassigned -> assigned  |  assigned\n",
      "1.1.2.1.1 -> 2.1.1.1  |  2.1.1.1\n",
      "thirteen -> thirty  |  thirty\n",
      "explanation -> explanations  |  explanations\n",
      "1.9 -> 2731.9  |  2731.9\n",
      "69.2 -> 9.2  |  9.2\n",
      "fourteen -> sixteen  |  sixteen\n",
      "fourteen -> sixteen  |  sixteen\n",
      "42.7 -> 2.7  |  2.7\n",
      "pigsa -> pigs  |  pigs\n",
      "s1 -> s17  |  \u001b[93ms18\u001b[0m\n",
      "1.2.2.1 -> 2.2.1  |  2.2.1\n",
      "purposely -> purpose  |  purpose\n",
      "midpoint -> endpoint  |  endpoint\n",
      "termination -> determination  |  determination\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pigsa -> pigs  |  pigs\n",
      "daysa -> days  |  days\n",
      "daysa -> days  |  days\n",
      "1.3.1 -> 2.3.1  |  2.3.1\n",
      "0.003160 -> 0.003  |  \u001b[93m0.006\u001b[0m\n",
      "0.13721x -> 0.1315  |  0.1315\n",
      "0.000643 -> 0.00062  |  0.00062\n",
      "0.13664x -> 0.1315  |  \u001b[93m0.135\u001b[0m\n",
      "0.99999 -> 0.999  |  \u001b[93m0.9993\u001b[0m\n",
      "differentiation -> differentiate  |  \u001b[93mdifferentiated\u001b[0m\n",
      "agglutination -> agglutinate  |  \u001b[93mhemagglutination\u001b[0m\n",
      "interpreted -> interpretation  |  interpretation\n",
      "credibility -> compatibility  |  compatibility\n",
      "throughput -> through  |  through\n",
      "electronics -> electronically  |  electronically\n",
      "8.00 -> 8.0  |  8.0\n",
      "adequately -> adequate  |  adequate\n",
      "comprising -> comprised  |  comprised\n",
      "8.00 -> 8.0  |  8.0\n",
      "deliberately -> deliberate  |  deliberate\n",
      "22.3oc -> 22.9  |  22.9\n",
      "3.2.p.3.2 -> 3.2.p.3.4  |  \u001b[93m3.2.p.3.3\u001b[0m\n",
      "dtapa -> dtap  |  dtap\n",
      "improvement -> improve  |  improve\n",
      "2.7.2 -> 2.7.20  |  2.7.20\n",
      "2.7.3 -> 2.7.6  |  \u001b[93m2.7.8\u001b[0m\n",
      "2.7.4 -> 2.7.6  |  \u001b[93m2.7.16\u001b[0m\n",
      "2.7.1.1 -> 2.7.1  |  2.7.1\n",
      "2.7.1.2 -> 2.7.1  |  2.7.1\n",
      "avoidance -> avoid  |  avoid\n",
      "rejecting -> reject  |  \u001b[93mrejection\u001b[0m\n",
      "retransformed -> transformed  |  transformed\n",
      "similarity -> similar  |  similar\n",
      "wise -> otherwise  |  \u001b[93mpairwise\u001b[0m\n",
      "sigmoidal -> sigma  |  sigma\n",
      "largely -> large  |  \u001b[93mlarger\u001b[0m\n",
      "8du -> du  |  du\n",
      "loqs -> loq  |  loq\n",
      "loqs -> loq  |  loq\n",
      "distinguishing -> distinguish  |  distinguish\n",
      "meningitides -> meningitidis  |  meningitidis\n",
      "valdereuil -> reuil  |  reuil\n",
      "0.0025 -> 0.002  |  0.002\n",
      "funnels -> funnel  |  funnel\n",
      "combine -> combines  |  combines\n",
      "vortexer -> vortexed  |  \u001b[93mvortex\u001b[0m\n",
      "carbonizes -> carbonate  |  carbonate\n",
      "digesting -> digestion  |  \u001b[93mdigest\u001b[0m\n",
      "pinkish -> pink  |  pink\n",
      "liberated -> deliberate  |  deliberate\n",
      "200.000 -> 40.000  |  \u001b[93m0.000\u001b[0m\n",
      "37.0 -> 7.0  |  7.0\n",
      "trichloroacetate -> trichloroacetic  |  trichloroacetic\n",
      "enhancer -> enhancement  |  \u001b[93minhibition\u001b[0m\n",
      "500 -> 2.500  |  \u001b[93m3.500\u001b[0m\n",
      "stacking -> packing  |  packing\n",
      "electrophoresed -> electrophoresis  |  electrophoresis\n",
      "scanned -> scanner  |  scanner\n",
      "antidiphtheria -> diphtheria  |  diphtheria\n",
      "17.8 -> 17.5  |  17.5\n",
      "alternatively -> alternative  |  alternative\n",
      "fourparameter -> parameter  |  parameter\n",
      "suspend -> resuspend  |  resuspend\n",
      "fourparameter -> parameter  |  parameter\n",
      "0.99995 -> 0.9996  |  0.9996\n",
      "0.99995 -> 0.9996  |  \u001b[93m0.995\u001b[0m\n",
      "0.99998 -> 0.9992  |  \u001b[93m0.9996\u001b[0m\n",
      "singledose -> single  |  single\n",
      "purposely -> purpose  |  purpose\n",
      "midpoint -> endpoint  |  endpoint\n",
      "termination -> determination  |  determination\n",
      "3.1.1 -> 2.1.1.1  |  2.1.1.1\n",
      "regular -> regulatory  |  regulatory\n",
      "0.13721 -> 0.1315  |  \u001b[93m0.132\u001b[0m\n",
      "0.13664 -> 0.1315  |  \u001b[93m0.0788\u001b[0m\n",
      "0.99825 -> 0.998  |  0.998\n",
      "0.99916 -> 0.9996  |  0.9996\n",
      "enzymelinked -> enzyme  |  \u001b[93mlinked\u001b[0m\n",
      "displaying -> display  |  display\n",
      "laborat -> laboratory  |  laboratory\n",
      "213875 -> k1387  |  \u001b[93mk1386\u001b[0m\n",
      "c000006353 -> 1.00000  |  1.00000\n",
      "c000006355 -> 1.00000  |  1.00000\n",
      "emphasize -> emphasis  |  emphasis\n",
      "h9fim -> fim  |  fim\n",
      "0.9932 -> 0.993  |  0.993\n",
      "'mock -> mock  |  mock\n",
      "unabsorbed -> absorbed  |  absorbed\n",
      "21.130 -> 21.9  |  21.9\n",
      "21.130 -> 21.9  |  21.9\n",
      "ranked -> linked  |  linked\n",
      "suggesting -> suggested  |  \u001b[93msuggests\u001b[0m\n",
      "influencing -> influence  |  influence\n",
      "subjective -> subject  |  \u001b[93msubjected\u001b[0m\n",
      "scenario -> scenarios  |  scenarios\n",
      "fimbria -> fimbriae  |  fimbriae\n",
      "fimbria -> fimbriae  |  fimbriae\n",
      "18.17 -> 4.1576  |  \u001b[93m18.2644\u001b[0m\n",
      "130.36 -> 0.36  |  0.36\n",
      "l1tet -> tet  |  tet\n",
      "l8tet -> tet  |  tet\n",
      "4tet -> tet  |  tet\n",
      "b4tet -> tet  |  tet\n",
      "b0tet -> tet  |  tet\n",
      "pr0tet -> pr0  |  \u001b[93mtet\u001b[0m\n",
      "b4tet -> tet  |  tet\n",
      "pr4tet -> pr4  |  \u001b[93mtet\u001b[0m\n",
      "14.44 -> 14.01.103  |  14.01.103\n",
      "0.97007to -> 0.97  |  0.97\n",
      "0.99610 -> 0.9993  |  \u001b[93m0.995\u001b[0m\n",
      "1.16575 -> 1.16  |  \u001b[93m1.12\u001b[0m\n",
      "l4tet -> tet  |  tet\n",
      "tighter -> tightened  |  tightened\n",
      "optimize -> optimized  |  optimized\n",
      "reassigned -> assigned  |  assigned\n",
      "54.5 -> 4.5  |  4.5\n",
      "42.7 -> 2.7  |  2.7\n",
      "3fold -> fold  |  fold\n",
      "5fold -> fold  |  fold\n",
      "singledose -> single  |  single\n",
      "prevalidation -> revalidation  |  revalidation\n",
      "3.1.1 -> 2.1.1.1  |  2.1.1.1\n",
      "c000006826 -> 1.00000  |  1.00000\n",
      "c000006860 -> 1.00000  |  1.00000\n",
      "c000006865 -> 1.00000  |  1.00000\n",
      "3.2.p.3.2 -> 3.2.p.3.4  |  \u001b[93m3.2.p.3.3\u001b[0m\n",
      "c000006826 -> 1.00000  |  1.00000\n",
      "c000006860 -> 1.00000  |  1.00000\n",
      "c000006865 -> 1.00000  |  1.00000\n",
      "c000007060 -> 1.00000  |  1.00000\n",
      "c000007061 -> 1.00000  |  1.00000\n",
      "c000007329 -> 1.00000  |  1.00000\n",
      "1934 -> 34  |  \u001b[93m19\u001b[0m\n",
      "cfr -> cf  |  cf\n",
      "610.15 -> 0.15  |  0.15\n",
      "bioequivalence -> equivalence  |  equivalence\n",
      "bioequivalence -> equivalence  |  equivalence\n",
      "avoided -> avoid  |  avoid\n",
      "distributions -> distribution  |  distribution\n",
      "4.31 -> 3.31  |  3.31\n",
      "16.47 -> 16.5  |  16.5\n",
      "sensitize -> sensitized  |  sensitized\n",
      "maintains -> maintaining  |  \u001b[93mmaintained\u001b[0m\n",
      "license -> licensed  |  licensed\n",
      "anywhere -> where  |  where\n",
      "127.7 -> 7.7  |  7.7\n",
      "globally -> global  |  global\n",
      "65.000 -> 25.000  |  25.000\n",
      "launches -> launch  |  launch\n",
      "mechanisms -> organisms  |  organisms\n",
      "lipopolysaccharide -> polysaccharide  |  polysaccharide\n",
      "enhanced -> enhancement  |  enhancement\n",
      "differentiation -> differentiate  |  \u001b[93mdifferentiated\u001b[0m\n",
      "issues -> issued  |  issued\n",
      "unsafe -> safe  |  safe\n",
      "enhanced -> enhancement  |  enhancement\n",
      "investigating -> investigate  |  \u001b[93minvestigation\u001b[0m\n",
      "activating -> activation  |  activation\n",
      "unsafe -> safe  |  safe\n",
      "nonetheless -> nevertheless  |  nevertheless\n",
      "formats -> format  |  format\n",
      "detoxified -> detoxification  |  detoxification\n",
      "maintain -> maintained  |  \u001b[93mmaintaining\u001b[0m\n",
      "november -> december  |  december\n",
      "c000007225 -> 1.00000  |  1.00000\n",
      "c000003452 -> 1.00000  |  1.00000\n",
      "c000003604 -> 1.00000  |  1.00000\n",
      "c000003907 -> 1.00000  |  1.00000\n",
      "investigations -> investigation  |  investigation\n",
      "c000007225 -> 1.00000  |  1.00000\n",
      "issues -> issued  |  issued\n",
      "categorize -> category  |  category\n",
      "mitigate -> investigate  |  \u001b[93minvestigated\u001b[0m\n",
      "risks -> risk  |  risk\n",
      "inadequate -> adequate  |  adequate\n",
      "unlikely -> likely  |  likely\n",
      "investigations -> investigation  |  investigation\n",
      "cgmp -> gmp  |  gmp\n",
      "unlikely -> likely  |  likely\n",
      "issue -> issued  |  issued\n",
      "launches -> launch  |  launch\n",
      "predicting -> prediction  |  \u001b[93mpredict\u001b[0m\n",
      "avoided -> avoid  |  avoid\n",
      "osmolar -> osmolarity  |  osmolarity\n",
      "employ -> employs  |  \u001b[93memployed\u001b[0m\n",
      "sensitize -> sensitized  |  sensitized\n",
      "13.0 -> 13.7  |  \u001b[93m23.0\u001b[0m\n",
      "fifth -> fifteen  |  fifteen\n",
      "diphosphate -> phosphate  |  phosphate\n",
      "fifth -> fifteen  |  fifteen\n",
      "dihydrochloride -> hydrochloride  |  hydrochloride\n",
      "subunit -> unit  |  unit\n",
      "2.2.1.7 -> 2.2.1  |  2.2.1\n",
      "41.3270 -> 41.7  |  41.7\n",
      "61.3270 -> 23.6  |  \u001b[93m61.8\u001b[0m\n",
      "0.6552 -> 0.65  |  0.65\n",
      "0.9882 -> 0.98  |  \u001b[93m0.984\u001b[0m\n",
      "0.0335 -> 0.031  |  0.031\n",
      "0.6552 -> 0.65  |  0.65\n",
      "0.9882 -> 0.98  |  \u001b[93m0.984\u001b[0m\n",
      "0.0335 -> 0.031  |  \u001b[93m0.03\u001b[0m\n",
      "0.9984 -> 0.998  |  0.998\n",
      "74.6 -> 4.6  |  4.6\n",
      "0.000038 -> 0.0001  |  0.0001\n",
      "3.58 -> 3.5  |  3.5\n",
      "comprises -> comprised  |  comprised\n",
      "91.83 -> 0.83  |  0.83\n",
      "1.333 -> 197.333  |  \u001b[93m183.667\u001b[0m\n",
      "5.979 -> 5.9  |  5.9\n",
      "1.333 -> 197.333  |  \u001b[93m183.667\u001b[0m\n",
      "5.979 -> 5.9  |  5.9\n",
      "0.9987 -> 0.998  |  0.998\n",
      "29.2 -> 9.2  |  9.2\n",
      "27.227 -> 27.17  |  27.17\n",
      "22.728 -> 22.9  |  22.9\n",
      "24.225 -> 24.2  |  24.2\n",
      "0.0635 -> 0.063  |  0.063\n",
      "0.0258 -> 0.025  |  0.025\n",
      "0.9631 -> 0.963  |  0.963\n",
      "0.0635 -> 0.063  |  0.063\n",
      "0.0258 -> 0.025  |  0.025\n",
      "0.9631 -> 0.963  |  0.963\n",
      "1.056 -> 1.05  |  \u001b[93m\u001b[0m\n",
      "overestimated -> overestimation  |  overestimation\n",
      "102.9 -> 102.15  |  102.15\n",
      "1.056 -> 1.05  |  \u001b[93m1.03\u001b[0m\n",
      "overestimated -> overestimation  |  overestimation\n",
      "0.013 -> 0.014  |  0.014\n",
      "0.040 -> 0.043  |  \u001b[93m0.021\u001b[0m\n",
      "0.029 -> 0.021  |  \u001b[93m0.024\u001b[0m\n",
      "98.8 -> 98.7  |  98.7\n",
      "97.3 -> 197.333  |  197.333\n",
      "102.0 -> 102.15  |  102.15\n",
      "96.6 -> 6.6  |  6.6\n",
      "102.1 -> 102.15  |  102.15\n",
      "0.9884 -> 0.98  |  \u001b[93m0.980\u001b[0m\n",
      "13.732 -> 13.7  |  13.7\n",
      "22.955 -> 22.9  |  \u001b[93m61.8\u001b[0m\n",
      "0.115 -> 0.113  |  0.113\n",
      "0.9777 -> 0.97  |  0.97\n",
      "13.207 -> 13.7  |  \u001b[93m13.2\u001b[0m\n",
      "0.112 -> 0.113  |  0.113\n",
      "0.9822 -> 0.98  |  \u001b[93m0.980\u001b[0m\n",
      "13.292 -> 13.2  |  13.2\n",
      "great -> greater  |  greater\n",
      "diafiltration -> filtration  |  filtration\n",
      "3.2.s.2.5 -> 3.2.s.2.4  |  3.2.s.2.4\n",
      "nitrosulphuric -> sulphuric  |  \u001b[93mnitrosulfuric\u001b[0m\n",
      "vapours -> vapour  |  vapour\n",
      "digested -> digest  |  digest\n",
      "subunit -> unit  |  unit\n",
      "2.2.2.7 -> 2.2.7  |  \u001b[93m2.2.29\u001b[0m\n",
      "37.15 -> 7.15  |  7.15\n",
      "18.58 -> 18.2644  |  \u001b[93m4.1576\u001b[0m\n",
      "47.15 -> 7.15  |  7.15\n",
      "57.15 -> 7.15  |  7.15\n",
      "67.15 -> 7.15  |  7.15\n",
      "77.15 -> 7.15  |  7.15\n",
      "0.037 -> 0.03  |  0.03\n",
      "1.216 -> 1.21  |  1.21\n",
      "0.989 -> 0.98  |  \u001b[93m0.984\u001b[0m\n",
      "0.022 -> 0.027  |  \u001b[93m0.021\u001b[0m\n",
      "0.9991 -> 0.9993  |  0.9993\n",
      "79.0 -> 9.0  |  9.0\n",
      "2.82 -> 1.82  |  1.82\n",
      "0.000038 -> 0.0001  |  0.0001\n",
      "79.0 -> 9.0  |  9.0\n",
      "100.8 -> 100.3  |  100.3\n",
      "1.9 -> 2731.9  |  2731.9\n",
      "0.9997 -> 0.9992  |  \u001b[93m0.9993\u001b[0m\n",
      "1.108 -> 1.10  |  \u001b[93m1.104\u001b[0m\n",
      "0.260 -> 0.2698  |  0.2698\n",
      "0.931 -> 0.93  |  0.93\n",
      "0.9965 -> 0.9974  |  \u001b[93m0.9993\u001b[0m\n",
      "lineregression -> regression  |  regression\n",
      "1.72 -> 1.7  |  1.7\n",
      "2.58 -> 2.5.33  |  2.5.33\n",
      "1.72 -> 1.7  |  1.7\n",
      "98.67 -> 98.7  |  \u001b[93m98.5\u001b[0m\n",
      "296.00 -> 6.00  |  6.00\n",
      "6.761 -> 6.7  |  6.7\n",
      "0.034 -> 0.03  |  0.03\n",
      "0.9985 -> 0.998  |  0.998\n",
      "lineaccuracy -> accuracy  |  accuracy\n",
      "analyse -> analysed  |  \u001b[93manalyses\u001b[0m\n",
      "5.8 -> 5.844  |  5.844\n",
      "33.576 -> 4.1576  |  4.1576\n",
      "0.0595 -> 0.05  |  0.05\n",
      "0.0489 -> 0.048  |  \u001b[93m0.032\u001b[0m\n",
      "0.9676 -> 0.967  |  \u001b[93m0.984\u001b[0m\n",
      "0.0361 -> 0.03  |  \u001b[93m0.031\u001b[0m\n",
      "0.9970 -> 0.997  |  \u001b[93m0.9974\u001b[0m\n",
      "176.7 -> 6.7  |  6.7\n",
      "176.7 -> 6.7  |  6.7\n",
      "106.5 -> 6.5  |  \u001b[93m36.5\u001b[0m\n",
      "0.9884 -> 0.98  |  \u001b[93m0.980\u001b[0m\n",
      "13.732 -> 13.7  |  13.7\n",
      "22.955 -> 22.9  |  \u001b[93m61.8\u001b[0m\n",
      "0.115 -> 0.113  |  0.113\n",
      "0.9777 -> 0.97  |  0.97\n",
      "13.207 -> 13.7  |  \u001b[93m13.2\u001b[0m\n",
      "0.112 -> 0.113  |  0.113\n",
      "0.9822 -> 0.98  |  \u001b[93m0.980\u001b[0m\n",
      "13.292 -> 13.2  |  13.2\n",
      "58 -> 358  |  358\n",
      "61 -> 61.8  |  \u001b[93m22.9\u001b[0m\n",
      "63 -> 0.563  |  0.563\n",
      "69 -> 0169  |  \u001b[93m1160\u001b[0m\n",
      "0.186 -> 0.18  |  0.18\n",
      "0.653 -> 0.65  |  0.65\n",
      "0.152 -> 0.15  |  0.15\n",
      "0.8694 -> 0.86  |  0.86\n",
      "0.9771 -> 0.97  |  0.97\n",
      "0.077 -> 0.07  |  \u001b[93m0.072\u001b[0m\n",
      "0.286 -> 0.285  |  0.285\n",
      "1.070 -> 1.07  |  \u001b[93m1.05\u001b[0m\n",
      "0.223 -> 0.22  |  0.22\n",
      "13.9 -> 13.7  |  \u001b[93m13.2\u001b[0m\n",
      "22.8 -> 52.8  |  52.8\n",
      "76 -> 4.1576  |  4.1576\n",
      "79 -> 919.1079  |  919.1079\n",
      "13.9 -> 13.7  |  \u001b[93m13.2\u001b[0m\n",
      "22.8 -> 52.8  |  52.8\n",
      "great -> greater  |  greater\n",
      "diafiltration -> filtration  |  filtration\n",
      "3.2.s.2.5 -> 3.2.s.2.4  |  3.2.s.2.4\n",
      "impossibility -> possibility  |  possibility\n",
      "pyridylzonaphthol -> pyridylazonaphthol  |  pyridylazonaphthol\n",
      "pyridylzonaphthol -> pyridylazonaphthol  |  pyridylazonaphthol\n",
      "2.2' -> 2.2.7  |  \u001b[93m2.2.3\u001b[0m\n",
      "manipulation -> population  |  population\n",
      "computed -> computer  |  computer\n",
      "shaped -> shape  |  shape\n",
      "shaped -> shape  |  shape\n",
      "3.2.p.6 -> 3.2.p.2.2  |  \u001b[93m3.2.p.8.3\u001b[0m\n",
      "100 -> 100  |  \u001b[93m100\u001b[0m\n",
      "supernanta -> supernatant  |  supernatant\n",
      "4000rpm -> rpm  |  rpm\n",
      "6400 -> 100  |  100\n",
      "100 -> 100  |  \u001b[93m100\u001b[0m\n",
      "emitted -> submitted  |  submitted\n",
      "array -> arrange  |  arrange\n",
      "wetting -> setting  |  setting\n",
      "launched -> launch  |  launch\n",
      "ultrafiltrated -> ultrafiltered  |  ultrafiltered\n",
      "pulse -> pulsed  |  pulsed\n",
      "comprising -> comprised  |  comprised\n",
      "5.1.1 -> 5.1.10  |  5.1.10\n",
      "operation -> operations  |  operations\n",
      "dilution -> dilution  |  dilution\n",
      "dilution -> dilution  |  dilution\n",
      "nonetheless -> nevertheless  |  nevertheless\n",
      "evidenced -> evidence  |  evidence\n",
      "evidenced -> evidence  |  evidence\n",
      "1.67 -> 1.6  |  1.6\n",
      "male -> female  |  female\n",
      "distribute -> distributed  |  distributed\n",
      "allocated -> located  |  located\n",
      "filamenteous -> filamentous  |  filamentous\n",
      "acknowledgement -> knowledge  |  knowledge\n",
      "acknowledgement -> knowledge  |  knowledge\n",
      "handled -> handling  |  handling\n",
      "handled -> handling  |  handling\n",
      "precisions -> precision  |  precision\n",
      "chose -> chosen  |  chosen\n",
      "bb0 -> bbo09  |  bbo09\n",
      "10.103 -> 0.103  |  \u001b[93m0.143\u001b[0m\n",
      "13.6 -> 23.6  |  \u001b[93m13.2\u001b[0m\n",
      "solution -> solution  |  solution\n",
      "119.1 -> 919.1079  |  919.1079\n",
      "28.6 -> 28.759.6  |  \u001b[93m61.8\u001b[0m\n",
      "55.10 -> 5.10  |  5.10\n",
      "1.130 -> 1.16  |  \u001b[93m1.104\u001b[0m\n",
      "13.49 -> 13.7  |  13.7\n",
      "1.695 -> 1.6  |  1.6\n",
      "119.1 -> 919.1079  |  919.1079\n",
      "28.6 -> 28.759.6  |  \u001b[93m61.8\u001b[0m\n",
      "49.6 -> 9.6  |  9.6\n",
      "vaccine -> vaccinea  |  vaccinea\n",
      "l80.27 -> 180.27  |  180.27\n",
      "300 -> 300sb  |  300sb\n",
      "180.31 -> 180.27  |  180.27\n",
      "171.12 -> 1.12  |  1.12\n",
      "vaccine -> vaccinea  |  vaccinea\n",
      "55.9 -> 5.9  |  5.9\n",
      "2.2.1.1 -> 2.2.1  |  2.2.1\n",
      "2.2.2.1 -> 2.2.1  |  2.2.1\n",
      "0.649 -> 0.64  |  0.64\n",
      "55.9 -> 5.9  |  5.9\n",
      "0.104 -> 0.103  |  \u001b[93m1.009\u001b[0m\n",
      "31.6 -> 1.6  |  1.6\n",
      "0.094 -> 0.09  |  \u001b[93m0.0931\u001b[0m\n",
      "0.469 -> 0.46  |  0.46\n",
      "0.134 -> 0.1315  |  \u001b[93m0.132\u001b[0m\n",
      "88.4 -> 8.4  |  8.4\n",
      "1.211.75 -> 1.21  |  1.21\n",
      "1.21 -> 1.21  |  1.21\n",
      "2.28 -> 2.2.28  |  2.2.28\n",
      "domain -> main  |  main\n",
      "representatives -> representative  |  representative\n",
      "representatives -> representative  |  representative\n",
      "ml -> ml  |  ml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.1 -> 8.1  |  8.1\n",
      "ameboecyte -> amebocyte  |  \u001b[93mlimulus\u001b[0m\n",
      "allowable -> allow  |  allow\n",
      "describe -> described  |  described\n",
      "hexaxim -> hexaxim  |  hexaxim\n",
      "concentration -> concentrat  |  concentrat\n",
      "concentration -> concentrat  |  concentrat\n",
      "0.26 -> 0.2698  |  0.2698\n",
      "0.105 -> 0.100  |  0.100\n",
      "processor -> process  |  process\n",
      "classify -> classified  |  classified\n",
      "classification -> classified  |  classified\n",
      "aemophilus -> haemophilus  |  haemophilus\n",
      "hexaxim -> hexaxim  |  hexaxim\n",
      "2.2.1.3 -> 2.2.1  |  2.2.1\n",
      "71.2 -> 1.2  |  1.2\n",
      "2.6.2 -> 2.6.9  |  \u001b[93m2.6.1\u001b[0m\n",
      "71.2 -> 1.2  |  1.2\n",
      "2.4.1.2 -> 2.4.18  |  2.4.18\n",
      "2.4.1.2 -> 2.4.18  |  2.4.18\n",
      "1.7861 -> 1.7  |  1.7\n",
      "bioanalytical -> analytical  |  analytical\n",
      "ewp -> dtwp  |  dtwp\n",
      "haemaglutinin -> haemagglutinin  |  haemagglutinin\n",
      "haemaglutinin -> haemagglutinin  |  haemagglutinin\n",
      "constraints -> constrained  |  constrained\n",
      "constraints -> constrained  |  constrained\n",
      "experimentally -> experimental  |  \u001b[93mexperiment\u001b[0m\n",
      "ctpa -> ctp  |  ctp\n",
      "12000 -> 2000  |  2000\n",
      "haemagglutinnin -> haemagglutinin  |  haemagglutinin\n",
      "competes -> compete  |  compete\n",
      "2.2.2.1 -> 2.2.1  |  2.2.1\n",
      "experimentally -> experimental  |  \u001b[93mexperiment\u001b[0m\n",
      "2.2.2 -> 2.2.23  |  \u001b[93m2.2.29\u001b[0m\n",
      "842.0 -> 2.0  |  2.0\n",
      "seruma -> serum  |  serum\n",
      "174.2 -> 24.2  |  24.2\n",
      "213.8 -> 3.8  |  3.8\n",
      "equivalently -> equivalent  |  equivalent\n",
      "equivalently -> equivalent  |  equivalent\n",
      "unquantifiable -> quantifiable  |  quantifiable\n",
      "322.8 -> 52.8  |  52.8\n",
      "seruma -> serum  |  serum\n",
      "673.5 -> 3.5  |  3.5\n",
      "0.076 -> 0.07  |  0.07\n",
      "lognormal -> normal  |  normal\n",
      "5series -> series  |  series\n",
      "1026.602 -> 102.15  |  102.15\n",
      "haemaglutinnin -> haemagglutinin  |  haemagglutinin\n",
      "2.3.11 -> 2.3.1  |  2.3.1\n",
      "0.535 -> 0.5396  |  0.5396\n",
      "1026.602 -> 102.15  |  102.15\n",
      "filamenteous -> filamentous  |  filamentous\n",
      "haemaglutinnin -> haemagglutinin  |  haemagglutinin\n",
      "0.807 -> 0.80  |  0.80\n",
      "atypic -> atypical  |  atypical\n",
      "0.953 -> 0.95  |  0.95\n",
      "922.686 -> 922.7  |  \u001b[93m1462.5\u001b[0m\n",
      "0.076 -> 0.07  |  \u001b[93m0.0788\u001b[0m\n",
      "2900.340 -> 2900.3  |  2900.3\n",
      "0.535 -> 0.5396  |  0.5396\n",
      "0.705 -> 0.70  |  0.70\n",
      "1462.534 -> 1462.5  |  \u001b[93m2900.3\u001b[0m\n",
      "0.807 -> 0.80  |  0.80\n",
      "0.128 -> 0.1295  |  \u001b[93m0.125\u001b[0m\n",
      "2731.862 -> 2731.9  |  \u001b[93m2900.3\u001b[0m\n",
      "1e57 -> 1e54  |  \u001b[93m2a15\u001b[0m\n",
      "2a72 -> 1f39  |  \u001b[93m2a15\u001b[0m\n",
      "0.9973 -> 0.997  |  \u001b[93m0.984\u001b[0m\n",
      "prevents -> prevent  |  prevent\n",
      "authorised -> authority  |  authority\n",
      "arbitrary -> barbital  |  barbital\n",
      "degrade -> degraded  |  degraded\n",
      "structure -> instruction  |  instruction\n",
      "2.1.12.1 -> 2.1.1.1  |  2.1.1.1\n",
      "waterbath -> water  |  water\n",
      "structure -> instruction  |  instruction\n",
      "doubt -> double  |  double\n",
      "0.67 -> 0.63  |  \u001b[93m0.68\u001b[0m\n",
      "0.34 -> 0.36  |  0.36\n",
      "discussion -> discussed  |  discussed\n",
      "irrespective -> respective  |  respective\n",
      "unlike -> like  |  like\n",
      "104& -> 104.6  |  104.6\n",
      "104& -> 104.6  |  104.6\n",
      "discussion -> discussed  |  discussed\n",
      "decreases -> decrease  |  decrease\n",
      "0.9973 -> 0.997  |  0.997\n",
      "1.41 -> 1.4  |  1.4\n",
      "surrounds -> surrounding  |  surrounding\n",
      "ingredients -> gradients  |  gradients\n",
      "concurrently -> currently  |  currently\n",
      "impossibility -> possibility  |  possibility\n",
      "attribute -> attributes  |  attributes\n",
      "attribute -> attributes  |  \u001b[93mattributed\u001b[0m\n",
      "subunits -> units  |  units\n",
      "nanometer -> chronometer  |  chronometer\n",
      "catalogue -> catalyst  |  \u001b[93mcatalyzes\u001b[0m\n",
      "stoppered -> stopper  |  stopper\n",
      "involve -> involves  |  involves\n",
      "abtsa -> abts  |  abts\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-590ba5764c7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mword\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mcorrections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextual_corrections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpred2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mpred1\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mpred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\033[93m'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpred2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\033[0m'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'->'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' | '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(2000) : \n",
    "    word  = corrections[i][0]\n",
    "    pred1 = corrections[i][1]\n",
    "    pred2 = contextual_corrections[i][1]\n",
    "    if pred2 != pred1 : pred2 = '\\033[93m' + pred2 + '\\033[0m'\n",
    "    print(word, '->', pred1, ' | ', pred2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
