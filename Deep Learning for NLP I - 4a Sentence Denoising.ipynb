{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part I - 4a <br><br><br>\n",
    "  Sentence Denoising\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=red>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "We consider as Sequence labelling task a **Sentence Denoising** problem, which consists in transforming a noisy sequence of words into a correctly formed sentence.<br> Training follows a denoising objective known as _Cloze task_ , which is used :\n",
    "\n",
    "- For the BERT model in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\libDL4NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importWords(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        words = ['.']\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            #print(pair)\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                words += cleanSentence(current_tile) + ['.']\n",
    "                title  = current_tile\n",
    "            words += cleanSentence(str(pair[1]).split(' | ')[-1]) + ['.']\n",
    "        return words\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    words = cleanDatabase(db)\n",
    "    return words\n",
    "\n",
    "\n",
    "def importAllWords(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus.append(importWords(file_name))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def importSentences(file_name) :\n",
    "    def cleanDatabase(db):\n",
    "        sentences = []\n",
    "        title = ''\n",
    "        for pair in db :\n",
    "            current_tile = pair[0].split(' | ')[-1]\n",
    "            if current_tile != title :\n",
    "                sentences.append(cleanSentence(current_tile))\n",
    "                title = current_tile\n",
    "            sentences.append(cleanSentence(str(pair[1]).split(' | ')[-1]))\n",
    "        return sentences\n",
    "\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    db = [el[:2] for el in db if el[-1] in [0,1, 10]]\n",
    "    sentences = cleanDatabase(db)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def importAllSentences(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus += importSentences(file_name)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = importAllWords(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31574"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = importAllSentences(path_to_NLP + '\\\\data\\\\AMM')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'test',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'assessing',\n",
       " 'the',\n",
       " 'dissolution',\n",
       " 'time',\n",
       " 'of',\n",
       " 'the',\n",
       " 'freeze',\n",
       " 'dried',\n",
       " 'yellow',\n",
       " 'fever',\n",
       " 'vaccine',\n",
       " 'after',\n",
       " 'adding',\n",
       " 'the',\n",
       " 'suitable',\n",
       " 'diluent',\n",
       " 'directly',\n",
       " 'into',\n",
       " 'the',\n",
       " 'original',\n",
       " 'container']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = FastText(size = 75, \n",
    "                    window = 5, \n",
    "                    min_count = 3, \n",
    "                    negative = 20,\n",
    "                    sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4662"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.train(sentences = sentences, \n",
    "               epochs = 50,\n",
    "               total_examples = word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentence denoising Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDenoiser(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SentenceDenoiser, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, lang_size)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length, lang_size)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        new_ws = [self.word2vec.lang.index2word[ind] for ind in inds]\n",
    "        print(' '.join([addColor(w1, w2, color) for w1, w2 in zip(words, new_ws)]))\n",
    "        return\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0.15,\n",
    "                                max_sentence_length = 50,\n",
    "                                tol = 10,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index\n",
    "        \n",
    "        def splitLongs(words, threshold = 50, tol = 10):\n",
    "            news = []\n",
    "            for i in range(0, len(words), threshold) :\n",
    "                if len(words)-i-threshold > tol : \n",
    "                    news.append(words[i : i + threshold])\n",
    "                else : \n",
    "                    news.append(words[i:])\n",
    "                    break\n",
    "            return news\n",
    "        \n",
    "        packed_data = []\n",
    "        random.seed(seed)\n",
    "        # prepare sentences\n",
    "        #sentences = [self.tokenizer(s) for s in sentences]\n",
    "        sentences = [[self.word2vec.lang.getIndex(w) for w in s] for s in sentences]\n",
    "        sentences = [[w for w in words if w is not None] for words in sentences]\n",
    "        sentences = [s for S in sentences for s in splitLongs(S, max_sentence_length, tol) if len([w for w in s if w != self.word2vec.lang.getIndex('UNK')]) > 1]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        # collect packs\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            # prepare mask\n",
    "            mask_xl = [[i for i, w in enumerate(p) if w != self.word2vec.lang.getIndex('UNK')] for p in pack]\n",
    "            mask_xs = [random.sample(m, k = int(mask_ratio*len(m) +1)) for m in mask_xl]\n",
    "            # prepare input and target packs\n",
    "            pack0    = [[ maskInput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask_xs)]\n",
    "            pack1_xs = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask_xs)]\n",
    "            pack1_xl = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack, mask_xl)]\n",
    "            lengths  = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            # padd\n",
    "            pack0    = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index)) \n",
    "            pack1_xs = list(itertools.zip_longest(*pack1_xs, fillvalue = self.ignore_index))\n",
    "            pack1_xl = list(itertools.zip_longest(*pack1_xl, fillvalue = self.ignore_index))\n",
    "            # turn into torch variables\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length)\n",
    "            pack1_xs = Variable(torch.LongTensor(pack1_xs).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            pack1_xl = Variable(torch.LongTensor(pack1_xl).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            # store pack\n",
    "            packed_data.append([[pack0, lengths], [pack1_xs, pack1_xl]])\n",
    "        return packed_data\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, \n",
    "            iters = None, epochs = None, lr = 0.025, unmasked_ratio = 0,\n",
    "            random_state = 42, print_every = 10, compute_accuracy = 'xs'):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index)\n",
    "            success = sum([self.ignore_index != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, unmasked_ratio, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs  = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets_xs = batch[1][0].to(self.device)               # dim = (batch_size, input_length)\n",
    "            targets_xl = batch[1][1].to(self.device)               # dim = (batch_size, input_length)\n",
    "            loss       = (1-unmasked_ratio)*self.criterion(log_probs, targets_xs) \\\n",
    "                       + unmasked_ratio    *self.criterion(log_probs, targets_xl)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            if compute_accuracy == 'xs': \n",
    "                accuracy = computeAccuracy(log_probs, targets_xs)\n",
    "                error = float(loss.item() / np.sum(targets_xs.data.cpu().numpy() != self.ignore_index))\n",
    "            elif compute_accuracy == 'xl': \n",
    "                accuracy = computeAccuracy(log_probs, targets_xl)\n",
    "                error = float(loss.item() / np.sum(targets_xl.data.cpu().numpy() != self.ignore_index))\n",
    "            else : \n",
    "                accuracy = 0\n",
    "                error = float(loss.item() / np.sum(targets_xs.data.cpu().numpy() != self.ignore_index))\n",
    "            return error, accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, unmasked_ratio, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer,unmasked_ratio, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627164"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoiser = SentenceDenoiser(device = torch.device('cpu'), # device\n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = Word2VecConnector(word2vec),\n",
    "                            hidden_dim = 75, \n",
    "                            n_layers = 3, \n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.AdamW)\n",
    "\n",
    "denoiser.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n",
      "9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for seed in [42, 854, 7956, 657, 124] :\n",
    "    batches += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 17s (- 25m 59s) (100 1%) loss : 11.134  accuracy : 5.8 %\n",
      "0m 35s (- 26m 34s) (200 2%) loss : 9.966  accuracy : 7.4 %\n",
      "0m 53s (- 26m 27s) (300 3%) loss : 9.677  accuracy : 8.0 %\n",
      "1m 13s (- 26m 45s) (400 4%) loss : 9.188  accuracy : 11.2 %\n",
      "1m 32s (- 26m 35s) (500 5%) loss : 8.802  accuracy : 11.6 %\n",
      "1m 50s (- 26m 11s) (600 6%) loss : 8.106  accuracy : 14.9 %\n",
      "2m 7s (- 25m 38s) (700 7%) loss : 7.740  accuracy : 16.3 %\n",
      "2m 25s (- 25m 22s) (800 8%) loss : 7.347  accuracy : 18.2 %\n",
      "2m 43s (- 25m 0s) (900 9%) loss : 7.091  accuracy : 17.6 %\n",
      "3m 2s (- 24m 47s) (1000 10%) loss : 6.870  accuracy : 19.8 %\n",
      "3m 20s (- 24m 29s) (1100 12%) loss : 6.764  accuracy : 18.3 %\n",
      "3m 37s (- 24m 2s) (1200 13%) loss : 6.668  accuracy : 21.2 %\n",
      "3m 57s (- 23m 55s) (1300 14%) loss : 6.355  accuracy : 22.7 %\n",
      "4m 16s (- 23m 37s) (1400 15%) loss : 6.060  accuracy : 22.4 %\n",
      "4m 31s (- 23m 5s) (1500 16%) loss : 5.973  accuracy : 22.5 %\n",
      "4m 48s (- 22m 40s) (1600 17%) loss : 6.077  accuracy : 23.0 %\n",
      "5m 6s (- 22m 22s) (1700 18%) loss : 5.874  accuracy : 23.3 %\n",
      "5m 25s (- 22m 10s) (1800 19%) loss : 5.801  accuracy : 24.3 %\n",
      "5m 43s (- 21m 51s) (1900 20%) loss : 5.710  accuracy : 24.8 %\n",
      "6m 2s (- 21m 36s) (2000 21%) loss : 5.675  accuracy : 23.4 %\n",
      "6m 25s (- 21m 33s) (2100 22%) loss : 5.579  accuracy : 24.2 %\n",
      "6m 42s (- 21m 11s) (2200 24%) loss : 5.468  accuracy : 25.0 %\n",
      "7m 0s (- 20m 51s) (2300 25%) loss : 5.516  accuracy : 24.5 %\n",
      "7m 16s (- 20m 28s) (2400 26%) loss : 5.270  accuracy : 26.7 %\n",
      "7m 35s (- 20m 12s) (2500 27%) loss : 5.384  accuracy : 26.2 %\n",
      "7m 55s (- 19m 58s) (2600 28%) loss : 5.231  accuracy : 28.2 %\n",
      "8m 12s (- 19m 36s) (2700 29%) loss : 4.984  accuracy : 29.9 %\n",
      "8m 32s (- 19m 22s) (2800 30%) loss : 5.061  accuracy : 28.5 %\n",
      "8m 51s (- 19m 5s) (2900 31%) loss : 5.022  accuracy : 27.8 %\n",
      "9m 9s (- 18m 47s) (3000 32%) loss : 5.056  accuracy : 28.2 %\n",
      "9m 29s (- 18m 32s) (3100 33%) loss : 5.030  accuracy : 29.2 %\n",
      "9m 49s (- 18m 15s) (3200 34%) loss : 4.802  accuracy : 31.1 %\n",
      "10m 9s (- 18m 0s) (3300 36%) loss : 4.784  accuracy : 31.6 %\n",
      "10m 29s (- 17m 44s) (3400 37%) loss : 4.862  accuracy : 30.7 %\n",
      "10m 48s (- 17m 26s) (3500 38%) loss : 4.670  accuracy : 31.4 %\n",
      "11m 10s (- 17m 13s) (3600 39%) loss : 4.588  accuracy : 32.3 %\n",
      "11m 33s (- 17m 1s) (3700 40%) loss : 4.758  accuracy : 30.8 %\n",
      "11m 56s (- 16m 48s) (3800 41%) loss : 4.765  accuracy : 29.1 %\n",
      "12m 22s (- 16m 39s) (3900 42%) loss : 4.708  accuracy : 29.2 %\n",
      "12m 44s (- 16m 23s) (4000 43%) loss : 4.533  accuracy : 32.0 %\n",
      "13m 4s (- 16m 6s) (4100 44%) loss : 4.557  accuracy : 32.5 %\n",
      "13m 24s (- 15m 47s) (4200 45%) loss : 4.336  accuracy : 33.7 %\n",
      "13m 41s (- 15m 26s) (4300 46%) loss : 4.605  accuracy : 31.1 %\n",
      "14m 0s (- 15m 7s) (4400 48%) loss : 4.612  accuracy : 30.8 %\n",
      "14m 20s (- 14m 49s) (4500 49%) loss : 4.450  accuracy : 32.1 %\n",
      "14m 40s (- 14m 30s) (4600 50%) loss : 4.534  accuracy : 32.2 %\n",
      "15m 2s (- 14m 14s) (4700 51%) loss : 4.635  accuracy : 29.5 %\n",
      "15m 21s (- 13m 55s) (4800 52%) loss : 4.445  accuracy : 32.3 %\n",
      "15m 39s (- 13m 35s) (4900 53%) loss : 4.458  accuracy : 31.9 %\n",
      "15m 59s (- 13m 16s) (5000 54%) loss : 4.398  accuracy : 32.2 %\n",
      "16m 17s (- 12m 56s) (5100 55%) loss : 4.205  accuracy : 34.1 %\n",
      "16m 34s (- 12m 35s) (5200 56%) loss : 4.232  accuracy : 34.3 %\n",
      "16m 54s (- 12m 16s) (5300 57%) loss : 4.333  accuracy : 33.1 %\n",
      "17m 16s (- 11m 59s) (5400 59%) loss : 4.524  accuracy : 30.8 %\n",
      "17m 37s (- 11m 41s) (5500 60%) loss : 4.251  accuracy : 33.2 %\n",
      "17m 55s (- 11m 22s) (5600 61%) loss : 4.228  accuracy : 34.0 %\n",
      "18m 14s (- 11m 2s) (5700 62%) loss : 4.025  accuracy : 35.9 %\n",
      "18m 35s (- 10m 44s) (5800 63%) loss : 4.234  accuracy : 33.3 %\n",
      "18m 53s (- 10m 24s) (5900 64%) loss : 4.024  accuracy : 36.5 %\n",
      "19m 10s (- 10m 4s) (6000 65%) loss : 4.054  accuracy : 34.8 %\n",
      "19m 31s (- 9m 45s) (6100 66%) loss : 4.187  accuracy : 33.5 %\n",
      "19m 50s (- 9m 26s) (6200 67%) loss : 4.085  accuracy : 34.9 %\n",
      "20m 9s (- 9m 6s) (6300 68%) loss : 3.954  accuracy : 37.1 %\n",
      "20m 28s (- 8m 48s) (6400 69%) loss : 4.106  accuracy : 34.4 %\n",
      "20m 47s (- 8m 28s) (6500 71%) loss : 4.030  accuracy : 34.6 %\n",
      "21m 5s (- 8m 8s) (6600 72%) loss : 4.203  accuracy : 32.4 %\n",
      "21m 23s (- 7m 49s) (6700 73%) loss : 4.019  accuracy : 36.4 %\n",
      "21m 42s (- 7m 30s) (6800 74%) loss : 4.096  accuracy : 34.9 %\n",
      "22m 0s (- 7m 10s) (6900 75%) loss : 3.852  accuracy : 37.4 %\n",
      "22m 19s (- 6m 51s) (7000 76%) loss : 4.028  accuracy : 35.3 %\n",
      "22m 38s (- 6m 32s) (7100 77%) loss : 3.888  accuracy : 37.6 %\n",
      "22m 59s (- 6m 13s) (7200 78%) loss : 4.005  accuracy : 34.5 %\n",
      "23m 17s (- 5m 54s) (7300 79%) loss : 3.840  accuracy : 37.7 %\n",
      "23m 33s (- 5m 34s) (7400 80%) loss : 3.999  accuracy : 35.3 %\n",
      "23m 52s (- 5m 15s) (7500 81%) loss : 3.931  accuracy : 34.7 %\n",
      "24m 11s (- 4m 55s) (7600 83%) loss : 3.917  accuracy : 36.5 %\n",
      "24m 31s (- 4m 37s) (7700 84%) loss : 3.948  accuracy : 36.4 %\n",
      "24m 50s (- 4m 18s) (7800 85%) loss : 3.873  accuracy : 36.9 %\n",
      "25m 10s (- 3m 58s) (7900 86%) loss : 3.819  accuracy : 36.5 %\n",
      "25m 31s (- 3m 40s) (8000 87%) loss : 3.982  accuracy : 35.2 %\n",
      "25m 53s (- 3m 21s) (8100 88%) loss : 3.964  accuracy : 34.8 %\n",
      "26m 14s (- 3m 2s) (8200 89%) loss : 3.865  accuracy : 36.5 %\n",
      "26m 36s (- 2m 43s) (8300 90%) loss : 3.928  accuracy : 36.1 %\n",
      "26m 57s (- 2m 24s) (8400 91%) loss : 3.962  accuracy : 34.4 %\n",
      "27m 15s (- 2m 5s) (8500 92%) loss : 3.803  accuracy : 36.2 %\n",
      "27m 35s (- 1m 45s) (8600 93%) loss : 3.706  accuracy : 38.0 %\n",
      "27m 55s (- 1m 26s) (8700 95%) loss : 3.850  accuracy : 37.2 %\n",
      "28m 16s (- 1m 7s) (8800 96%) loss : 3.741  accuracy : 38.4 %\n",
      "28m 45s (- 0m 48s) (8900 97%) loss : 3.913  accuracy : 34.7 %\n",
      "29m 6s (- 0m 29s) (9000 98%) loss : 3.773  accuracy : 39.4 %\n",
      "29m 24s (- 0m 9s) (9100 99%) loss : 3.656  accuracy : 39.2 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches, epochs = 1, lr = 0.001, unmasked_ratio = 0.15, compute_accuracy = 'xs', print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n",
      "3660\n",
      "5490\n",
      "7320\n",
      "9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches2 = []\n",
    "for seed in [777, 5821, 12, 693, 45824] :\n",
    "    batches2 += denoiser.generatePackedSentences(sentences, \n",
    "                                                batch_size = 16,\n",
    "                                                mask_ratio = 0.15,\n",
    "                                                max_sentence_length = 50,\n",
    "                                                tol = 10,\n",
    "                                                seed = seed)\n",
    "    print(len(batches2))\n",
    "len(batches2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 18s (- 28m 10s) (100 1%) loss : 3.760  accuracy : 36.8 %\n",
      "0m 34s (- 25m 55s) (200 2%) loss : 3.601  accuracy : 39.3 %\n",
      "0m 52s (- 25m 58s) (300 3%) loss : 3.774  accuracy : 37.7 %\n",
      "1m 9s (- 25m 27s) (400 4%) loss : 3.690  accuracy : 39.0 %\n",
      "1m 28s (- 25m 37s) (500 5%) loss : 3.741  accuracy : 37.8 %\n",
      "1m 45s (- 25m 2s) (600 6%) loss : 3.710  accuracy : 37.3 %\n",
      "2m 3s (- 24m 52s) (700 7%) loss : 3.690  accuracy : 37.7 %\n",
      "2m 21s (- 24m 38s) (800 8%) loss : 3.532  accuracy : 40.4 %\n",
      "2m 40s (- 24m 35s) (900 9%) loss : 3.757  accuracy : 37.6 %\n",
      "2m 58s (- 24m 12s) (1000 10%) loss : 3.556  accuracy : 39.9 %\n",
      "3m 17s (- 24m 4s) (1100 12%) loss : 3.704  accuracy : 38.4 %\n",
      "3m 33s (- 23m 35s) (1200 13%) loss : 3.672  accuracy : 39.0 %\n",
      "3m 51s (- 23m 18s) (1300 14%) loss : 3.746  accuracy : 37.0 %\n",
      "4m 9s (- 23m 1s) (1400 15%) loss : 3.518  accuracy : 40.7 %\n",
      "4m 26s (- 22m 40s) (1500 16%) loss : 3.487  accuracy : 40.2 %\n",
      "4m 44s (- 22m 23s) (1600 17%) loss : 3.723  accuracy : 37.7 %\n",
      "5m 2s (- 22m 5s) (1700 18%) loss : 3.566  accuracy : 40.1 %\n",
      "5m 20s (- 21m 47s) (1800 19%) loss : 3.576  accuracy : 39.9 %\n",
      "5m 36s (- 21m 25s) (1900 20%) loss : 3.647  accuracy : 38.8 %\n",
      "5m 54s (- 21m 8s) (2000 21%) loss : 3.627  accuracy : 39.2 %\n",
      "6m 14s (- 20m 58s) (2100 22%) loss : 3.710  accuracy : 37.9 %\n",
      "6m 31s (- 20m 35s) (2200 24%) loss : 3.491  accuracy : 41.0 %\n",
      "6m 49s (- 20m 19s) (2300 25%) loss : 3.595  accuracy : 38.3 %\n",
      "7m 8s (- 20m 4s) (2400 26%) loss : 3.497  accuracy : 40.0 %\n",
      "7m 25s (- 19m 44s) (2500 27%) loss : 3.619  accuracy : 39.2 %\n",
      "7m 42s (- 19m 24s) (2600 28%) loss : 3.567  accuracy : 39.2 %\n",
      "8m 0s (- 19m 7s) (2700 29%) loss : 3.574  accuracy : 38.5 %\n",
      "8m 17s (- 18m 47s) (2800 30%) loss : 3.614  accuracy : 39.7 %\n",
      "8m 34s (- 18m 28s) (2900 31%) loss : 3.605  accuracy : 40.3 %\n",
      "8m 52s (- 18m 11s) (3000 32%) loss : 3.659  accuracy : 38.2 %\n",
      "9m 9s (- 17m 53s) (3100 33%) loss : 3.647  accuracy : 39.7 %\n",
      "9m 28s (- 17m 37s) (3200 34%) loss : 3.612  accuracy : 39.1 %\n",
      "9m 45s (- 17m 17s) (3300 36%) loss : 3.547  accuracy : 39.5 %\n",
      "10m 8s (- 17m 8s) (3400 37%) loss : 3.604  accuracy : 38.6 %\n",
      "10m 27s (- 16m 53s) (3500 38%) loss : 3.464  accuracy : 40.6 %\n",
      "10m 47s (- 16m 37s) (3600 39%) loss : 3.560  accuracy : 38.7 %\n",
      "11m 4s (- 16m 18s) (3700 40%) loss : 3.430  accuracy : 41.2 %\n",
      "11m 22s (- 16m 0s) (3800 41%) loss : 3.628  accuracy : 39.5 %\n",
      "11m 40s (- 15m 43s) (3900 42%) loss : 3.538  accuracy : 38.9 %\n",
      "11m 57s (- 15m 23s) (4000 43%) loss : 3.660  accuracy : 38.8 %\n",
      "12m 12s (- 15m 2s) (4100 44%) loss : 3.604  accuracy : 39.7 %\n",
      "12m 30s (- 14m 44s) (4200 45%) loss : 3.533  accuracy : 39.7 %\n",
      "12m 48s (- 14m 27s) (4300 46%) loss : 3.478  accuracy : 40.0 %\n",
      "13m 6s (- 14m 9s) (4400 48%) loss : 3.447  accuracy : 41.8 %\n",
      "13m 22s (- 13m 49s) (4500 49%) loss : 3.519  accuracy : 39.9 %\n",
      "13m 39s (- 13m 30s) (4600 50%) loss : 3.664  accuracy : 38.4 %\n",
      "13m 57s (- 13m 13s) (4700 51%) loss : 3.475  accuracy : 40.4 %\n",
      "14m 14s (- 12m 54s) (4800 52%) loss : 3.535  accuracy : 40.2 %\n",
      "14m 33s (- 12m 37s) (4900 53%) loss : 3.688  accuracy : 38.3 %\n",
      "14m 50s (- 12m 19s) (5000 54%) loss : 3.472  accuracy : 40.3 %\n",
      "15m 10s (- 12m 2s) (5100 55%) loss : 3.582  accuracy : 39.4 %\n",
      "15m 26s (- 11m 44s) (5200 56%) loss : 3.496  accuracy : 42.0 %\n",
      "15m 45s (- 11m 26s) (5300 57%) loss : 3.536  accuracy : 39.4 %\n",
      "16m 1s (- 11m 8s) (5400 59%) loss : 3.604  accuracy : 38.6 %\n",
      "16m 19s (- 10m 50s) (5500 60%) loss : 3.364  accuracy : 41.0 %\n",
      "16m 36s (- 10m 31s) (5600 61%) loss : 3.545  accuracy : 39.8 %\n",
      "16m 55s (- 10m 14s) (5700 62%) loss : 3.402  accuracy : 41.8 %\n",
      "17m 12s (- 9m 56s) (5800 63%) loss : 3.702  accuracy : 37.5 %\n",
      "17m 29s (- 9m 38s) (5900 64%) loss : 3.632  accuracy : 38.3 %\n",
      "17m 47s (- 9m 20s) (6000 65%) loss : 3.498  accuracy : 40.6 %\n",
      "18m 4s (- 9m 2s) (6100 66%) loss : 3.412  accuracy : 41.1 %\n",
      "18m 21s (- 8m 43s) (6200 67%) loss : 3.333  accuracy : 43.0 %\n",
      "18m 37s (- 8m 25s) (6300 68%) loss : 3.417  accuracy : 42.2 %\n",
      "18m 55s (- 8m 8s) (6400 69%) loss : 3.590  accuracy : 39.1 %\n",
      "19m 13s (- 7m 50s) (6500 71%) loss : 3.477  accuracy : 40.7 %\n",
      "19m 31s (- 7m 32s) (6600 72%) loss : 3.431  accuracy : 41.2 %\n",
      "19m 49s (- 7m 14s) (6700 73%) loss : 3.495  accuracy : 39.0 %\n",
      "20m 6s (- 6m 57s) (6800 74%) loss : 3.529  accuracy : 40.3 %\n",
      "20m 22s (- 6m 38s) (6900 75%) loss : 3.507  accuracy : 39.5 %\n",
      "20m 40s (- 6m 21s) (7000 76%) loss : 3.507  accuracy : 40.8 %\n",
      "20m 58s (- 6m 3s) (7100 77%) loss : 3.568  accuracy : 39.8 %\n",
      "21m 15s (- 5m 45s) (7200 78%) loss : 3.531  accuracy : 39.5 %\n",
      "21m 34s (- 5m 28s) (7300 79%) loss : 3.606  accuracy : 39.8 %\n",
      "21m 53s (- 5m 10s) (7400 80%) loss : 3.582  accuracy : 37.6 %\n",
      "22m 12s (- 4m 53s) (7500 81%) loss : 3.640  accuracy : 39.0 %\n",
      "22m 28s (- 4m 35s) (7600 83%) loss : 3.360  accuracy : 42.7 %\n",
      "22m 45s (- 4m 17s) (7700 84%) loss : 3.409  accuracy : 41.3 %\n",
      "23m 5s (- 3m 59s) (7800 85%) loss : 3.320  accuracy : 42.8 %\n",
      "23m 21s (- 3m 41s) (7900 86%) loss : 3.529  accuracy : 40.4 %\n",
      "23m 39s (- 3m 24s) (8000 87%) loss : 3.458  accuracy : 40.8 %\n",
      "23m 57s (- 3m 6s) (8100 88%) loss : 3.534  accuracy : 40.1 %\n",
      "24m 14s (- 2m 48s) (8200 89%) loss : 3.541  accuracy : 39.8 %\n",
      "24m 31s (- 2m 30s) (8300 90%) loss : 3.517  accuracy : 40.9 %\n",
      "24m 51s (- 2m 13s) (8400 91%) loss : 3.479  accuracy : 40.2 %\n",
      "25m 9s (- 1m 55s) (8500 92%) loss : 3.464  accuracy : 40.6 %\n",
      "25m 26s (- 1m 37s) (8600 93%) loss : 3.636  accuracy : 39.2 %\n",
      "25m 41s (- 1m 19s) (8700 95%) loss : 3.383  accuracy : 41.6 %\n",
      "25m 59s (- 1m 2s) (8800 96%) loss : 3.538  accuracy : 40.7 %\n",
      "26m 17s (- 0m 44s) (8900 97%) loss : 3.481  accuracy : 40.3 %\n",
      "26m 35s (- 0m 26s) (9000 98%) loss : 3.545  accuracy : 39.6 %\n",
      "26m 53s (- 0m 8s) (9100 99%) loss : 3.466  accuracy : 39.9 %\n"
     ]
    }
   ],
   "source": [
    "denoiser.fit(batches2, epochs = 1, lr = 0.0001, unmasked_ratio = 0.15, compute_accuracy = 'xs', print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(denoiser.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth')\n",
    "\n",
    "# load\n",
    "#denoiser.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_sentence_denoiser.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstitution diluent specific of the product to be analysed\n",
      "\n",
      "\n",
      "reconstitution diluent specific of the product to be \u001b[93manalyzed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "denoiser.eval()\n",
    "sentence = ' '.join(sentences[14]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "denoiser(sentence, color = '\\033[93m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "through the vial stopper add the appropriate volume of reconstitution solvent using a syringe fitted with a needle ( diameter smaller than or equal to 0.6 mm )\n",
      "\n",
      "\n",
      "through the vial \u001b[93mflask\u001b[0m add the appropriate volume of reconstitution solvent using a syringe \u001b[93msyringe\u001b[0m with a needle ( diameter \u001b[93mgreater\u001b[0m than or equal to 0.6 \u001b[93m%\u001b[0m )\n"
     ]
    }
   ],
   "source": [
    "denoiser.eval()\n",
    "sentence = ' '.join(sentences[21]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "denoiser(sentence, color = '\\033[93m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
