{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part II - 1 <br><br><br>\n",
    "  Text Classification\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. <font color=red>**Text Classification**</font>\n",
    "\n",
    "2. Machine Translation\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "1. Abstractive Summarization\n",
    "\n",
    "2. Question Answering\n",
    "\n",
    "3. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | | |\n",
    "|------|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | [Open source models](#open_source_models) | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\DL4NLP lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le corpus est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots (où ici les délimiteurs inter-phrases sont clairement identifiables par '|').<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [[text, label]]<br>\n",
    "- text   = str<br>\n",
    "- label = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sentence): # -------------------------  str\n",
    "    sw = ['']\n",
    "    #sw += nltk.corpus.stopwords.words('english')\n",
    "    #sw += nltk.corpus.stopwords.words('french')\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        '''Remove rare symbols from a string'''\n",
    "        s = unicodeToAscii(s.lower().strip()) # \n",
    "        #s = re.sub(r\"[^a-zA-Z\\.\\(\\)\\[\\]]+\", r\" \", s)  # 'r' before a string is for 'raw' # ?&\\%\\_\\- removed # set('''.,:;()*#&-_%!?/\\'\")''')\n",
    "        return s\n",
    "\n",
    "    def wordTokenizerFunction():\n",
    "        # base version\n",
    "        function = lambda sentence : sentence.strip().split()\n",
    "\n",
    "        # nltk version\n",
    "        #function = word_tokenize    \n",
    "        return function\n",
    "\n",
    "    # 1 - caractères spéciaux\n",
    "    def clean_sentence_punct(text): # --------------  str\n",
    "        text = normalizeString(text)\n",
    "        # suppression de la dernière ponctuation\n",
    "        if (len(text) > 0 and text[-1] in ['.', ',', ';', ':', '!', '?']) : text = text[:-1]\n",
    "\n",
    "        text = text.replace(r'(', r' ( ')\n",
    "        text = text.replace(r')', r' ) ')\n",
    "        text = text.replace(r'[', r' [ ')\n",
    "        text = text.replace(r']', r' ] ')\n",
    "        text = text.replace(r'<', r' < ')\n",
    "        text = text.replace(r'>', r' > ')\n",
    "\n",
    "        text = text.replace(r':', r' : ')\n",
    "        text = text.replace(r';', r' ; ')\n",
    "        for i in range(5) :\n",
    "            text = re.sub('(?P<val1>[0-9])\\.(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "            text = re.sub('(?P<val1>[0-9]),(?P<val2>[0-9])', '\\g<val1>__-__\\g<val2>', text)\n",
    "        text = text.replace(r',', ' , ')\n",
    "        text = text.replace(r'.', ' . ')\n",
    "        for i in range(5) : text = re.sub('(?P<val1>[p0-9])__-__(?P<val2>[p0-9])', '\\g<val1>.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. p \\. (?P<val2>[0-9])', '\\g<val1>.p.\\g<val2>', text)\n",
    "        text = re.sub('(?P<val1>[0-9]) \\. s \\. (?P<val2>[0-9])', '\\g<val1>.s.\\g<val2>', text)\n",
    "\n",
    "        text = text.replace(r'\"', r' \" ')\n",
    "        text = text.replace(r'’', r\" ' \")\n",
    "        text = text.replace(r'”', r' \" ')\n",
    "        text = text.replace(r'“', r' \" ')\n",
    "        text = text.replace(r'/', r' / ')\n",
    "\n",
    "        text = re.sub('(…)+', ' … ', text)\n",
    "        text = text.replace('≤', ' ≤ ')          \n",
    "        text = text.replace('≥', ' ≥ ')\n",
    "        text = text.replace('°c', ' °c ')\n",
    "        text = text.replace('°C', ' °c ')\n",
    "        text = text.replace('ºc', ' °c ')\n",
    "        text = text.replace('n°', 'n° ')\n",
    "        text = text.replace('%', ' % ')\n",
    "        text = text.replace('*', ' * ')\n",
    "        text = text.replace('+', ' + ')\n",
    "        text = text.replace('-', ' - ')\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace('®', ' ')\n",
    "        text = text.replace('™', ' ')\n",
    "        text = text.replace('±', ' ± ')\n",
    "        text = text.replace('÷', ' ÷ ')\n",
    "        text = text.replace('–', ' - ')\n",
    "        text = text.replace('μg', ' µg')\n",
    "        text = text.replace('µg', ' µg')\n",
    "        text = text.replace('µl', ' µl')\n",
    "        text = text.replace('μl', ' µl')\n",
    "        text = text.replace('µm', ' µm')\n",
    "        text = text.replace('μm', ' µm')\n",
    "        text = text.replace('ppm', ' ppm')\n",
    "        text = re.sub('(?P<val1>[0-9])mm', '\\g<val1> mm', text)\n",
    "        text = re.sub('(?P<val1>[0-9])g', '\\g<val1> g', text)\n",
    "        text = text.replace('nm', ' nm')\n",
    "\n",
    "        text = re.sub('fa(?P<val1>[0-9])', 'fa \\g<val1>', text)\n",
    "        text = re.sub('g(?P<val1>[0-9])', 'g \\g<val1>', text)\n",
    "        text = re.sub('n(?P<val1>[0-9])', 'n \\g<val1>', text)\n",
    "        text = re.sub('p(?P<val1>[0-9])', 'p \\g<val1>', text)\n",
    "        text = re.sub('q_(?P<val1>[0-9])', 'q_ \\g<val1>', text)\n",
    "        text = re.sub('u(?P<val1>[0-9])', 'u \\g<val1>', text)\n",
    "        text = re.sub('ud(?P<val1>[0-9])', 'ud \\g<val1>', text)\n",
    "        text = re.sub('ui(?P<val1>[0-9])', 'ui \\g<val1>', text)\n",
    "\n",
    "        text = text.replace('=', ' ')\n",
    "        text = text.replace('!', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace(r' , ', ' ')\n",
    "        text = text.replace(r' . ', ' ')\n",
    "\n",
    "        text = re.sub('(?P<val>[0-9])ml', '\\g<val> ml', text)\n",
    "        text = re.sub('(?P<val>[0-9])mg', '\\g<val> mg', text)\n",
    "\n",
    "        for i in range(5) : text = re.sub('( [0-9]+ )', ' ', text)\n",
    "        #text = re.sub('cochran(\\S)*', 'cochran ', text)\n",
    "        return text\n",
    "\n",
    "    # 3 - split des mots\n",
    "    def wordSplit(sentence, tokenizeur): # ------------- [str]\n",
    "        return tokenizeur(sentence)\n",
    "\n",
    "    # 4 - mise en minuscule et enlèvement des stopwords\n",
    "    def stopwordsRemoval(sentence, sw): # ------------- [[str]]\n",
    "        return [word for word in sentence if word not in sw]\n",
    "\n",
    "    # 6 - correction des mots\n",
    "    def correction(text):\n",
    "        def correct(word):\n",
    "            return spelling.suggest(word)[0]\n",
    "        list_of_list_of_words = [[correct(word) for word in sentence] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    # 7 - stemming\n",
    "    def stemming(text): # ------------------------- [[str]]\n",
    "        list_of_list_of_words = [[PorterStemmer().stem(word) for word in sentence if word not in sw] for sentence in text]\n",
    "        return list_of_list_of_words\n",
    "\n",
    "    tokenizeur = wordTokenizerFunction()\n",
    "    sentence = clean_sentence_punct(str(sentence))\n",
    "    sentence = wordSplit(sentence, tokenizeur)\n",
    "    sentence = stopwordsRemoval(sentence, sw)\n",
    "    #text = correction(text)\n",
    "    #text = stemming(text)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def importSheet(file_name) :\n",
    "    df = pd.read_excel(file_name, sep = ',', header = None)\n",
    "    headers = [i for i, titre in enumerate(df.iloc[0,:].values) if i in [1, 2] or titre == 'score manuel'] \n",
    "    db = df.iloc[1:, headers].values.tolist()\n",
    "    labelled_sentences = [[' '.join(cleanSentence(str(el[0]) + ' | ' + str(el[1]))), el[-1]] for el in db if el[-1] in [0, 1]]\n",
    "    #labelled_texts = [[[s.strip() for s in el[0].split('|')], el[1]] for el in labelled_sentences]\n",
    "    return labelled_sentences\n",
    "\n",
    "\n",
    "def importCorpus(path_to_data) :\n",
    "    corpus = []\n",
    "    reps = os.listdir(path_to_data)\n",
    "    for rep in reps :\n",
    "        files = os.listdir(path_to_data + '\\\\' + rep)\n",
    "        for file in files :\n",
    "            file_name = path_to_data + '\\\\' + rep + '\\\\' + file\n",
    "            corpus += importSheet(file_name)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_sentences = importCorpus(path_to_NLP + '\\\\data\\\\AMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section 3.2.p.5.1 specification ( s ) | the testing performed on the finished product ( fp ) is in compliance with both current european pharmacopoeia ( ph eur ) and world health organization ( who ) requirements of the vaccine',\n",
       " 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "_Remark_ : The pre-trained Word2vec models are the same as those used in **Part I - 2 Sentence Classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_level_custom\"></a>\n",
    "\n",
    "\n",
    "#### 1.1.1 Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_word2vec = torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I1_skipgram.pt').freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gensim\"></a>\n",
    "\n",
    "#### 1.1.2 Gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_word2vec = Word2VecConnector(Word2Vec.load(get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_skipgram_gensim.model\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fastText\"></a>\n",
    "\n",
    "#### 1.1.3 FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_word2vec = Word2VecConnector(FastText.load(get_tmpfile(path_to_NLP + \"\\\\saves\\\\models\\\\DL4NLP_I1_fasttext.model\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "<a id=\"bi_gru\"></a>\n",
    "\n",
    "#### 1.2.1 Bi-directionnal GRU contextualization\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Attention module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "<a id=\"attention\"></a>\n",
    "\n",
    "#### 1.3.1 Classical Attention Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from libDL4NLP.modules import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dim, query_dim, \n",
    "                 dropout = 0, \n",
    "                 method = 'concat' \n",
    "                ): \n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.method = method\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.output_dim = embedding_dim\n",
    "        \n",
    "        # parameters\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.attn_layer = nn.Linear(embedding_dim + query_dim, embedding_dim)\n",
    "        self.attn_v = nn.Linear(embedding_dim, 1, bias = False)\n",
    "        self.act = F.softmax\n",
    "        \n",
    "    def forward(self, embeddings, query = None):\n",
    "        if self.method == 'concat' :\n",
    "            weights = torch.cat((query.expand(-1, embeddings.size(1), -1), embeddings), 2) if query is not None else embeddings\n",
    "            weights = self.attn_layer(weights).tanh()          # size (batch_size, input_length, embedding_dim)\n",
    "            weights = self.act(self.attn_v(weights), dim = 1)  # size (batch_size, input_length, 1)\n",
    "            weights = torch.transpose(weights, 1, 2)           # size (batch_size, 1, input_length)\n",
    "        elif self.method == 'dot' :\n",
    "            query = torch.transpose(query, 1, 2)               # size (batch_size, query_dim, 1)\n",
    "            weights = torch.bmm(embeddings, query)             # size (batch_size, input_length, 1)\n",
    "            weights = self.act(weights, dim = 1)               # size (batch_size, input_length, 1)\n",
    "            weights = torch.transpose(weights, 1, 2)           # size (batch_size, 1, input_length)\n",
    "        applied = self.dropout(torch.bmm(weights, embeddings)) # size (batch_size, 1, embedding_dim)\n",
    "        return applied, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Hierarchical Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.modules import HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    '''Ce module d'attention est :\n",
    "    \n",
    "    - hiérarchique avec bi-GRU entre les deux niveaux d'attention\n",
    "    - globalement multi-hopé, où il est possible d'effectuer plusieurs passes pour accumuler de l'information\n",
    "    '''\n",
    "    def __init__(self, embedding_dim, hidden_dim, query_dim,\n",
    "                 n_layers = 1,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(HAN, self).__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = self.query_dim if (transf or (hops > 1 and query_dim != hidden_dim)) else hidden_dim\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        \n",
    "        # modules\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        # first attention module\n",
    "        if share : self.attn1 = nn.ModuleList([Attention(embedding_dim, query_dim, dropout)] * hops)\n",
    "        else     : self.attn1 = nn.ModuleList([Attention(embedding_dim, query_dim, dropout) for _ in range(hops)])\n",
    "        # intermediate encoder module\n",
    "        self.bigru = RecurrentEncoder(embedding_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        # second attention module\n",
    "        if share : self.attn2 = nn.ModuleList([Attention(self.bigru.output_dim, query_dim, dropout)] * hops)\n",
    "        else     : self.attn2 = nn.ModuleList([Attention(self.bigru.output_dim, query_dim, dropout) for _ in range(hops)])\n",
    "        # accumulation step\n",
    "        self.transf = nn.Linear(self.bigru.output_dim, self.output_dim, bias = False) \\\n",
    "                      if (transf or (self.hops > 1 and query_dim != self.bigru.output_dim)) else None\n",
    "        \n",
    "    def singlePass(self, packed_embeddings, query, attn1, attn2): \n",
    "        # first attention\n",
    "        output, weights1 = attn1(packed_embeddings, query) # size (dialogue_length, 1, embedding_dim)\n",
    "        # intermediate biGRU\n",
    "        output, _ = self.bigru(output.transpose(0, 1))     # size (1, dialogue_length, hidden_dim)\n",
    "        output = self.dropout(output)\n",
    "        # second attention\n",
    "        output, weights2 = attn2(output, query)            # size (1, 1, hidden_dim)\n",
    "        # output decision vector\n",
    "        if self.transf is not None : output = self.transf(output) # size (1, 1, output_dim)\n",
    "        if query is not None       : output = output + query\n",
    "        # return\n",
    "        return output, weights1, weights2\n",
    "        \n",
    "    def forward(self, packed_embeddings, query = None):\n",
    "        weights1_list = []\n",
    "        weights2_list = []\n",
    "        if packed_embeddings is not None :\n",
    "            for hop in range(self.hops) :\n",
    "                query, weights1, weights2 = self.singlePass(packed_embeddings, query, self.attn1[hop], self.attn2[hop])\n",
    "                weights1_list.append(weights1)\n",
    "                weights2_list.append(weights2)\n",
    "        # output decision vector\n",
    "        return query, weights1_list, weights2_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation of attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.utils import HANViewerOnWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HANViewerOnWords(attn_words, attn_sentences, sentences, \n",
    "                                       colors = 'Reds', n = 8) : \n",
    "    '''attn_words = [1D np.array]\n",
    "       attn_sentences = 1D np.array\n",
    "       sentences = [[str]]\n",
    "    '''\n",
    "    def generateColors(colors, n):\n",
    "        colors = plt.get_cmap(colors)\n",
    "        Triplets = []\n",
    "        for i in range(n) :\n",
    "            triplet = [int(j * 256) for j in colors(i/10)[:3]]\n",
    "            Triplets.append(triplet)\n",
    "        return Triplets\n",
    "    \n",
    "    def weight2color(weight, triplets):\n",
    "        n = len(triplets)\n",
    "        for i in range(n):\n",
    "            if weight >= i/n and weight <= (i+1)/n : \n",
    "                return triplets[i]\n",
    "            \n",
    "    def addColor(texte, RGB = (100,100,100)):\n",
    "        new_texte = '\\x1b[48;2;'  + str(RGB[0]) + \";\" + str(RGB[1]) + \";\" + str(RGB[2]) + \"m\"  + texte + \"\\x1b[0m\"\n",
    "        return new_texte\n",
    "    \n",
    "    # -- main --\n",
    "    Triplets = generateColors(colors, n)\n",
    "    Colored_text = ''\n",
    "    for i, s in enumerate(sentences) :\n",
    "        s_color = weight2color(attn_sentences[i], Triplets)\n",
    "        Colored_text += addColor('  ', s_color) + ' '\n",
    "        for j, w in enumerate(s) :\n",
    "            color = weight2color(attn_words[i][j], Triplets)\n",
    "            Colored_text += addColor(w, color) + ' '\n",
    "        Colored_text += '\\n' \n",
    "    print(Colored_text)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Text Classifier\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.models import TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden1_dim = 100,\n",
    "                 hidden2_dim = 100,\n",
    "                 n1_layers = 1, \n",
    "                 n2_layers = 1,\n",
    "                 hops = 1, \n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 n_class = 2, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                ):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        # embedding\n",
    "        self.bin_mode  = (n_class == 'binary')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, \n",
    "                                          hidden1_dim, \n",
    "                                          n1_layers, \n",
    "                                          dropout, \n",
    "                                          bidirectional = True)\n",
    "        self.attention = HAN(embedding_dim = self.context.output_dim,\n",
    "                             hidden_dim = hidden2_dim,\n",
    "                             query_dim = 0, # self.context.output_dim,\n",
    "                             n_layers = n2_layers,\n",
    "                             hops = hops,\n",
    "                             share = share,\n",
    "                             transf = transf,\n",
    "                             dropout = dropout)\n",
    "        self.out       = nn.Linear(self.attention.output_dim, (1 if self.bin_mode else n_class))\n",
    "        self.act       = F.sigmoid if self.bin_mode else F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        if self.bin_mode : self.criterion = nn.BCEWithLogitsLoss(size_average = False)\n",
    "        else             : self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def showAttention(self, words, attn) :\n",
    "        for i in range(attn.size(1)) :\n",
    "            fig, ax  = plt.subplots()\n",
    "            im       = heatmap(np.array(attn[:, i, :].data.cpu().numpy()),  [' '], words, ax=ax, cmap=\"YlGn\", cbarlabel=\"harvest [t/year]\")\n",
    "            texts    = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "        return\n",
    "        \n",
    "    def forward(self, text, attention_method = None) :\n",
    "        '''classifies a sentence as string'''\n",
    "        sentences        = self.tokenizer(text)\n",
    "        embeddings       = [self.word2vec(words, self.device).squeeze(0) for words in sentences] # list of tensors of size (1, n_words, embedding_dim)\n",
    "        embeddings       = nn.utils.rnn.pad_sequence(embeddings, batch_first = True, padding_value = 0)  # size (n_sentences, n_words, embedding_dim)\n",
    "        hiddens, _       = self.context(embeddings, enforce_sorted = False) # size (n_sentences, n_words, embedding_dim)\n",
    "        attended, w1, w2 = self.attention(hiddens)  # size (1, 1, embedding_dim)\n",
    "        if self.bin_mode : prediction = self.act(self.out(attended).view(-1)).data.topk(1)[0].item()\n",
    "        else             : prediction = self.act(self.out(attended.squeeze(1)), dim = 1).data.topk(1)[1].item()\n",
    "        if attention_method is not None :\n",
    "            attn_words     = [np.array(s.view(-1).data.cpu().numpy()) for s in w1[0]]\n",
    "            attn_sentences = np.array(w2[0].view(-1).data.cpu().numpy())\n",
    "            attention_method(attn_words, attn_sentences, sentences)\n",
    "        return prediction\n",
    "    \n",
    "    def generatePaddedTexts(self, texts) :\n",
    "        padded_data = []\n",
    "        for text, label in texts :\n",
    "            pack0 = self.tokenizer(text)\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in words] for words in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0])               # size = (text_length) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))     # size = (text_length, max_length)\n",
    "            pack1 = [label]\n",
    "            if self.bin_mode : pack1 = Variable(torch.FloatTensor(pack1)) # size = (1) \n",
    "            else             : pack1 = Variable(torch.LongTensor(pack1))  # size = (1) \n",
    "            padded_data.append([[pack0, lengths], pack1])\n",
    "        return padded_data\n",
    "    \n",
    "    def compute_accuracy(self, texts) :\n",
    "        #batches = self.generatePaddedTexts(texts)\n",
    "        score = 0\n",
    "        for text, label in texts :\n",
    "            predict = self(text)\n",
    "            if self.bin_mode : score += (abs(label - predict) < 0.5)\n",
    "            else             : score += (label == predict)\n",
    "        return score * 100 / len(texts)\n",
    "    \n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings       = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens, _       = self.context(embeddings, lengths = batch[1].to(self.device), enforce_sorted = False)\n",
    "            attended, w1, w2 = self.attention(hiddens)\n",
    "            if self.bin_mode : return self.out(attended).view(-1)\n",
    "            else             : return F.log_softmax(self.out(attended.squeeze(1)))\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            if self.bin_mode : return sum(torch.abs(targets - self.act(log_probs)) < 0.5).item() * 100 / targets.size(0)\n",
    "            else             : return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "            \n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets = batch[1].to(self.device).view(-1)\n",
    "            loss    = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119551"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = TextClassifier(device,\n",
    "                            tokenizer = lambda t : [s.strip().split(' ') for s in t.split('|')],\n",
    "                            word2vec = custom_word2vec,\n",
    "                            hidden1_dim = 50,\n",
    "                            hidden2_dim = 50,\n",
    "                            n1_layers = 2,\n",
    "                            n2_layers = 1,\n",
    "                            hops = 1,\n",
    "                            share = True,\n",
    "                            n_class = 'binary', \n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.Adam)\n",
    "\n",
    "classifier.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (word2vec): Word2Vec(\n",
       "    (embedding): Embedding(4066, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (bigru): GRU(75, 50, num_layers=2, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (attention): HAN(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (attn1): ModuleList(\n",
       "      (0): Attention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attn_layer): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (attn_v): Linear(in_features=50, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bigru): RecurrentEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (bigru): GRU(50, 50, bidirectional=True)\n",
       "    )\n",
       "    (attn2): ModuleList(\n",
       "      (0): Attention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attn_layer): Linear(in_features=50, out_features=50, bias=True)\n",
       "        (attn_v): Linear(in_features=50, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = classifier.generatePaddedTexts(labelled_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 41s (- 13m 42s) (1000 4%) loss : 0.451  accuracy : 80.9 %\n",
      "1m 16s (- 11m 56s) (2000 9%) loss : 0.309  accuracy : 86.8 %\n",
      "1m 51s (- 11m 3s) (3000 14%) loss : 0.265  accuracy : 88.4 %\n",
      "2m 24s (- 10m 5s) (4000 19%) loss : 0.249  accuracy : 89.1 %\n",
      "2m 57s (- 9m 21s) (5000 24%) loss : 0.213  accuracy : 90.9 %\n",
      "3m 33s (- 8m 44s) (6000 28%) loss : 0.207  accuracy : 91.7 %\n",
      "4m 6s (- 8m 4s) (7000 33%) loss : 0.199  accuracy : 92.1 %\n",
      "4m 38s (- 7m 25s) (8000 38%) loss : 0.224  accuracy : 90.8 %\n",
      "5m 11s (- 6m 47s) (9000 43%) loss : 0.198  accuracy : 92.2 %\n",
      "5m 43s (- 6m 10s) (10000 48%) loss : 0.153  accuracy : 93.5 %\n",
      "6m 16s (- 5m 34s) (11000 52%) loss : 0.219  accuracy : 91.3 %\n",
      "6m 49s (- 4m 59s) (12000 57%) loss : 0.185  accuracy : 92.8 %\n",
      "7m 23s (- 4m 25s) (13000 62%) loss : 0.184  accuracy : 91.7 %\n",
      "7m 55s (- 3m 50s) (14000 67%) loss : 0.155  accuracy : 93.1 %\n",
      "8m 28s (- 3m 16s) (15000 72%) loss : 0.207  accuracy : 90.9 %\n",
      "9m 3s (- 2m 42s) (16000 76%) loss : 0.193  accuracy : 92.6 %\n",
      "9m 40s (- 2m 9s) (17000 81%) loss : 0.182  accuracy : 93.2 %\n",
      "10m 13s (- 1m 34s) (18000 86%) loss : 0.181  accuracy : 92.5 %\n",
      "10m 46s (- 1m 0s) (19000 91%) loss : 0.180  accuracy : 91.9 %\n",
      "11m 23s (- 0m 26s) (20000 96%) loss : 0.150  accuracy : 93.5 %\n",
      "epoch 1\n",
      "0m 32s (- 10m 39s) (1000 4%) loss : 0.140  accuracy : 94.3 %\n",
      "1m 4s (- 10m 4s) (2000 9%) loss : 0.113  accuracy : 95.4 %\n",
      "1m 36s (- 9m 31s) (3000 14%) loss : 0.151  accuracy : 93.2 %\n",
      "2m 9s (- 9m 4s) (4000 19%) loss : 0.157  accuracy : 94.2 %\n",
      "2m 41s (- 8m 29s) (5000 24%) loss : 0.141  accuracy : 94.0 %\n",
      "3m 13s (- 7m 56s) (6000 28%) loss : 0.138  accuracy : 93.6 %\n",
      "3m 45s (- 7m 23s) (7000 33%) loss : 0.117  accuracy : 94.8 %\n",
      "4m 17s (- 6m 51s) (8000 38%) loss : 0.125  accuracy : 94.7 %\n",
      "4m 49s (- 6m 19s) (9000 43%) loss : 0.126  accuracy : 94.9 %\n",
      "5m 21s (- 5m 46s) (10000 48%) loss : 0.131  accuracy : 94.0 %\n",
      "5m 56s (- 5m 16s) (11000 52%) loss : 0.140  accuracy : 94.3 %\n",
      "6m 32s (- 4m 47s) (12000 57%) loss : 0.119  accuracy : 95.2 %\n",
      "7m 7s (- 4m 16s) (13000 62%) loss : 0.118  accuracy : 95.5 %\n",
      "7m 49s (- 3m 47s) (14000 67%) loss : 0.128  accuracy : 94.9 %\n",
      "8m 42s (- 3m 21s) (15000 72%) loss : 0.098  accuracy : 96.3 %\n",
      "9m 33s (- 2m 51s) (16000 76%) loss : 0.133  accuracy : 94.0 %\n",
      "10m 17s (- 2m 17s) (17000 81%) loss : 0.141  accuracy : 93.8 %\n",
      "10m 51s (- 1m 40s) (18000 86%) loss : 0.123  accuracy : 95.5 %\n",
      "11m 25s (- 1m 4s) (19000 91%) loss : 0.121  accuracy : 95.1 %\n",
      "11m 58s (- 0m 28s) (20000 96%) loss : 0.122  accuracy : 95.1 %\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(batches, epochs = 1, lr = 0.05, print_every = 1000)\n",
    "classifier.fit(batches, epochs = 1, lr = 0.01, print_every = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "#torch.save(classifier.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_II1_text_classifier.pth')\n",
    "\n",
    "# load\n",
    "classifier.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_II1_text_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation single-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;253;202;181m  \u001b[0m \u001b[48;2;255;229;217msection\u001b[0m \u001b[48;2;256;245;240m3.2.p.5.3\u001b[0m \u001b[48;2;256;245;240mvalidation\u001b[0m \u001b[48;2;256;245;240mof\u001b[0m \u001b[48;2;256;245;240manalytical\u001b[0m \u001b[48;2;255;229;217mprocedures\u001b[0m \n",
      "\u001b[48;2;253;202;181m  \u001b[0m \u001b[48;2;256;245;240mbacterial\u001b[0m \u001b[48;2;256;245;240mendotoxins\u001b[0m \u001b[48;2;256;245;240mtest\u001b[0m \n",
      "\u001b[48;2;255;229;217m  \u001b[0m \u001b[48;2;255;229;217mpreliminary\u001b[0m \u001b[48;2;255;229;217mtests\u001b[0m \n",
      "\u001b[48;2;255;229;217m  \u001b[0m \u001b[48;2;255;229;217mthe\u001b[0m \u001b[48;2;255;229;217mchosen\u001b[0m \u001b[48;2;255;229;217mdilution\u001b[0m \u001b[48;2;253;202;181mis\u001b[0m \u001b[48;2;256;245;240mthe\u001b[0m \u001b[48;2;256;245;240mpure\u001b[0m \u001b[48;2;256;245;240mproduct\u001b[0m \u001b[48;2;256;245;240mfor\u001b[0m \u001b[48;2;256;245;240mthe\u001b[0m \u001b[48;2;256;245;240myellow\u001b[0m \u001b[48;2;256;245;240mfever\u001b[0m \u001b[48;2;256;245;240mvaccine\u001b[0m \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0455450676381588"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention heads = 1\n",
    "classifier.eval()\n",
    "text = labelled_sentences[157][0]\n",
    "classifier(text, attention_method = HANViewerOnWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.65704387990762"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.eval()\n",
    "classifier.compute_accuracy(labelled_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(classifier.state_dict(), path_to_NLP + '\\\\saves\\\\DL4NLP_I2_sentence_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
