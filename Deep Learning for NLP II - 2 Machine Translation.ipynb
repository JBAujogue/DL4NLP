{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part II - 2 <br><br><br>\n",
    "  Machine Translation\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. <font color=red>**Machine Translation**</font>\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.3.1\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\libDL4NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Normalisation -------------------------------\n",
    "def normalizeString(s):\n",
    "    '''Remove rare symbols from a string'''\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    " \n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #s = re.sub(r\"[^a-zA-Z0-9?&\\%\\-\\_]+\", r\" \", s) \n",
    "    s = re.sub(\"\\(\", r\" ( \", s)\n",
    "    s = re.sub(\"\\)\", r\" ) \", s)\n",
    "    s = re.sub(r\"\\.\", r\" . \", s)\n",
    "    s = re.sub(r\",\", r\" , \", s)\n",
    "    s = re.sub(r\"!\", r\" ! \", s)\n",
    "    s = re.sub(r\":\", r\" : \", s)\n",
    "    s = re.sub(r\"-\", r\" - \", s)\n",
    "    s = re.sub(r\"'\", r\" ' \", s)\n",
    "    s = re.sub(r\";\", r\" ; \", s)\n",
    "    s = re.sub(r' +', r' ', s).strip()\n",
    "    return s \n",
    "\n",
    "\n",
    "\n",
    "#--------------------- import des dialogues --------------------\n",
    "def importDialogues(path, limit = None):\n",
    "    '''Import a textfile containing dialogues and returns a list, each element \n",
    "       corresponding to a dialogue and also being under the form of a list, with \n",
    "       each element being a list of two elements : an element giving a user \n",
    "       utterance and another element giving the bot response. Both elements are \n",
    "       normalized strings.\n",
    "       Ex. The dialogue :\n",
    "       \n",
    "               hi    hello what can i help you with today\n",
    "               can you book a table    i m on it\n",
    "               \n",
    "       now becomes :\n",
    "       \n",
    "              [['hi', 'hello what can i help you with today'], \n",
    "               ['can you book a table', 'i m on it']]\n",
    "               \n",
    "       Lines corresponding to user utterance with no bot response are discarted.\n",
    "    '''\n",
    "    def cleanS(s):\n",
    "        cleans = normalizeString(s)\n",
    "        cleans = cleans.replace('?', ' ? ').strip()\n",
    "        return cleans\n",
    "    \n",
    "    dialogues = []\n",
    "    dialogues_import = open(path, encoding='utf-8').read().strip().split('\\n\\n')\n",
    "    for i, d in enumerate(dialogues_import):\n",
    "        dialogue = []\n",
    "        lines = d.split('\\n')\n",
    "        for l in lines:\n",
    "            if len(l.split('\\t')) == 2 :\n",
    "                pair = [cleanS(s) for s in l.split('\\t')]\n",
    "                dialogue.append(pair)\n",
    "            elif len(l.split('\\t')) == 3 :\n",
    "                pair = [cleanS(s) for s in l.split('\\t')[:2]]\n",
    "                dialogue.append(pair)\n",
    "        dialogues.append(dialogue)\n",
    "        if limit is not None and i == limit -1 : break\n",
    "    return dialogues\n",
    "\n",
    "\n",
    "def getUniqueQAs(dialogues) :\n",
    "    uniq = []\n",
    "    for qa in dialogues :\n",
    "        if qa not in uniq : uniq.append(qa)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "\n",
    "#------------------ Dictionnaire des mots variables -----------------------------\n",
    "def motVar(file):\n",
    "    '''Applies to the Master's program dataset.\n",
    "       Import the collection of pairs token-content for a set of variable words.\n",
    "    '''\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "    motsVar = {}\n",
    "    for l in lines :\n",
    "        cle, valeur = l.split('\\t')\n",
    "        motsVar[cle.lower()] = valeur\n",
    "    return motsVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "motsVar    = motVar(path_to_NLP + '\\\\data\\\\M2DS\\\\M2DS_2019_07\\\\chatbot-M2-DS-Variables.txt')\n",
    "dialogues  = importDialogues(path_to_NLP + '\\\\data\\\\M2DS\\\\M2DS_2019_10\\\\ChatbotDS_P_Train.tsv')\n",
    "dialogues_tst = importDialogues(path_to_NLP + '\\\\data\\\\M2DS\\\\M2DS_2019_10\\\\ChatbotDS_P_Test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200224 20561\n"
     ]
    }
   ],
   "source": [
    "qa_trn = [qa for dialogue in dialogues for qa in dialogue] # getUniqueQAs(\n",
    "qa_tst = [qa for dialogue in dialogues_tst for qa in dialogue] # getUniqueQAs(\n",
    "\n",
    "print(len(qa_trn), len(qa_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il vaut mieux suivre quel parcours  ? \t les deux parcours sont equivalents , c ' est a vous de faire un choix .\n",
      "quand est ce que commence les cours  ? \t la rentree est le daterentree\n",
      "comment je prend contact avec la scolarite  ? \t les contacts de la scolarite sont accessibles sur : sitescolarite\n",
      "est ce que il y a des cours en anglais  ? \t plusieurs supports de cours sont en anglais\n",
      "tous les cours sont obligatoires  ? \t tous les cours sont obligatoires , il n ' existe pas d ' options .\n",
      "merci pour les renseignements \t je vous en prie\n",
      "peux tu m ' aider  ? \t je suis la pour vous renseigner sur le master 2 data science .\n",
      "le cours de data visualisation est - il en ligne  ? \t vous devriez prendre contact avec l ' enseignant de l ' ue et lui demander\n",
      "j ' ai envie de faire de la data science \t alors vous devriez songer a candidater a ce master !\n",
      "il y a des livres a lire en statistique  ? \t vous devriez prendre contact avec l ' enseignant de l ' ue et lui demander\n",
      "il y a combien de candidats  ? \t il y a plus de nbrcandidats candidats chaque annee , la selection est rude !\n",
      "combien de places sont ouvertes  ? \t souhaitez - vous suivre le parcours informatique ou mathematiques  ?\n",
      "informatique \t etes - vous un etudiant inscrit en france  ?\n",
      "non \t il y a nbreplaces_hors_france_informatique places disponibles pour les etudiants hors france en parcours informatique .\n",
      "comment je peux payer  ? \t vous pouvez payer par carte , par cheque bancaire , par virement ou en espece .\n",
      "avez - vous une maquette detaillee de ce master  ? \t les details de la formation est disponible sur la page du master parcours informatique : site_informatique\n",
      "comment ca va  ? \t je vais bien merci , en quoi puis - je vous aider  ?\n"
     ]
    }
   ],
   "source": [
    "for qa in dialogues[0] : print(qa[0], '\\t', qa[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_in  = [['SOS'] + [w for w in qa[0].split(' ')] + ['EOS'] for qa in qa_trn]\n",
    "corpus_out = [['SOS'] + [w for w in qa[1].split(' ')] + ['EOS'] for qa in qa_trn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareWord2vec(corpus) :\n",
    "    fastText_word2vec = FastText(size = 75, \n",
    "                                 window = 5, \n",
    "                                 min_count = 1, \n",
    "                                 negative = 20,\n",
    "                                 sg = 1)\n",
    "    fastText_word2vec.build_vocab(corpus)\n",
    "    print(len(fastText_word2vec.wv.vocab))\n",
    "    fastText_word2vec.train(sentences = corpus, \n",
    "                            epochs = 5,\n",
    "                            total_examples = fastText_word2vec.corpus_count)\n",
    "    word2vec = Word2VecConnector(fastText_word2vec)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "896\n"
     ]
    }
   ],
   "source": [
    "word2vec_in  = prepareWord2vec(corpus_in)\n",
    "word2vec_out = prepareWord2vec(corpus_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bonjour', 0.9852568507194519),\n",
       " ('mal', 0.8179829120635986),\n",
       " ('principal', 0.6931003928184509),\n",
       " ('principalement', 0.6789224147796631),\n",
       " ('bien', 0.6547073125839233),\n",
       " ('english', 0.6544337272644043),\n",
       " ('handle', 0.6447221040725708),\n",
       " ('ok', 0.6249094605445862),\n",
       " ('cordialement', 0.6209567785263062),\n",
       " ('speak', 0.618739128112793)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_in.word2vec.most_similar('bonjou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bonjour', 0.9725614786148071),\n",
       " ('accord', 0.786864161491394),\n",
       " ('comment', 0.6106002330780029),\n",
       " ('aider', 0.6066528558731079),\n",
       " ('accordez', 0.5910225510597229),\n",
       " ('excusez', 0.5597412586212158),\n",
       " ('monotonie', 0.5362932682037354),\n",
       " ('puis', 0.5213144421577454),\n",
       " ('vais', 0.5169289112091064),\n",
       " ('dynamiques', 0.4986083209514618)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_out.word2vec.most_similar('bonjou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Attention module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "<a id=\"attention\"></a>\n",
    "\n",
    "We use here a classical Attention Module :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Decoder module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "<a id=\"decoder\"></a>\n",
    "\n",
    "#### 1.4.1 Classical Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.modules import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, word2vec, hidden_dim, \n",
    "                 n_layers = 1,\n",
    "                 dropout = 0.1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(Decoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bound = bound\n",
    "        # modules\n",
    "        self.word2vec = word2vec\n",
    "        self.gru = nn.GRU(word2vec.output_dim, \n",
    "                          hidden_dim, \n",
    "                          n_layers, \n",
    "                          dropout = dropout, \n",
    "                          batch_first = True)\n",
    "        self.out = nn.Linear(hidden_dim, word2vec.lang.n_words)\n",
    "        self.act = F.log_softmax\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generateWord(self, hidden, embeddings, word_index):\n",
    "        # update hidden state\n",
    "        embedding = self.word2vec.embedding(word_index) # size (batch_size, 1, embedding_dim)\n",
    "        embedding = self.dropout(embedding)\n",
    "        _, hidden = self.gru(embedding, hidden)         # size (n_layers, batch_size, embedding_dim)\n",
    "        # generate next word\n",
    "        log_prob = self.out(hidden[-1])                 # size (batch_size, lang_size)\n",
    "        log_prob = self.act(log_prob, dim = 1)          # size (batch_size, lang_size)\n",
    "        return log_prob, hidden\n",
    "    \n",
    "    def forward(self, hidden, embeddings = None, device = None) :\n",
    "        answer = []\n",
    "        EOS_token  = self.word2vec.lang.getIndex('EOS')\n",
    "        word = self.word2vec.lang.getIndex('SOS')\n",
    "        word = Variable(torch.LongTensor([[word]])) # size (1)\n",
    "        hidden = hidden[-self.n_layers:]\n",
    "        for t in range(self.bound) :\n",
    "            # compute next word\n",
    "            if device is not None : word = word.to(device) # size (1)\n",
    "            log_prob, hidden = self.generateWord(hidden, embeddings, word)\n",
    "            word = log_prob.topk(1, dim = 1)[1].view(1, 1)\n",
    "            # add to output\n",
    "            if word.item() == EOS_token : break\n",
    "            else : answer.append(word.item())\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Attention Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.modules import AttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, word2vec, embedding_dim, hidden_dim, \n",
    "                 n_layers = 1,\n",
    "                 dropout = 0.1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bound = bound\n",
    "        # modules\n",
    "        self.word2vec = word2vec\n",
    "        self.gru = nn.GRU(word2vec.output_dim, \n",
    "                          hidden_dim, \n",
    "                          n_layers, \n",
    "                          dropout = dropout, \n",
    "                          batch_first = True)\n",
    "        self.attn = Attention(embedding_dim, query_dim = hidden_dim, dropout = dropout)\n",
    "        self.merge = nn.Linear(self.attn.output_dim + hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, word2vec.lang.n_words)\n",
    "        self.act = F.log_softmax\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generateWord(self, hidden, embeddings, word_index):\n",
    "        # update hidden state\n",
    "        embedding = self.word2vec.embedding(word_index) # size (batch_size, 1, embedding_dim)\n",
    "        embedding = self.dropout(embedding)\n",
    "        _, hidden = self.gru(embedding, hidden)         # size (n_layers, batch_size, embedding_dim)\n",
    "        # merge with attention\n",
    "        attn, weights = self.attn(embeddings, hidden[-1].unsqueeze(1))   # size (batch_size, 1, embedding_dim)\n",
    "        merge = torch.cat([hidden[-1], attn.squeeze(1)], dim = 1)\n",
    "        merge = self.merge(merge).tanh()                # size (batch_size, embedding_dim)\n",
    "        merge = self.dropout(merge)\n",
    "        # generate next word\n",
    "        log_prob = self.out(merge)                      # size (batch_size, lang_size)\n",
    "        log_prob = self.act(log_prob, dim = 1)          # size (batch_size, lang_size)\n",
    "        return log_prob, hidden\n",
    "    \n",
    "    def forward(self, hidden, embeddings, device = None) :\n",
    "        answer = []\n",
    "        EOS_token  = self.word2vec.lang.getIndex('EOS')\n",
    "        word = self.word2vec.lang.getIndex('SOS')\n",
    "        word = Variable(torch.LongTensor([[word]])) # size (1)\n",
    "        hidden = hidden[-self.n_layers:]\n",
    "        for t in range(self.bound) :\n",
    "            # compute next word\n",
    "            if device is not None : word = word.to(device) # size (1)\n",
    "            log_prob, hidden = self.generateWord(hidden, embeddings, word)\n",
    "            word = log_prob.topk(1, dim = 1)[1].view(1, 1)\n",
    "            # add to output\n",
    "            if word.item() == EOS_token : break\n",
    "            else : answer.append(word.item())\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Machine Translation Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec_in, word2vec_out, \n",
    "                 hidden_dim = 100,\n",
    "                 n_layers_in = 1,\n",
    "                 n_layers_out = 1,\n",
    "                 bound = 25,\n",
    "                 dropout = 0, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.word2vec_in  = word2vec_in\n",
    "        self.word2vec_out = word2vec_out\n",
    "        self.context      = RecurrentEncoder(word2vec_in.output_dim, hidden_dim, n_layers_in, dropout, bidirectional = True)\n",
    "        self.decoder      = Decoder(word2vec_out, hidden_dim, n_layers_out, dropout, bound)\n",
    "        #self.decoder      = AttnDecoder(word2vec_out, word2vec_in.output_dim, hidden_dim, n_layers_out, dropout, bound)\n",
    "        # optimizer\n",
    "        self.ignore_index_in  = self.word2vec_in.lang.getIndex('PADDING_WORD')\n",
    "        self.ignore_index_out = self.word2vec_out.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, ignore_index = self.ignore_index_out)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence, color_code = '\\033[94m'):\n",
    "        # encode sentence\n",
    "        words = [self.word2vec_in.lang.getIndex(w) for w in self.tokenizer(sentence)]\n",
    "        words = [w for w in words if w is not None]\n",
    "        words = Variable(torch.LongTensor([words])).to(self.device)\n",
    "        embeddings = self.word2vec_in.embedding(words)\n",
    "        #words  = self.tokenizer(sentence)\n",
    "        #embeddings = self.word2vec_in(words, self.device)\n",
    "        embeddings, hidden  = self.context(embeddings)\n",
    "        # sum along directions\n",
    "        if self.context.bidirectional :\n",
    "            hidden = hidden.view(self.context.n_layers, 2, -1, self.context.hidden_dim)\n",
    "            hidden = torch.sum(hidden, dim = 1) # size (n_layers, batch_size, hidden_dim)\n",
    "        ## compute answer\n",
    "        answer = self.decoder(hidden, embeddings, self.device)\n",
    "        answer = [self.word2vec_out.lang.index2word[i] for i in answer]\n",
    "        answer = ' '.join(answer)\n",
    "        #print(' '.join(self.tokenizer(sentence) + [':', color_code] + answer + ['\\033[0m']))\n",
    "        return answer\n",
    "\n",
    "    def generatePackedSentences(self, sentences, batch_size = 32) : \n",
    "        sentences.sort(key = lambda s: len(s[1]), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            # prepare input and target pack\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            pack.sort(key = lambda s: len(self.tokenizer(s[0])), reverse = True)\n",
    "            pack0 = [[self.word2vec_in.lang.getIndex(w) for w in self.tokenizer(qa[0])] for qa in pack]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack1 = [[self.word2vec_out.lang.getIndex(w) for w in self.tokenizer(qa[1]) + ['EOS']] for qa in pack]\n",
    "            pack1 = [[w for w in words if w is not None] for words in pack1]\n",
    "            lengths = torch.tensor([len(p) for p in pack0])           # size (batch_size) \n",
    "            # padd packs\n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index_in))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1)) # size (batch_size, max_length0) \n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index_out))\n",
    "            pack1 = Variable(torch.LongTensor(pack1))       # WARNING : size (max_length1, batch_size) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def compute_accuracy(self, sentences) :\n",
    "        batches = self.generatePackedSentences(sentences, batch_size = 32)\n",
    "        score = 0\n",
    "        for batch, target in batches :\n",
    "            embeddings  = self.word2vec_in.embedding(batch[0].to(self.device))\n",
    "            hiddens, _  = self.context(embeddings, lengths = batch[1].to(self.device))\n",
    "            attended, _ = self.attention(hiddens)\n",
    "            if self.bin_mode : \n",
    "                vects  = self.out(attended).view(-1)\n",
    "                target = target.to(self.device).view(-1)\n",
    "                score += sum(torch.abs(target - self.act(vects)) < 0.5).item()\n",
    "            else : \n",
    "                log_probs = F.log_softmax(self.out(attended.squeeze(1)))\n",
    "                target    = target.to(self.device).view(-1)\n",
    "                score    += sum([target[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(target.size(0))])\n",
    "        return score * 100 / len(sentences)\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, tf_ratio = 0, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeSuccess(log_probs, targets) :\n",
    "            success = sum([self.ignore_index_out != targets[i].item() == log_probs[i].topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0))])\n",
    "            return success\n",
    "        \n",
    "        def computeLogProbs(batch, target, tf_ratio = 0, compute_accuracy = True) :\n",
    "            loss = 0\n",
    "            success = 0\n",
    "            forcing = (random.random() < tf_ratio)\n",
    "            # encode sentences\n",
    "            embeddings = self.word2vec_in.embedding(batch[0].to(self.device))\n",
    "            embeddings, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # size (n_layers * num_directions, batch_size, hidden_dim)\n",
    "            # sum along directions\n",
    "            if self.context.bidirectional :\n",
    "                hidden = hidden.view(self.context.n_layers, 2, -1, self.context.hidden_dim)\n",
    "                hidden = torch.sum(hidden, dim = 1)               # size (n_layers_in, batch_size, hidden_dim)\n",
    "            # compute answers\n",
    "            word_index = self.word2vec_out.lang.getIndex('SOS')\n",
    "            word_index = Variable(torch.LongTensor([word_index])) # size (1)\n",
    "            word_index = word_index.expand(target.size(1))        # size (batch_size)\n",
    "            hidden = hidden[-self.decoder.n_layers:]\n",
    "            for t in range(target.size(0)) :\n",
    "                # compute word probs\n",
    "                log_prob, hidden = self.decoder.generateWord(hidden, embeddings, word_index.unsqueeze(1).to(self.device))\n",
    "                # compute loss\n",
    "                loss += self.criterion(log_prob, target[t])\n",
    "                if compute_accuracy : success += computeSuccess(log_prob, target[t])\n",
    "                # apply teacher forcing\n",
    "                if forcing : word_index = target[t]                             # size (batch_size) \n",
    "                else       : word_index = log_prob.topk(1, dim = 1)[1].view(-1) # size (batch_size)\n",
    "            return loss, success       \n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, tf_ratio = 0, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            target = batch[1].to(self.device)\n",
    "            total = np.sum(target.data.cpu().numpy() != self.ignore_index_out)\n",
    "            loss, success = computeLogProbs(batch[0], target, tf_ratio, compute_accuracy)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return float(loss.item() / total), float(success * 100 / total)\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, tf_ratio, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, tf_ratio, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(qa[1].split(' ')) for qa in qa_trn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491697"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = EncoderDecoder(device = torch.device(\"cpu\"),\n",
    "                         tokenizer = lambda s : normalizeString(s).split(' '),\n",
    "                         word2vec_in = word2vec_in,\n",
    "                         word2vec_out = word2vec_out,\n",
    "                         hidden_dim = 100, \n",
    "                         n_layers_in = 2,\n",
    "                         n_layers_out = 2,\n",
    "                         bound = 75,\n",
    "                         dropout = 0.1,\n",
    "                         optimizer = optim.SGD)\n",
    "\n",
    "chatbot.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (word2vec_in): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(784, 75)\n",
       "    )\n",
       "    (embedding): Embedding(784, 75)\n",
       "  )\n",
       "  (word2vec_out): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(898, 75)\n",
       "    )\n",
       "    (embedding): Embedding(898, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (bigru): GRU(75, 100, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (word2vec): Word2VecConnector(\n",
       "      (twin): Word2Vec(\n",
       "        (embedding): Embedding(898, 75)\n",
       "      )\n",
       "      (embedding): Embedding(898, 75)\n",
       "    )\n",
       "    (gru): GRU(75, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (out): Linear(in_features=100, out_features=897, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6257"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = chatbot.generatePackedSentences(qa_trn, batch_size = 32)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 13s (- 14m 0s) (100 1%) loss : 5.514  accuracy : 7.9 %\n",
      "0m 27s (- 14m 0s) (200 3%) loss : 5.349  accuracy : 11.1 %\n",
      "0m 41s (- 13m 50s) (300 4%) loss : 4.387  accuracy : 22.3 %\n",
      "0m 54s (- 13m 20s) (400 6%) loss : 3.631  accuracy : 33.8 %\n",
      "1m 7s (- 12m 59s) (500 7%) loss : 3.339  accuracy : 38.9 %\n",
      "1m 21s (- 12m 45s) (600 9%) loss : 2.979  accuracy : 43.3 %\n",
      "1m 35s (- 12m 38s) (700 11%) loss : 3.194  accuracy : 40.4 %\n",
      "1m 49s (- 12m 29s) (800 12%) loss : 2.549  accuracy : 50.1 %\n",
      "2m 3s (- 12m 17s) (900 14%) loss : 2.201  accuracy : 55.5 %\n",
      "2m 17s (- 12m 0s) (1000 15%) loss : 2.333  accuracy : 53.5 %\n",
      "2m 31s (- 11m 49s) (1100 17%) loss : 2.453  accuracy : 51.8 %\n",
      "2m 44s (- 11m 34s) (1200 19%) loss : 2.128  accuracy : 56.9 %\n",
      "2m 58s (- 11m 19s) (1300 20%) loss : 1.941  accuracy : 61.2 %\n",
      "3m 11s (- 11m 3s) (1400 22%) loss : 1.999  accuracy : 59.4 %\n",
      "3m 24s (- 10m 48s) (1500 23%) loss : 1.723  accuracy : 64.1 %\n",
      "3m 38s (- 10m 35s) (1600 25%) loss : 1.569  accuracy : 66.5 %\n",
      "3m 51s (- 10m 20s) (1700 27%) loss : 1.429  accuracy : 69.7 %\n",
      "4m 5s (- 10m 7s) (1800 28%) loss : 1.547  accuracy : 67.8 %\n",
      "4m 18s (- 9m 53s) (1900 30%) loss : 1.568  accuracy : 65.7 %\n",
      "4m 32s (- 9m 40s) (2000 31%) loss : 1.464  accuracy : 68.8 %\n",
      "4m 46s (- 9m 27s) (2100 33%) loss : 1.129  accuracy : 74.9 %\n",
      "5m 1s (- 9m 16s) (2200 35%) loss : 1.235  accuracy : 72.0 %\n",
      "5m 15s (- 9m 3s) (2300 36%) loss : 1.221  accuracy : 73.2 %\n",
      "5m 28s (- 8m 48s) (2400 38%) loss : 1.346  accuracy : 71.0 %\n",
      "5m 43s (- 8m 36s) (2500 39%) loss : 0.992  accuracy : 77.4 %\n",
      "5m 58s (- 8m 24s) (2600 41%) loss : 0.986  accuracy : 77.1 %\n",
      "6m 12s (- 8m 10s) (2700 43%) loss : 1.050  accuracy : 76.5 %\n",
      "6m 26s (- 7m 57s) (2800 44%) loss : 1.032  accuracy : 76.3 %\n",
      "6m 40s (- 7m 43s) (2900 46%) loss : 0.938  accuracy : 78.5 %\n",
      "6m 54s (- 7m 30s) (3000 47%) loss : 1.020  accuracy : 76.2 %\n",
      "7m 8s (- 7m 16s) (3100 49%) loss : 1.006  accuracy : 76.7 %\n",
      "7m 22s (- 7m 3s) (3200 51%) loss : 0.869  accuracy : 79.4 %\n",
      "7m 36s (- 6m 49s) (3300 52%) loss : 0.776  accuracy : 81.8 %\n",
      "7m 50s (- 6m 35s) (3400 54%) loss : 0.952  accuracy : 77.8 %\n",
      "8m 5s (- 6m 22s) (3500 55%) loss : 0.797  accuracy : 81.2 %\n",
      "8m 19s (- 6m 8s) (3600 57%) loss : 0.838  accuracy : 80.3 %\n",
      "8m 32s (- 5m 53s) (3700 59%) loss : 0.708  accuracy : 83.2 %\n",
      "8m 46s (- 5m 40s) (3800 60%) loss : 0.558  accuracy : 86.5 %\n",
      "9m 0s (- 5m 26s) (3900 62%) loss : 0.645  accuracy : 84.4 %\n",
      "9m 14s (- 5m 13s) (4000 63%) loss : 0.575  accuracy : 86.7 %\n",
      "9m 29s (- 4m 59s) (4100 65%) loss : 0.699  accuracy : 82.8 %\n",
      "9m 44s (- 4m 46s) (4200 67%) loss : 0.598  accuracy : 85.5 %\n",
      "9m 58s (- 4m 32s) (4300 68%) loss : 0.539  accuracy : 87.1 %\n",
      "10m 12s (- 4m 18s) (4400 70%) loss : 0.649  accuracy : 84.1 %\n",
      "10m 26s (- 4m 4s) (4500 71%) loss : 0.582  accuracy : 86.2 %\n",
      "10m 39s (- 3m 50s) (4600 73%) loss : 0.527  accuracy : 87.1 %\n",
      "10m 52s (- 3m 36s) (4700 75%) loss : 0.529  accuracy : 87.3 %\n",
      "11m 5s (- 3m 22s) (4800 76%) loss : 0.508  accuracy : 87.6 %\n",
      "11m 19s (- 3m 8s) (4900 78%) loss : 0.472  accuracy : 88.9 %\n",
      "11m 33s (- 2m 54s) (5000 79%) loss : 0.407  accuracy : 90.3 %\n",
      "11m 47s (- 2m 40s) (5100 81%) loss : 0.445  accuracy : 89.1 %\n",
      "12m 0s (- 2m 26s) (5200 83%) loss : 0.435  accuracy : 89.3 %\n",
      "12m 15s (- 2m 12s) (5300 84%) loss : 0.417  accuracy : 90.2 %\n",
      "12m 28s (- 1m 58s) (5400 86%) loss : 0.390  accuracy : 90.8 %\n",
      "12m 41s (- 1m 44s) (5500 87%) loss : 0.424  accuracy : 89.6 %\n",
      "12m 55s (- 1m 30s) (5600 89%) loss : 0.340  accuracy : 91.9 %\n",
      "13m 9s (- 1m 17s) (5700 91%) loss : 0.477  accuracy : 88.7 %\n",
      "13m 23s (- 1m 3s) (5800 92%) loss : 0.375  accuracy : 90.8 %\n",
      "13m 37s (- 0m 49s) (5900 94%) loss : 0.335  accuracy : 92.1 %\n",
      "13m 50s (- 0m 35s) (6000 95%) loss : 0.399  accuracy : 90.2 %\n",
      "14m 3s (- 0m 21s) (6100 97%) loss : 0.279  accuracy : 93.4 %\n",
      "14m 16s (- 0m 7s) (6200 99%) loss : 0.308  accuracy : 92.9 %\n",
      "epoch 1\n",
      "0m 13s (- 14m 0s) (100 1%) loss : 3.552  accuracy : 55.0 %\n",
      "0m 27s (- 13m 43s) (200 3%) loss : 3.147  accuracy : 52.3 %\n",
      "0m 41s (- 13m 43s) (300 4%) loss : 2.774  accuracy : 57.0 %\n",
      "0m 55s (- 13m 26s) (400 6%) loss : 2.655  accuracy : 57.7 %\n",
      "1m 10s (- 13m 26s) (500 7%) loss : 3.006  accuracy : 51.9 %\n",
      "1m 25s (- 13m 25s) (600 9%) loss : 2.512  accuracy : 59.3 %\n",
      "1m 39s (- 13m 10s) (700 11%) loss : 1.857  accuracy : 70.6 %\n",
      "1m 53s (- 12m 55s) (800 12%) loss : 1.972  accuracy : 67.6 %\n",
      "2m 8s (- 12m 42s) (900 14%) loss : 1.999  accuracy : 67.6 %\n",
      "2m 21s (- 12m 23s) (1000 15%) loss : 2.455  accuracy : 60.4 %\n",
      "2m 35s (- 12m 10s) (1100 17%) loss : 1.786  accuracy : 69.6 %\n",
      "2m 49s (- 11m 55s) (1200 19%) loss : 1.903  accuracy : 68.2 %\n",
      "3m 4s (- 11m 43s) (1300 20%) loss : 2.445  accuracy : 59.7 %\n",
      "3m 18s (- 11m 27s) (1400 22%) loss : 2.282  accuracy : 60.6 %\n",
      "3m 33s (- 11m 16s) (1500 23%) loss : 2.102  accuracy : 64.4 %\n",
      "3m 47s (- 11m 1s) (1600 25%) loss : 1.852  accuracy : 68.9 %\n",
      "4m 2s (- 10m 49s) (1700 27%) loss : 2.144  accuracy : 63.3 %\n",
      "4m 16s (- 10m 35s) (1800 28%) loss : 1.723  accuracy : 68.6 %\n",
      "4m 30s (- 10m 20s) (1900 30%) loss : 2.099  accuracy : 62.7 %\n",
      "4m 46s (- 10m 8s) (2000 31%) loss : 2.193  accuracy : 61.7 %\n",
      "5m 0s (- 9m 55s) (2100 33%) loss : 1.992  accuracy : 65.3 %\n",
      "5m 13s (- 9m 39s) (2200 35%) loss : 1.630  accuracy : 70.7 %\n",
      "5m 27s (- 9m 22s) (2300 36%) loss : 1.781  accuracy : 67.6 %\n",
      "5m 42s (- 9m 10s) (2400 38%) loss : 1.833  accuracy : 66.6 %\n",
      "5m 56s (- 8m 55s) (2500 39%) loss : 1.779  accuracy : 68.8 %\n",
      "6m 10s (- 8m 41s) (2600 41%) loss : 1.571  accuracy : 70.0 %\n",
      "6m 25s (- 8m 28s) (2700 43%) loss : 2.256  accuracy : 59.6 %\n",
      "6m 40s (- 8m 14s) (2800 44%) loss : 1.753  accuracy : 68.0 %\n",
      "6m 55s (- 8m 1s) (2900 46%) loss : 2.095  accuracy : 61.6 %\n",
      "7m 10s (- 7m 47s) (3000 47%) loss : 2.057  accuracy : 62.5 %\n",
      "7m 23s (- 7m 31s) (3100 49%) loss : 1.314  accuracy : 76.2 %\n",
      "7m 37s (- 7m 17s) (3200 51%) loss : 1.492  accuracy : 72.9 %\n",
      "7m 52s (- 7m 3s) (3300 52%) loss : 1.414  accuracy : 73.9 %\n",
      "8m 6s (- 6m 48s) (3400 54%) loss : 1.532  accuracy : 71.2 %\n",
      "8m 20s (- 6m 34s) (3500 55%) loss : 1.661  accuracy : 70.6 %\n",
      "8m 34s (- 6m 19s) (3600 57%) loss : 1.600  accuracy : 72.0 %\n",
      "8m 47s (- 6m 4s) (3700 59%) loss : 1.288  accuracy : 76.2 %\n",
      "9m 1s (- 5m 50s) (3800 60%) loss : 1.520  accuracy : 72.5 %\n",
      "9m 15s (- 5m 35s) (3900 62%) loss : 1.130  accuracy : 79.6 %\n",
      "9m 28s (- 5m 21s) (4000 63%) loss : 1.330  accuracy : 75.9 %\n",
      "9m 43s (- 5m 7s) (4100 65%) loss : 1.462  accuracy : 71.7 %\n",
      "9m 57s (- 4m 52s) (4200 67%) loss : 1.074  accuracy : 80.0 %\n",
      "10m 11s (- 4m 38s) (4300 68%) loss : 0.976  accuracy : 82.3 %\n",
      "10m 25s (- 4m 23s) (4400 70%) loss : 1.339  accuracy : 75.0 %\n",
      "10m 38s (- 4m 9s) (4500 71%) loss : 1.007  accuracy : 81.6 %\n",
      "10m 52s (- 3m 55s) (4600 73%) loss : 1.263  accuracy : 76.9 %\n",
      "11m 7s (- 3m 41s) (4700 75%) loss : 1.369  accuracy : 74.5 %\n",
      "11m 21s (- 3m 26s) (4800 76%) loss : 1.024  accuracy : 81.9 %\n",
      "11m 36s (- 3m 12s) (4900 78%) loss : 1.065  accuracy : 80.5 %\n",
      "11m 49s (- 2m 58s) (5000 79%) loss : 0.929  accuracy : 83.6 %\n",
      "12m 3s (- 2m 44s) (5100 81%) loss : 1.264  accuracy : 76.9 %\n",
      "12m 16s (- 2m 29s) (5200 83%) loss : 1.165  accuracy : 78.6 %\n",
      "12m 30s (- 2m 15s) (5300 84%) loss : 0.933  accuracy : 82.0 %\n",
      "12m 44s (- 2m 1s) (5400 86%) loss : 1.176  accuracy : 77.4 %\n",
      "12m 58s (- 1m 47s) (5500 87%) loss : 1.040  accuracy : 80.3 %\n",
      "13m 12s (- 1m 32s) (5600 89%) loss : 1.114  accuracy : 80.0 %\n",
      "13m 24s (- 1m 18s) (5700 91%) loss : 1.048  accuracy : 81.6 %\n",
      "13m 38s (- 1m 4s) (5800 92%) loss : 0.776  accuracy : 85.4 %\n",
      "13m 53s (- 0m 50s) (5900 94%) loss : 0.957  accuracy : 82.1 %\n",
      "14m 6s (- 0m 36s) (6000 95%) loss : 0.982  accuracy : 80.8 %\n",
      "14m 20s (- 0m 22s) (6100 97%) loss : 1.194  accuracy : 78.5 %\n",
      "14m 34s (- 0m 8s) (6200 99%) loss : 0.938  accuracy : 83.0 %\n",
      "epoch 1\n",
      "0m 14s (- 14m 28s) (100 1%) loss : 0.800  accuracy : 85.7 %\n",
      "0m 28s (- 14m 18s) (200 3%) loss : 0.808  accuracy : 85.7 %\n",
      "0m 42s (- 14m 4s) (300 4%) loss : 0.832  accuracy : 85.5 %\n",
      "0m 55s (- 13m 39s) (400 6%) loss : 0.728  accuracy : 87.1 %\n",
      "1m 9s (- 13m 19s) (500 7%) loss : 0.839  accuracy : 82.9 %\n",
      "1m 22s (- 12m 57s) (600 9%) loss : 1.050  accuracy : 80.4 %\n",
      "1m 35s (- 12m 40s) (700 11%) loss : 0.702  accuracy : 86.1 %\n",
      "1m 50s (- 12m 31s) (800 12%) loss : 0.868  accuracy : 81.8 %\n",
      "2m 4s (- 12m 20s) (900 14%) loss : 0.891  accuracy : 82.9 %\n",
      "2m 18s (- 12m 8s) (1000 15%) loss : 0.742  accuracy : 86.1 %\n",
      "2m 31s (- 11m 52s) (1100 17%) loss : 0.708  accuracy : 85.8 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 45s (- 11m 35s) (1200 19%) loss : 0.633  accuracy : 87.1 %\n",
      "2m 58s (- 11m 20s) (1300 20%) loss : 0.733  accuracy : 85.7 %\n",
      "3m 11s (- 11m 5s) (1400 22%) loss : 0.643  accuracy : 88.3 %\n",
      "3m 26s (- 10m 54s) (1500 23%) loss : 0.760  accuracy : 86.2 %\n",
      "3m 40s (- 10m 41s) (1600 25%) loss : 0.829  accuracy : 82.6 %\n",
      "3m 54s (- 10m 27s) (1700 27%) loss : 0.781  accuracy : 84.5 %\n",
      "4m 8s (- 10m 16s) (1800 28%) loss : 0.918  accuracy : 81.4 %\n",
      "4m 23s (- 10m 4s) (1900 30%) loss : 0.791  accuracy : 84.9 %\n",
      "4m 37s (- 9m 51s) (2000 31%) loss : 0.786  accuracy : 85.4 %\n",
      "4m 52s (- 9m 38s) (2100 33%) loss : 0.745  accuracy : 84.9 %\n",
      "5m 6s (- 9m 25s) (2200 35%) loss : 0.577  accuracy : 89.8 %\n",
      "5m 24s (- 9m 17s) (2300 36%) loss : 0.808  accuracy : 84.9 %\n",
      "5m 40s (- 9m 6s) (2400 38%) loss : 0.764  accuracy : 84.6 %\n",
      "5m 54s (- 8m 53s) (2500 39%) loss : 0.763  accuracy : 84.7 %\n",
      "6m 9s (- 8m 39s) (2600 41%) loss : 0.608  accuracy : 88.6 %\n",
      "6m 24s (- 8m 26s) (2700 43%) loss : 0.591  accuracy : 89.0 %\n",
      "6m 39s (- 8m 13s) (2800 44%) loss : 0.820  accuracy : 84.6 %\n",
      "6m 53s (- 7m 58s) (2900 46%) loss : 0.755  accuracy : 84.7 %\n",
      "7m 7s (- 7m 44s) (3000 47%) loss : 0.768  accuracy : 84.2 %\n",
      "7m 21s (- 7m 29s) (3100 49%) loss : 0.682  accuracy : 85.8 %\n",
      "7m 35s (- 7m 14s) (3200 51%) loss : 0.607  accuracy : 88.1 %\n",
      "7m 49s (- 7m 0s) (3300 52%) loss : 0.655  accuracy : 87.7 %\n",
      "8m 3s (- 6m 45s) (3400 54%) loss : 0.843  accuracy : 83.3 %\n",
      "8m 18s (- 6m 32s) (3500 55%) loss : 0.632  accuracy : 88.9 %\n",
      "8m 33s (- 6m 18s) (3600 57%) loss : 0.729  accuracy : 86.1 %\n",
      "8m 47s (- 6m 4s) (3700 59%) loss : 0.776  accuracy : 85.3 %\n",
      "9m 1s (- 5m 50s) (3800 60%) loss : 0.679  accuracy : 86.8 %\n",
      "9m 15s (- 5m 35s) (3900 62%) loss : 0.644  accuracy : 87.4 %\n",
      "9m 29s (- 5m 21s) (4000 63%) loss : 0.749  accuracy : 85.9 %\n",
      "9m 43s (- 5m 7s) (4100 65%) loss : 0.804  accuracy : 84.0 %\n",
      "9m 58s (- 4m 53s) (4200 67%) loss : 0.618  accuracy : 87.5 %\n",
      "10m 12s (- 4m 38s) (4300 68%) loss : 0.789  accuracy : 85.5 %\n",
      "10m 27s (- 4m 24s) (4400 70%) loss : 0.599  accuracy : 88.2 %\n",
      "10m 41s (- 4m 10s) (4500 71%) loss : 0.580  accuracy : 88.7 %\n",
      "10m 56s (- 3m 56s) (4600 73%) loss : 0.619  accuracy : 86.2 %\n",
      "11m 10s (- 3m 42s) (4700 75%) loss : 0.686  accuracy : 86.3 %\n",
      "11m 23s (- 3m 27s) (4800 76%) loss : 0.717  accuracy : 86.8 %\n",
      "11m 37s (- 3m 13s) (4900 78%) loss : 0.628  accuracy : 88.3 %\n",
      "11m 52s (- 2m 59s) (5000 79%) loss : 0.621  accuracy : 87.4 %\n",
      "12m 8s (- 2m 45s) (5100 81%) loss : 0.845  accuracy : 83.8 %\n",
      "12m 22s (- 2m 30s) (5200 83%) loss : 0.622  accuracy : 88.3 %\n",
      "12m 36s (- 2m 16s) (5300 84%) loss : 0.625  accuracy : 86.3 %\n",
      "12m 52s (- 2m 2s) (5400 86%) loss : 0.689  accuracy : 86.4 %\n",
      "13m 6s (- 1m 48s) (5500 87%) loss : 0.756  accuracy : 84.7 %\n",
      "13m 20s (- 1m 33s) (5600 89%) loss : 0.549  accuracy : 89.8 %\n",
      "13m 34s (- 1m 19s) (5700 91%) loss : 0.568  accuracy : 89.0 %\n",
      "13m 47s (- 1m 5s) (5800 92%) loss : 0.503  accuracy : 90.2 %\n",
      "14m 1s (- 0m 50s) (5900 94%) loss : 0.751  accuracy : 84.8 %\n",
      "14m 14s (- 0m 36s) (6000 95%) loss : 0.548  accuracy : 88.7 %\n",
      "14m 27s (- 0m 22s) (6100 97%) loss : 0.693  accuracy : 86.9 %\n",
      "14m 43s (- 0m 8s) (6200 99%) loss : 0.494  accuracy : 90.9 %\n",
      "epoch 1\n",
      "0m 13s (- 14m 14s) (100 1%) loss : 0.769  accuracy : 84.3 %\n",
      "0m 28s (- 14m 24s) (200 3%) loss : 0.743  accuracy : 85.8 %\n",
      "0m 41s (- 13m 52s) (300 4%) loss : 0.758  accuracy : 84.6 %\n",
      "0m 55s (- 13m 38s) (400 6%) loss : 0.640  accuracy : 87.7 %\n",
      "1m 10s (- 13m 29s) (500 7%) loss : 0.660  accuracy : 86.2 %\n",
      "1m 24s (- 13m 14s) (600 9%) loss : 0.618  accuracy : 86.7 %\n",
      "1m 38s (- 13m 4s) (700 11%) loss : 0.736  accuracy : 83.6 %\n",
      "1m 53s (- 12m 53s) (800 12%) loss : 0.758  accuracy : 84.4 %\n",
      "2m 7s (- 12m 37s) (900 14%) loss : 0.781  accuracy : 83.4 %\n",
      "2m 21s (- 12m 22s) (1000 15%) loss : 0.750  accuracy : 84.1 %\n",
      "2m 35s (- 12m 7s) (1100 17%) loss : 0.762  accuracy : 82.9 %\n",
      "2m 49s (- 11m 56s) (1200 19%) loss : 0.677  accuracy : 86.2 %\n",
      "3m 3s (- 11m 39s) (1300 20%) loss : 0.617  accuracy : 86.8 %\n",
      "3m 17s (- 11m 24s) (1400 22%) loss : 0.777  accuracy : 84.3 %\n",
      "3m 31s (- 11m 10s) (1500 23%) loss : 0.768  accuracy : 83.6 %\n",
      "3m 45s (- 10m 57s) (1600 25%) loss : 0.576  accuracy : 88.8 %\n",
      "4m 0s (- 10m 43s) (1700 27%) loss : 0.731  accuracy : 83.9 %\n",
      "4m 14s (- 10m 30s) (1800 28%) loss : 0.920  accuracy : 80.9 %\n",
      "4m 28s (- 10m 15s) (1900 30%) loss : 0.591  accuracy : 88.6 %\n",
      "4m 44s (- 10m 5s) (2000 31%) loss : 0.798  accuracy : 82.6 %\n",
      "4m 59s (- 9m 52s) (2100 33%) loss : 0.687  accuracy : 85.9 %\n",
      "5m 13s (- 9m 38s) (2200 35%) loss : 0.710  accuracy : 85.5 %\n",
      "5m 27s (- 9m 23s) (2300 36%) loss : 0.784  accuracy : 84.4 %\n",
      "5m 42s (- 9m 9s) (2400 38%) loss : 0.773  accuracy : 84.0 %\n",
      "5m 55s (- 8m 54s) (2500 39%) loss : 0.737  accuracy : 84.7 %\n",
      "6m 10s (- 8m 40s) (2600 41%) loss : 0.632  accuracy : 86.3 %\n",
      "6m 24s (- 8m 26s) (2700 43%) loss : 0.644  accuracy : 86.8 %\n",
      "6m 38s (- 8m 12s) (2800 44%) loss : 0.663  accuracy : 86.0 %\n",
      "6m 53s (- 7m 58s) (2900 46%) loss : 0.625  accuracy : 87.1 %\n",
      "7m 7s (- 7m 44s) (3000 47%) loss : 0.717  accuracy : 85.5 %\n",
      "7m 21s (- 7m 29s) (3100 49%) loss : 0.765  accuracy : 83.0 %\n",
      "7m 34s (- 7m 13s) (3200 51%) loss : 0.721  accuracy : 85.2 %\n",
      "7m 48s (- 7m 0s) (3300 52%) loss : 0.811  accuracy : 82.2 %\n",
      "8m 2s (- 6m 45s) (3400 54%) loss : 0.655  accuracy : 86.7 %\n",
      "8m 17s (- 6m 31s) (3500 55%) loss : 0.727  accuracy : 84.5 %\n",
      "8m 32s (- 6m 18s) (3600 57%) loss : 0.742  accuracy : 85.1 %\n",
      "8m 46s (- 6m 4s) (3700 59%) loss : 0.680  accuracy : 84.8 %\n",
      "9m 1s (- 5m 49s) (3800 60%) loss : 0.625  accuracy : 86.5 %\n",
      "9m 14s (- 5m 35s) (3900 62%) loss : 0.581  accuracy : 87.4 %\n",
      "9m 28s (- 5m 20s) (4000 63%) loss : 0.612  accuracy : 87.1 %\n",
      "9m 44s (- 5m 7s) (4100 65%) loss : 0.678  accuracy : 85.4 %\n",
      "9m 58s (- 4m 53s) (4200 67%) loss : 0.665  accuracy : 86.8 %\n",
      "10m 12s (- 4m 38s) (4300 68%) loss : 0.649  accuracy : 86.6 %\n",
      "10m 25s (- 4m 24s) (4400 70%) loss : 0.720  accuracy : 85.4 %\n",
      "10m 40s (- 4m 9s) (4500 71%) loss : 0.668  accuracy : 86.7 %\n",
      "10m 53s (- 3m 55s) (4600 73%) loss : 0.517  accuracy : 89.8 %\n",
      "11m 6s (- 3m 40s) (4700 75%) loss : 0.741  accuracy : 85.1 %\n",
      "11m 21s (- 3m 26s) (4800 76%) loss : 0.796  accuracy : 83.7 %\n",
      "11m 34s (- 3m 12s) (4900 78%) loss : 0.732  accuracy : 84.3 %\n",
      "11m 49s (- 2m 58s) (5000 79%) loss : 0.766  accuracy : 82.6 %\n",
      "12m 3s (- 2m 44s) (5100 81%) loss : 0.662  accuracy : 86.8 %\n",
      "12m 18s (- 2m 30s) (5200 83%) loss : 0.675  accuracy : 86.6 %\n",
      "12m 32s (- 2m 15s) (5300 84%) loss : 0.705  accuracy : 86.0 %\n",
      "12m 47s (- 2m 1s) (5400 86%) loss : 0.579  accuracy : 88.0 %\n",
      "13m 1s (- 1m 47s) (5500 87%) loss : 0.725  accuracy : 83.2 %\n",
      "13m 15s (- 1m 33s) (5600 89%) loss : 0.673  accuracy : 84.2 %\n",
      "13m 31s (- 1m 19s) (5700 91%) loss : 0.750  accuracy : 84.6 %\n",
      "13m 45s (- 1m 5s) (5800 92%) loss : 0.671  accuracy : 85.4 %\n",
      "13m 58s (- 0m 50s) (5900 94%) loss : 0.591  accuracy : 87.4 %\n",
      "14m 11s (- 0m 36s) (6000 95%) loss : 0.690  accuracy : 85.3 %\n",
      "14m 24s (- 0m 22s) (6100 97%) loss : 0.657  accuracy : 86.8 %\n",
      "14m 39s (- 0m 8s) (6200 99%) loss : 0.630  accuracy : 87.1 %\n"
     ]
    }
   ],
   "source": [
    "chatbot.fit(batches, epochs = 1, lr = 0.001, tf_ratio = 1,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.001, tf_ratio = 0.5,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.00025, tf_ratio = 0.5,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.0001, tf_ratio = 0.35,  print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 14s (- 14m 43s) (100 1%) loss : 5.612  accuracy : 7.6 %\n",
      "0m 27s (- 13m 41s) (200 3%) loss : 4.905  accuracy : 11.3 %\n",
      "0m 40s (- 13m 29s) (300 4%) loss : 4.558  accuracy : 17.3 %\n",
      "0m 54s (- 13m 15s) (400 6%) loss : 4.116  accuracy : 26.0 %\n",
      "1m 8s (- 13m 8s) (500 7%) loss : 3.581  accuracy : 34.3 %\n",
      "1m 21s (- 12m 50s) (600 9%) loss : 3.561  accuracy : 33.5 %\n",
      "1m 34s (- 12m 33s) (700 11%) loss : 2.933  accuracy : 44.1 %\n",
      "1m 48s (- 12m 17s) (800 12%) loss : 2.642  accuracy : 47.7 %\n",
      "2m 2s (- 12m 7s) (900 14%) loss : 2.546  accuracy : 49.7 %\n",
      "2m 15s (- 11m 52s) (1000 15%) loss : 2.439  accuracy : 52.0 %\n",
      "2m 28s (- 11m 38s) (1100 17%) loss : 2.028  accuracy : 59.0 %\n",
      "2m 43s (- 11m 29s) (1200 19%) loss : 1.980  accuracy : 59.7 %\n",
      "2m 57s (- 11m 15s) (1300 20%) loss : 1.695  accuracy : 64.0 %\n",
      "3m 10s (- 10m 59s) (1400 22%) loss : 1.664  accuracy : 65.1 %\n",
      "3m 23s (- 10m 44s) (1500 23%) loss : 1.507  accuracy : 67.6 %\n",
      "3m 36s (- 10m 30s) (1600 25%) loss : 1.513  accuracy : 66.6 %\n",
      "3m 49s (- 10m 15s) (1700 27%) loss : 1.311  accuracy : 71.0 %\n",
      "4m 4s (- 10m 5s) (1800 28%) loss : 1.439  accuracy : 69.4 %\n",
      "4m 19s (- 9m 54s) (1900 30%) loss : 1.262  accuracy : 72.6 %\n",
      "4m 33s (- 9m 42s) (2000 31%) loss : 1.219  accuracy : 73.3 %\n",
      "4m 47s (- 9m 30s) (2100 33%) loss : 1.288  accuracy : 71.9 %\n",
      "5m 1s (- 9m 16s) (2200 35%) loss : 1.139  accuracy : 74.7 %\n",
      "5m 14s (- 9m 0s) (2300 36%) loss : 1.090  accuracy : 75.9 %\n",
      "5m 28s (- 8m 47s) (2400 38%) loss : 1.086  accuracy : 75.7 %\n",
      "5m 41s (- 8m 33s) (2500 39%) loss : 0.989  accuracy : 76.2 %\n",
      "5m 55s (- 8m 19s) (2600 41%) loss : 0.970  accuracy : 77.5 %\n",
      "6m 8s (- 8m 5s) (2700 43%) loss : 0.901  accuracy : 79.4 %\n",
      "6m 21s (- 7m 51s) (2800 44%) loss : 0.788  accuracy : 81.4 %\n",
      "6m 35s (- 7m 37s) (2900 46%) loss : 0.730  accuracy : 83.0 %\n",
      "6m 48s (- 7m 23s) (3000 47%) loss : 0.739  accuracy : 82.4 %\n",
      "7m 3s (- 7m 10s) (3100 49%) loss : 0.586  accuracy : 86.0 %\n",
      "7m 15s (- 6m 56s) (3200 51%) loss : 0.656  accuracy : 84.2 %\n",
      "7m 28s (- 6m 42s) (3300 52%) loss : 0.618  accuracy : 84.9 %\n",
      "7m 42s (- 6m 28s) (3400 54%) loss : 0.666  accuracy : 84.0 %\n",
      "7m 56s (- 6m 14s) (3500 55%) loss : 0.716  accuracy : 83.1 %\n",
      "8m 9s (- 6m 1s) (3600 57%) loss : 0.664  accuracy : 84.0 %\n",
      "8m 23s (- 5m 47s) (3700 59%) loss : 0.637  accuracy : 85.0 %\n",
      "8m 36s (- 5m 33s) (3800 60%) loss : 0.685  accuracy : 82.9 %\n",
      "8m 51s (- 5m 20s) (3900 62%) loss : 0.483  accuracy : 88.1 %\n",
      "9m 4s (- 5m 7s) (4000 63%) loss : 0.441  accuracy : 89.2 %\n",
      "9m 18s (- 4m 53s) (4100 65%) loss : 0.537  accuracy : 86.7 %\n",
      "9m 33s (- 4m 40s) (4200 67%) loss : 0.445  accuracy : 88.9 %\n",
      "9m 48s (- 4m 27s) (4300 68%) loss : 0.481  accuracy : 88.7 %\n",
      "10m 1s (- 4m 13s) (4400 70%) loss : 0.475  accuracy : 88.5 %\n",
      "10m 15s (- 4m 0s) (4500 71%) loss : 0.420  accuracy : 89.8 %\n",
      "10m 28s (- 3m 46s) (4600 73%) loss : 0.465  accuracy : 88.7 %\n",
      "10m 42s (- 3m 32s) (4700 75%) loss : 0.368  accuracy : 90.8 %\n",
      "10m 55s (- 3m 19s) (4800 76%) loss : 0.442  accuracy : 88.8 %\n",
      "11m 9s (- 3m 5s) (4900 78%) loss : 0.328  accuracy : 92.1 %\n",
      "11m 23s (- 2m 51s) (5000 79%) loss : 0.411  accuracy : 89.8 %\n",
      "11m 36s (- 2m 37s) (5100 81%) loss : 0.364  accuracy : 91.1 %\n",
      "11m 50s (- 2m 24s) (5200 83%) loss : 0.340  accuracy : 91.1 %\n",
      "12m 3s (- 2m 10s) (5300 84%) loss : 0.364  accuracy : 91.1 %\n",
      "12m 16s (- 1m 56s) (5400 86%) loss : 0.306  accuracy : 92.3 %\n",
      "12m 29s (- 1m 43s) (5500 87%) loss : 0.338  accuracy : 91.7 %\n",
      "12m 43s (- 1m 29s) (5600 89%) loss : 0.279  accuracy : 92.9 %\n",
      "12m 55s (- 1m 15s) (5700 91%) loss : 0.284  accuracy : 93.1 %\n",
      "13m 8s (- 1m 2s) (5800 92%) loss : 0.274  accuracy : 92.9 %\n",
      "13m 21s (- 0m 48s) (5900 94%) loss : 0.254  accuracy : 93.9 %\n",
      "13m 35s (- 0m 34s) (6000 95%) loss : 0.286  accuracy : 92.7 %\n",
      "13m 49s (- 0m 21s) (6100 97%) loss : 0.263  accuracy : 93.3 %\n",
      "14m 2s (- 0m 7s) (6200 99%) loss : 0.259  accuracy : 93.7 %\n",
      "epoch 1\n",
      "0m 13s (- 14m 5s) (100 1%) loss : 2.374  accuracy : 71.8 %\n",
      "0m 28s (- 14m 23s) (200 3%) loss : 2.801  accuracy : 59.9 %\n",
      "0m 41s (- 13m 52s) (300 4%) loss : 2.584  accuracy : 60.5 %\n",
      "0m 56s (- 13m 40s) (400 6%) loss : 2.438  accuracy : 60.4 %\n",
      "1m 9s (- 13m 25s) (500 7%) loss : 2.055  accuracy : 66.4 %\n",
      "1m 24s (- 13m 12s) (600 9%) loss : 1.631  accuracy : 74.2 %\n",
      "1m 37s (- 12m 57s) (700 11%) loss : 1.464  accuracy : 76.3 %\n",
      "1m 51s (- 12m 39s) (800 12%) loss : 1.413  accuracy : 77.2 %\n",
      "2m 4s (- 12m 23s) (900 14%) loss : 1.856  accuracy : 70.6 %\n",
      "2m 17s (- 12m 3s) (1000 15%) loss : 1.690  accuracy : 72.0 %\n",
      "2m 31s (- 11m 48s) (1100 17%) loss : 1.785  accuracy : 70.2 %\n",
      "2m 44s (- 11m 33s) (1200 19%) loss : 1.899  accuracy : 68.1 %\n",
      "2m 59s (- 11m 24s) (1300 20%) loss : 1.637  accuracy : 71.1 %\n",
      "3m 14s (- 11m 13s) (1400 22%) loss : 1.479  accuracy : 75.3 %\n",
      "3m 28s (- 11m 1s) (1500 23%) loss : 1.499  accuracy : 74.5 %\n",
      "3m 41s (- 10m 45s) (1600 25%) loss : 1.590  accuracy : 73.2 %\n",
      "3m 55s (- 10m 31s) (1700 27%) loss : 1.437  accuracy : 74.4 %\n",
      "4m 8s (- 10m 14s) (1800 28%) loss : 1.173  accuracy : 79.7 %\n",
      "4m 21s (- 10m 0s) (1900 30%) loss : 1.111  accuracy : 80.0 %\n",
      "4m 35s (- 9m 46s) (2000 31%) loss : 1.611  accuracy : 73.3 %\n",
      "4m 50s (- 9m 34s) (2100 33%) loss : 1.438  accuracy : 74.5 %\n",
      "5m 2s (- 9m 18s) (2200 35%) loss : 1.145  accuracy : 80.0 %\n",
      "5m 16s (- 9m 4s) (2300 36%) loss : 1.201  accuracy : 79.5 %\n",
      "5m 29s (- 8m 50s) (2400 38%) loss : 1.187  accuracy : 80.1 %\n",
      "5m 43s (- 8m 36s) (2500 39%) loss : 1.292  accuracy : 76.9 %\n",
      "5m 57s (- 8m 23s) (2600 41%) loss : 1.326  accuracy : 76.3 %\n",
      "6m 11s (- 8m 9s) (2700 43%) loss : 1.596  accuracy : 70.8 %\n",
      "6m 24s (- 7m 54s) (2800 44%) loss : 1.209  accuracy : 78.8 %\n",
      "6m 38s (- 7m 41s) (2900 46%) loss : 1.579  accuracy : 71.4 %\n",
      "6m 52s (- 7m 27s) (3000 47%) loss : 1.371  accuracy : 75.1 %\n",
      "7m 5s (- 7m 13s) (3100 49%) loss : 1.035  accuracy : 81.9 %\n",
      "7m 19s (- 6m 59s) (3200 51%) loss : 1.227  accuracy : 77.2 %\n",
      "7m 33s (- 6m 46s) (3300 52%) loss : 1.220  accuracy : 78.9 %\n",
      "7m 48s (- 6m 33s) (3400 54%) loss : 1.260  accuracy : 77.1 %\n",
      "8m 1s (- 6m 19s) (3500 55%) loss : 1.136  accuracy : 79.2 %\n",
      "8m 15s (- 6m 5s) (3600 57%) loss : 0.929  accuracy : 82.7 %\n",
      "8m 29s (- 5m 52s) (3700 59%) loss : 1.021  accuracy : 81.2 %\n",
      "8m 42s (- 5m 38s) (3800 60%) loss : 1.013  accuracy : 80.3 %\n",
      "8m 57s (- 5m 25s) (3900 62%) loss : 1.005  accuracy : 81.8 %\n",
      "9m 10s (- 5m 10s) (4000 63%) loss : 0.807  accuracy : 85.6 %\n",
      "9m 24s (- 4m 57s) (4100 65%) loss : 1.002  accuracy : 81.7 %\n",
      "9m 38s (- 4m 43s) (4200 67%) loss : 1.034  accuracy : 81.6 %\n",
      "9m 51s (- 4m 29s) (4300 68%) loss : 1.037  accuracy : 81.1 %\n",
      "10m 5s (- 4m 15s) (4400 70%) loss : 0.781  accuracy : 86.3 %\n",
      "10m 18s (- 4m 1s) (4500 71%) loss : 0.903  accuracy : 83.6 %\n",
      "10m 32s (- 3m 47s) (4600 73%) loss : 0.840  accuracy : 83.9 %\n",
      "10m 45s (- 3m 33s) (4700 75%) loss : 1.004  accuracy : 81.3 %\n",
      "10m 59s (- 3m 20s) (4800 76%) loss : 1.068  accuracy : 79.5 %\n",
      "11m 13s (- 3m 6s) (4900 78%) loss : 0.896  accuracy : 82.2 %\n",
      "11m 26s (- 2m 52s) (5000 79%) loss : 0.842  accuracy : 83.5 %\n",
      "11m 40s (- 2m 38s) (5100 81%) loss : 1.101  accuracy : 76.0 %\n",
      "11m 53s (- 2m 25s) (5200 83%) loss : 0.704  accuracy : 86.7 %\n",
      "12m 7s (- 2m 11s) (5300 84%) loss : 0.901  accuracy : 83.0 %\n",
      "12m 21s (- 1m 57s) (5400 86%) loss : 0.811  accuracy : 84.1 %\n",
      "12m 35s (- 1m 44s) (5500 87%) loss : 0.902  accuracy : 81.7 %\n",
      "12m 51s (- 1m 30s) (5600 89%) loss : 0.782  accuracy : 84.0 %\n",
      "13m 5s (- 1m 16s) (5700 91%) loss : 0.921  accuracy : 83.2 %\n",
      "13m 19s (- 1m 2s) (5800 92%) loss : 0.755  accuracy : 85.3 %\n",
      "13m 33s (- 0m 49s) (5900 94%) loss : 0.769  accuracy : 85.9 %\n",
      "13m 47s (- 0m 35s) (6000 95%) loss : 0.682  accuracy : 86.9 %\n",
      "14m 1s (- 0m 21s) (6100 97%) loss : 0.817  accuracy : 84.5 %\n",
      "14m 14s (- 0m 7s) (6200 99%) loss : 0.620  accuracy : 87.8 %\n",
      "epoch 1\n",
      "0m 13s (- 14m 19s) (100 1%) loss : 0.644  accuracy : 87.5 %\n",
      "0m 27s (- 13m 49s) (200 3%) loss : 0.707  accuracy : 85.7 %\n",
      "0m 40s (- 13m 29s) (300 4%) loss : 0.544  accuracy : 89.7 %\n",
      "0m 54s (- 13m 19s) (400 6%) loss : 0.609  accuracy : 87.7 %\n",
      "1m 7s (- 12m 55s) (500 7%) loss : 0.409  accuracy : 91.9 %\n",
      "1m 21s (- 12m 48s) (600 9%) loss : 0.560  accuracy : 89.9 %\n",
      "1m 34s (- 12m 33s) (700 11%) loss : 0.667  accuracy : 85.3 %\n",
      "1m 49s (- 12m 25s) (800 12%) loss : 0.527  accuracy : 88.1 %\n",
      "2m 2s (- 12m 11s) (900 14%) loss : 0.527  accuracy : 89.3 %\n",
      "2m 16s (- 11m 59s) (1000 15%) loss : 0.511  accuracy : 90.5 %\n",
      "2m 30s (- 11m 47s) (1100 17%) loss : 0.676  accuracy : 84.8 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 44s (- 11m 33s) (1200 19%) loss : 0.458  accuracy : 89.8 %\n",
      "2m 57s (- 11m 17s) (1300 20%) loss : 0.460  accuracy : 90.7 %\n",
      "3m 11s (- 11m 3s) (1400 22%) loss : 0.486  accuracy : 90.7 %\n",
      "3m 25s (- 10m 51s) (1500 23%) loss : 0.442  accuracy : 91.6 %\n",
      "3m 38s (- 10m 36s) (1600 25%) loss : 0.611  accuracy : 88.6 %\n",
      "3m 52s (- 10m 22s) (1700 27%) loss : 0.581  accuracy : 88.1 %\n",
      "4m 6s (- 10m 9s) (1800 28%) loss : 0.533  accuracy : 89.3 %\n",
      "4m 19s (- 9m 55s) (1900 30%) loss : 0.504  accuracy : 90.5 %\n",
      "4m 33s (- 9m 42s) (2000 31%) loss : 0.504  accuracy : 90.8 %\n",
      "4m 48s (- 9m 30s) (2100 33%) loss : 0.504  accuracy : 90.4 %\n",
      "5m 1s (- 9m 16s) (2200 35%) loss : 0.591  accuracy : 87.5 %\n",
      "5m 15s (- 9m 2s) (2300 36%) loss : 0.472  accuracy : 91.1 %\n",
      "5m 30s (- 8m 50s) (2400 38%) loss : 0.588  accuracy : 88.1 %\n",
      "5m 42s (- 8m 35s) (2500 39%) loss : 0.582  accuracy : 88.0 %\n",
      "5m 55s (- 8m 20s) (2600 41%) loss : 0.518  accuracy : 89.4 %\n",
      "6m 9s (- 8m 6s) (2700 43%) loss : 0.493  accuracy : 89.3 %\n",
      "6m 22s (- 7m 52s) (2800 44%) loss : 0.392  accuracy : 92.4 %\n",
      "6m 36s (- 7m 38s) (2900 46%) loss : 0.445  accuracy : 91.2 %\n",
      "6m 50s (- 7m 26s) (3000 47%) loss : 0.515  accuracy : 90.2 %\n",
      "7m 4s (- 7m 12s) (3100 49%) loss : 0.509  accuracy : 88.7 %\n",
      "7m 19s (- 6m 59s) (3200 51%) loss : 0.540  accuracy : 87.7 %\n",
      "7m 33s (- 6m 46s) (3300 52%) loss : 0.464  accuracy : 90.7 %\n",
      "7m 47s (- 6m 32s) (3400 54%) loss : 0.538  accuracy : 88.8 %\n",
      "8m 0s (- 6m 18s) (3500 55%) loss : 0.564  accuracy : 87.8 %\n",
      "8m 14s (- 6m 5s) (3600 57%) loss : 0.466  accuracy : 91.0 %\n",
      "8m 28s (- 5m 51s) (3700 59%) loss : 0.533  accuracy : 89.2 %\n",
      "8m 43s (- 5m 38s) (3800 60%) loss : 0.547  accuracy : 87.7 %\n",
      "8m 57s (- 5m 24s) (3900 62%) loss : 0.480  accuracy : 90.1 %\n",
      "9m 11s (- 5m 11s) (4000 63%) loss : 0.590  accuracy : 86.7 %\n",
      "9m 25s (- 4m 57s) (4100 65%) loss : 0.431  accuracy : 92.2 %\n",
      "9m 38s (- 4m 43s) (4200 67%) loss : 0.370  accuracy : 92.1 %\n",
      "9m 53s (- 4m 30s) (4300 68%) loss : 0.562  accuracy : 87.7 %\n",
      "10m 5s (- 4m 15s) (4400 70%) loss : 0.471  accuracy : 90.3 %\n",
      "10m 20s (- 4m 2s) (4500 71%) loss : 0.471  accuracy : 91.2 %\n",
      "10m 33s (- 3m 48s) (4600 73%) loss : 0.504  accuracy : 89.1 %\n",
      "10m 47s (- 3m 34s) (4700 75%) loss : 0.389  accuracy : 92.6 %\n",
      "11m 1s (- 3m 20s) (4800 76%) loss : 0.438  accuracy : 90.8 %\n",
      "11m 15s (- 3m 7s) (4900 78%) loss : 0.491  accuracy : 88.4 %\n",
      "11m 29s (- 2m 53s) (5000 79%) loss : 0.499  accuracy : 89.0 %\n",
      "11m 42s (- 2m 39s) (5100 81%) loss : 0.418  accuracy : 91.5 %\n",
      "11m 56s (- 2m 25s) (5200 83%) loss : 0.515  accuracy : 88.6 %\n",
      "12m 9s (- 2m 11s) (5300 84%) loss : 0.497  accuracy : 88.6 %\n",
      "12m 23s (- 1m 57s) (5400 86%) loss : 0.368  accuracy : 93.7 %\n",
      "12m 37s (- 1m 44s) (5500 87%) loss : 0.505  accuracy : 87.9 %\n",
      "12m 50s (- 1m 30s) (5600 89%) loss : 0.482  accuracy : 89.1 %\n",
      "13m 4s (- 1m 16s) (5700 91%) loss : 0.512  accuracy : 88.4 %\n",
      "13m 17s (- 1m 2s) (5800 92%) loss : 0.552  accuracy : 87.4 %\n",
      "13m 31s (- 0m 49s) (5900 94%) loss : 0.385  accuracy : 92.1 %\n",
      "13m 45s (- 0m 35s) (6000 95%) loss : 0.453  accuracy : 89.6 %\n",
      "13m 59s (- 0m 21s) (6100 97%) loss : 0.432  accuracy : 89.8 %\n",
      "14m 13s (- 0m 7s) (6200 99%) loss : 0.448  accuracy : 91.4 %\n",
      "epoch 1\n",
      "0m 14s (- 14m 48s) (100 1%) loss : 0.418  accuracy : 92.0 %\n",
      "0m 27s (- 13m 49s) (200 3%) loss : 0.489  accuracy : 88.3 %\n",
      "0m 40s (- 13m 27s) (300 4%) loss : 0.560  accuracy : 87.1 %\n",
      "0m 54s (- 13m 12s) (400 6%) loss : 0.535  accuracy : 87.0 %\n",
      "1m 7s (- 13m 1s) (500 7%) loss : 0.484  accuracy : 89.3 %\n",
      "1m 22s (- 12m 53s) (600 9%) loss : 0.408  accuracy : 92.4 %\n",
      "1m 36s (- 12m 48s) (700 11%) loss : 0.474  accuracy : 89.4 %\n",
      "1m 50s (- 12m 32s) (800 12%) loss : 0.502  accuracy : 88.3 %\n",
      "2m 3s (- 12m 13s) (900 14%) loss : 0.473  accuracy : 88.4 %\n",
      "2m 16s (- 11m 59s) (1000 15%) loss : 0.492  accuracy : 89.5 %\n",
      "2m 31s (- 11m 50s) (1100 17%) loss : 0.504  accuracy : 88.4 %\n",
      "2m 46s (- 11m 41s) (1200 19%) loss : 0.404  accuracy : 91.3 %\n",
      "3m 0s (- 11m 28s) (1300 20%) loss : 0.505  accuracy : 87.5 %\n",
      "3m 13s (- 11m 12s) (1400 22%) loss : 0.530  accuracy : 87.7 %\n",
      "3m 27s (- 10m 58s) (1500 23%) loss : 0.483  accuracy : 88.5 %\n",
      "3m 40s (- 10m 42s) (1600 25%) loss : 0.523  accuracy : 87.1 %\n",
      "3m 54s (- 10m 27s) (1700 27%) loss : 0.467  accuracy : 90.5 %\n",
      "4m 8s (- 10m 16s) (1800 28%) loss : 0.527  accuracy : 89.5 %\n",
      "4m 22s (- 10m 2s) (1900 30%) loss : 0.424  accuracy : 90.7 %\n",
      "4m 36s (- 9m 48s) (2000 31%) loss : 0.556  accuracy : 85.8 %\n",
      "4m 49s (- 9m 33s) (2100 33%) loss : 0.539  accuracy : 86.3 %\n",
      "5m 3s (- 9m 19s) (2200 35%) loss : 0.458  accuracy : 89.0 %\n",
      "5m 17s (- 9m 5s) (2300 36%) loss : 0.471  accuracy : 89.0 %\n",
      "5m 30s (- 8m 51s) (2400 38%) loss : 0.428  accuracy : 90.3 %\n",
      "5m 43s (- 8m 36s) (2500 39%) loss : 0.484  accuracy : 88.0 %\n",
      "5m 57s (- 8m 22s) (2600 41%) loss : 0.464  accuracy : 88.2 %\n",
      "6m 10s (- 8m 8s) (2700 43%) loss : 0.405  accuracy : 91.1 %\n",
      "6m 25s (- 7m 55s) (2800 44%) loss : 0.531  accuracy : 86.5 %\n",
      "6m 38s (- 7m 41s) (2900 46%) loss : 0.511  accuracy : 87.5 %\n",
      "6m 51s (- 7m 26s) (3000 47%) loss : 0.528  accuracy : 86.6 %\n",
      "7m 5s (- 7m 12s) (3100 49%) loss : 0.511  accuracy : 88.8 %\n",
      "7m 19s (- 6m 59s) (3200 51%) loss : 0.515  accuracy : 87.7 %\n",
      "7m 32s (- 6m 45s) (3300 52%) loss : 0.460  accuracy : 89.8 %\n",
      "7m 45s (- 6m 31s) (3400 54%) loss : 0.474  accuracy : 88.5 %\n",
      "7m 59s (- 6m 17s) (3500 55%) loss : 0.411  accuracy : 90.2 %\n",
      "8m 13s (- 6m 3s) (3600 57%) loss : 0.420  accuracy : 90.7 %\n",
      "8m 26s (- 5m 49s) (3700 59%) loss : 0.615  accuracy : 86.0 %\n",
      "8m 40s (- 5m 36s) (3800 60%) loss : 0.429  accuracy : 89.8 %\n",
      "8m 54s (- 5m 22s) (3900 62%) loss : 0.549  accuracy : 88.0 %\n",
      "9m 7s (- 5m 8s) (4000 63%) loss : 0.514  accuracy : 88.2 %\n",
      "9m 21s (- 4m 55s) (4100 65%) loss : 0.489  accuracy : 88.1 %\n",
      "9m 33s (- 4m 41s) (4200 67%) loss : 0.485  accuracy : 88.7 %\n",
      "9m 47s (- 4m 27s) (4300 68%) loss : 0.489  accuracy : 88.6 %\n",
      "10m 0s (- 4m 13s) (4400 70%) loss : 0.530  accuracy : 87.9 %\n",
      "10m 14s (- 4m 0s) (4500 71%) loss : 0.517  accuracy : 88.9 %\n",
      "10m 28s (- 3m 46s) (4600 73%) loss : 0.601  accuracy : 84.8 %\n",
      "10m 43s (- 3m 33s) (4700 75%) loss : 0.424  accuracy : 90.0 %\n",
      "10m 57s (- 3m 19s) (4800 76%) loss : 0.519  accuracy : 88.8 %\n",
      "11m 11s (- 3m 5s) (4900 78%) loss : 0.485  accuracy : 88.7 %\n",
      "11m 23s (- 2m 51s) (5000 79%) loss : 0.405  accuracy : 90.5 %\n",
      "11m 37s (- 2m 38s) (5100 81%) loss : 0.442  accuracy : 90.7 %\n",
      "11m 50s (- 2m 24s) (5200 83%) loss : 0.471  accuracy : 88.2 %\n",
      "12m 4s (- 2m 10s) (5300 84%) loss : 0.467  accuracy : 88.8 %\n",
      "12m 18s (- 1m 57s) (5400 86%) loss : 0.525  accuracy : 88.1 %\n",
      "12m 31s (- 1m 43s) (5500 87%) loss : 0.450  accuracy : 90.0 %\n",
      "12m 44s (- 1m 29s) (5600 89%) loss : 0.456  accuracy : 89.1 %\n",
      "12m 58s (- 1m 16s) (5700 91%) loss : 0.454  accuracy : 90.6 %\n",
      "13m 11s (- 1m 2s) (5800 92%) loss : 0.427  accuracy : 90.6 %\n",
      "13m 24s (- 0m 48s) (5900 94%) loss : 0.492  accuracy : 89.5 %\n",
      "13m 39s (- 0m 35s) (6000 95%) loss : 0.537  accuracy : 86.1 %\n",
      "13m 53s (- 0m 21s) (6100 97%) loss : 0.485  accuracy : 89.3 %\n",
      "14m 7s (- 0m 7s) (6200 99%) loss : 0.413  accuracy : 91.1 %\n"
     ]
    }
   ],
   "source": [
    "chatbot.fit(batches, epochs = 1, lr = 0.001, tf_ratio = 1,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.001, tf_ratio = 0.5,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.00025, tf_ratio = 0.5,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.0001, tf_ratio = 0.35,  print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(chatbot.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_II2_encoder_attndecoder.pth')\n",
    "\n",
    "# load\n",
    "#chatbot.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_II2_encoder_decoder.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceMotVar(motsVar, raw_sentence):\n",
    "    sentence = []\n",
    "    word_list = raw_sentence.split(' ')\n",
    "    for word in word_list :\n",
    "        if word in motsVar.keys() :\n",
    "            sentence.append(motsVar[word])\n",
    "        else :\n",
    "            sentence.append(word)\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "def repair(sentence) :\n",
    "    s = re.sub(\" ' \", \"'\", sentence)\n",
    "    s = re.sub(\" - \", \"-\", s)\n",
    "    s = re.sub(\" ,\", \",\", s)\n",
    "    s = re.sub(r'(?<=\\d) \\. (?=\\d)', '.', s)\n",
    "    s = re.sub(\" \\.\", \".\", s)\n",
    "    s = re.sub(\"\\( \", \"(\", s)\n",
    "    s = re.sub(\" \\)\", \")\", s)\n",
    "    s = s[0].upper() + s[1:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def InteractiveEvaluation(agent) :\n",
    "    print(\"Interactive mode (Press 'q' to exit)\")\n",
    "    agent.eval()\n",
    "    while True :\n",
    "        text = input('User : ')\n",
    "        #print('User : {}'.format(text))\n",
    "        if text == 'eoc' or text == 'q' or text == 'quit' : \n",
    "            break\n",
    "        reponse = agent(text)\n",
    "        reponse = ReplaceMotVar(motsVar, reponse)\n",
    "        reponse = repair(reponse)\n",
    "        print('Bot  : {}'.format(reponse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode (Press 'q' to exit)\n",
      "User : coucou\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : salut\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : tu fais quoi\n",
      "Bot  : Je suis la pour vous renseigner sur le master 2 data science.\n",
      "User : cool, sinon tu sais faire quoi\n",
      "Bot  : D'accord, comment puis-je vous aider ?\n",
      "User : bonjour\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : caca\n",
      "Bot  : Le parcours informatique est mathematiques.\n",
      "User : la statistique inférentielle c'st du caca ?\n",
      "Bot  : Le master data science possede deux : : le parcours de et.\n",
      "User : ha d'accord, j'ai donc un diplome caca\n",
      "Bot  : C'est a vous de faire un choix.\n",
      "User : en caca pardon\n",
      "Bot  : Le parcours comprend trois ue de de et pour.\n",
      "User : des quoi\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : quoi\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : bonjour\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : je comprends rien\n",
      "Bot  : Avez-vous d'autres questions ?\n",
      "User : quit\n"
     ]
    }
   ],
   "source": [
    "InteractiveEvaluation(chatbot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
