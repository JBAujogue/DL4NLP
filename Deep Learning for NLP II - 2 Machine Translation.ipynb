{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Part II - 2 <br><br><br>\n",
    "  Machine Translation\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. <font color=red>**Machine Translation**</font>\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.3.1\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\libDL4NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots.<br> Le corpus est donc une fois importé sous le forme :<br>\n",
    "\n",
    "- corpus = [text]<br>\n",
    "- text   = [word]<br>\n",
    "- word   = str<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Normalisation -------------------------------\n",
    "def normalizeString(s):\n",
    "    '''Remove rare symbols from a string'''\n",
    "    def unicodeToAscii(s):\n",
    "        \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    " \n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #s = re.sub(r\"[^a-zA-Z0-9?&\\%\\-\\_]+\", r\" \", s) \n",
    "    s = re.sub(\"\\(\", r\" ( \", s)\n",
    "    s = re.sub(\"\\)\", r\" ) \", s)\n",
    "    s = re.sub(r\"\\.\", r\" . \", s)\n",
    "    s = re.sub(r\",\", r\" , \", s)\n",
    "    s = re.sub(r\"!\", r\" ! \", s)\n",
    "    s = re.sub(r\":\", r\" : \", s)\n",
    "    s = re.sub(r\"-\", r\" - \", s)\n",
    "    s = re.sub(r\"'\", r\" ' \", s)\n",
    "    s = re.sub(r\";\", r\" ; \", s)\n",
    "    s = re.sub(r' +', r' ', s).strip()\n",
    "    return s \n",
    "\n",
    "\n",
    "\n",
    "#--------------------- import des dialogues --------------------\n",
    "def importDialogues(path, limit = None):\n",
    "    '''Import a textfile containing dialogues and returns a list, each element \n",
    "       corresponding to a dialogue and also being under the form of a list, with \n",
    "       each element being a list of two elements : an element giving a user \n",
    "       utterance and another element giving the bot response. Both elements are \n",
    "       normalized strings.\n",
    "       Ex. The dialogue :\n",
    "       \n",
    "               hi    hello what can i help you with today\n",
    "               can you book a table    i m on it\n",
    "               \n",
    "       now becomes :\n",
    "       \n",
    "              [['hi', 'hello what can i help you with today'], \n",
    "               ['can you book a table', 'i m on it']]\n",
    "               \n",
    "       Lines corresponding to user utterance with no bot response are discarted.\n",
    "    '''\n",
    "    def cleanS(s):\n",
    "        cleans = normalizeString(s)\n",
    "        cleans = cleans.replace('?', ' ? ').strip()\n",
    "        return cleans\n",
    "    \n",
    "    dialogues = []\n",
    "    dialogues_import = open(path, encoding='utf-8').read().strip().split('\\n\\n')\n",
    "    for i, d in enumerate(dialogues_import):\n",
    "        dialogue = []\n",
    "        lines = d.split('\\n')\n",
    "        for l in lines:\n",
    "            if len(l.split('\\t')) == 2 :\n",
    "                pair = [cleanS(s) for s in l.split('\\t')]\n",
    "                dialogue.append(pair)\n",
    "            elif len(l.split('\\t')) == 3 :\n",
    "                pair = [cleanS(s) for s in l.split('\\t')[:2]]\n",
    "                dialogue.append(pair)\n",
    "        dialogues.append(dialogue)\n",
    "        if limit is not None and i == limit -1 : break\n",
    "    return dialogues\n",
    "\n",
    "\n",
    "def getUniqueQAs(dialogues) :\n",
    "    uniq = []\n",
    "    for qa in dialogues :\n",
    "        if qa not in uniq : uniq.append(qa)\n",
    "    return uniq\n",
    "\n",
    "\n",
    "\n",
    "#------------------ Dictionnaire des mots variables -----------------------------\n",
    "def motVar(file):\n",
    "    '''Applies to the Master's program dataset.\n",
    "       Import the collection of pairs token-content for a set of variable words.\n",
    "    '''\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "    motsVar = {}\n",
    "    for l in lines :\n",
    "        cle, valeur = l.split('\\t')\n",
    "        motsVar[cle.lower()] = valeur\n",
    "    return motsVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "motsVar    = motVar(path_to_NLP + '\\\\data\\\\M2DS\\\\M2DS_2019_07\\\\chatbot-M2-DS-Variables.txt')\n",
    "dialogues  = importDialogues(path_to_NLP + '\\\\data\\\\M2DS\\\\M2DS_2019_10\\\\ChatbotDS_P_Train.tsv')\n",
    "dialogues_tst = importDialogues(path_to_NLP + '\\\\data\\\\M2DS\\\\M2DS_2019_10\\\\ChatbotDS_P_Test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200224 20561\n"
     ]
    }
   ],
   "source": [
    "qa_trn = [qa for dialogue in dialogues for qa in dialogue] # getUniqueQAs(\n",
    "qa_tst = [qa for dialogue in dialogues_tst for qa in dialogue] # getUniqueQAs(\n",
    "\n",
    "print(len(qa_trn), len(qa_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il vaut mieux suivre quel parcours  ? \t les deux parcours sont equivalents , c ' est a vous de faire un choix .\n",
      "quand est ce que commence les cours  ? \t la rentree est le daterentree\n",
      "comment je prend contact avec la scolarite  ? \t les contacts de la scolarite sont accessibles sur : sitescolarite\n",
      "est ce que il y a des cours en anglais  ? \t plusieurs supports de cours sont en anglais\n",
      "tous les cours sont obligatoires  ? \t tous les cours sont obligatoires , il n ' existe pas d ' options .\n",
      "merci pour les renseignements \t je vous en prie\n",
      "peux tu m ' aider  ? \t je suis la pour vous renseigner sur le master 2 data science .\n",
      "le cours de data visualisation est - il en ligne  ? \t vous devriez prendre contact avec l ' enseignant de l ' ue et lui demander\n",
      "j ' ai envie de faire de la data science \t alors vous devriez songer a candidater a ce master !\n",
      "il y a des livres a lire en statistique  ? \t vous devriez prendre contact avec l ' enseignant de l ' ue et lui demander\n",
      "il y a combien de candidats  ? \t il y a plus de nbrcandidats candidats chaque annee , la selection est rude !\n",
      "combien de places sont ouvertes  ? \t souhaitez - vous suivre le parcours informatique ou mathematiques  ?\n",
      "informatique \t etes - vous un etudiant inscrit en france  ?\n",
      "non \t il y a nbreplaces_hors_france_informatique places disponibles pour les etudiants hors france en parcours informatique .\n",
      "comment je peux payer  ? \t vous pouvez payer par carte , par cheque bancaire , par virement ou en espece .\n",
      "avez - vous une maquette detaillee de ce master  ? \t les details de la formation est disponible sur la page du master parcours informatique : site_informatique\n",
      "comment ca va  ? \t je vais bien merci , en quoi puis - je vous aider  ?\n"
     ]
    }
   ],
   "source": [
    "for qa in dialogues[0] : print(qa[0], '\\t', qa[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_in  = [['SOS'] + [w for w in qa[0].split(' ')] + ['EOS'] for qa in qa_trn]\n",
    "corpus_out = [['SOS'] + [w for w in qa[1].split(' ')] + ['EOS'] for qa in qa_trn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareWord2vec(corpus) :\n",
    "    fastText_word2vec = FastText(size = 75, \n",
    "                                 window = 5, \n",
    "                                 min_count = 1, \n",
    "                                 negative = 20,\n",
    "                                 sg = 1)\n",
    "    fastText_word2vec.build_vocab(corpus)\n",
    "    print(len(fastText_word2vec.wv.vocab))\n",
    "    fastText_word2vec.train(sentences = corpus, \n",
    "                            epochs = 5,\n",
    "                            total_examples = fastText_word2vec.corpus_count)\n",
    "    word2vec = Word2VecConnector(fastText_word2vec)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "896\n"
     ]
    }
   ],
   "source": [
    "word2vec_in  = prepareWord2vec(corpus_in)\n",
    "word2vec_out = prepareWord2vec(corpus_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bonjour', 0.9852568507194519),\n",
       " ('mal', 0.8179829120635986),\n",
       " ('principal', 0.6931003928184509),\n",
       " ('principalement', 0.6789224147796631),\n",
       " ('bien', 0.6547073125839233),\n",
       " ('english', 0.6544337272644043),\n",
       " ('handle', 0.6447221040725708),\n",
       " ('ok', 0.6249094605445862),\n",
       " ('cordialement', 0.6209567785263062),\n",
       " ('speak', 0.618739128112793)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_in.word2vec.most_similar('bonjou')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bonjour', 0.9725614786148071),\n",
       " ('accord', 0.786864161491394),\n",
       " ('comment', 0.6106002330780029),\n",
       " ('aider', 0.6066528558731079),\n",
       " ('accordez', 0.5910225510597229),\n",
       " ('excusez', 0.5597412586212158),\n",
       " ('monotonie', 0.5362932682037354),\n",
       " ('puis', 0.5213144421577454),\n",
       " ('vais', 0.5169289112091064),\n",
       " ('dynamiques', 0.4986083209514618)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_out.word2vec.most_similar('bonjou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Attention module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "<a id=\"attention\"></a>\n",
    "\n",
    "We use here a classical Attention Module :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Decoder module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "<a id=\"decoder\"></a>\n",
    "\n",
    "#### 1.4.1 Classical Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.modules import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, word2vec, hidden_dim, \n",
    "                 n_layers = 1,\n",
    "                 dropout = 0.1,\n",
    "                 bound = 25\n",
    "                ):\n",
    "        super(Decoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bound = bound\n",
    "        # modules\n",
    "        self.word2vec = word2vec\n",
    "        self.gru = nn.GRU(word2vec.output_dim, \n",
    "                          hidden_dim, \n",
    "                          n_layers, \n",
    "                          dropout = dropout, \n",
    "                          batch_first = True)\n",
    "        self.out = nn.Linear(hidden_dim, word2vec.lang.n_words)\n",
    "        self.act = F.log_softmax\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generateWord(self, hidden, word_index):\n",
    "        # update hidden state\n",
    "        embedding = self.word2vec.embedding(word_index) # size (batch_size, 1, embedding_dim)\n",
    "        embedding = self.dropout(embedding)\n",
    "        _, hidden = self.gru(embedding, hidden)         # size (n_layers, batch_size, embedding_dim)\n",
    "        # generate next word\n",
    "        log_prob = self.out(hidden[-1])                 # size (batch_size, lang_size)\n",
    "        log_prob = self.act(log_prob, dim = 1)          # size (batch_size, lang_size)\n",
    "        return log_prob, hidden\n",
    "    \n",
    "    def forward(self, hidden, device = None) :\n",
    "        answer = []\n",
    "        EOS_token  = self.word2vec.lang.getIndex('EOS')\n",
    "        word = self.word2vec.lang.getIndex('SOS')\n",
    "        word = Variable(torch.LongTensor([[word]])) # size (1)\n",
    "        for t in range(self.bound) :\n",
    "            # compute next word\n",
    "            if device is not None : word = word.to(device) # size (1)\n",
    "            log_prob, hidden = self.generateWord(hidden, word)\n",
    "            word = log_prob.topk(1, dim = 1)[1].view(1, 1)\n",
    "            # add to output\n",
    "            if word.item() == EOS_token : break\n",
    "            else : answer.append(word.item())\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Attention Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.modules import AttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Machine Translation Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec_in, word2vec_out, \n",
    "                 hidden_dim = 100,\n",
    "                 n_layers = 1, \n",
    "                 bound = 25,\n",
    "                 dropout = 0, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.word2vec_in  = word2vec_in\n",
    "        self.word2vec_out = word2vec_out\n",
    "        self.context      = RecurrentEncoder(word2vec_in.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.decoder      = Decoder(word2vec_out, hidden_dim, n_layers, dropout, bound)\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index_in  = self.word2vec_in.lang.getIndex('PADDING_WORD')\n",
    "        self.ignore_index_out = self.word2vec_out.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(size_average = False, ignore_index = self.ignore_index_out)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, sentence, color_code = '\\033[94m'):\n",
    "        # encode sentence\n",
    "        words = [self.word2vec_in.lang.getIndex(w) for w in self.tokenizer(sentence)]\n",
    "        words = [w for w in words if w is not None]\n",
    "        words = Variable(torch.LongTensor([words])).to(self.device)\n",
    "        embeddings = self.word2vec_in.embedding(words)\n",
    "        #words  = self.tokenizer(sentence)\n",
    "        #embeddings = self.word2vec_in(words, self.device)\n",
    "        _, hidden  = self.context(embeddings)\n",
    "        # sum along directions\n",
    "        if self.context.bidirectional :\n",
    "            hidden = hidden.view(self.context.n_layers, 2, -1, self.context.hidden_dim)\n",
    "            hidden = torch.sum(hidden, dim = 1) # size (n_layers, batch_size, hidden_dim)\n",
    "        ## compute answer\n",
    "        answer = self.decoder(hidden, self.device)\n",
    "        answer = [self.word2vec_out.lang.index2word[i] for i in answer]\n",
    "        answer = ' '.join(answer)\n",
    "        #print(' '.join(self.tokenizer(sentence) + [':', color_code] + answer + ['\\033[0m']))\n",
    "        return answer\n",
    "\n",
    "    def generatePackedSentences(self, sentences, batch_size = 32) : \n",
    "        sentences.sort(key = lambda s: len(s[1]), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            # prepare input and target pack\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            pack.sort(key = lambda s: len(self.tokenizer(s[0])), reverse = True)\n",
    "            pack0 = [[self.word2vec_in.lang.getIndex(w) for w in self.tokenizer(qa[0])] for qa in pack]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack1 = [[self.word2vec_out.lang.getIndex(w) for w in self.tokenizer(qa[1]) + ['EOS']] for qa in pack]\n",
    "            pack1 = [[w for w in words if w is not None] for words in pack1]\n",
    "            lengths = torch.tensor([len(p) for p in pack0])           # size (batch_size) \n",
    "            # padd packs\n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index_in))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1)) # size (batch_size, max_length0) \n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index_out))\n",
    "            pack1 = Variable(torch.LongTensor(pack1))       # WARNING : size (max_length1, batch_size) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def compute_accuracy(self, sentences) :\n",
    "        batches = self.generatePackedSentences(sentences, batch_size = 32)\n",
    "        score = 0\n",
    "        for batch, target in batches :\n",
    "            embeddings  = self.word2vec_in.embedding(batch[0].to(self.device))\n",
    "            hiddens, _  = self.context(embeddings, lengths = batch[1].to(self.device))\n",
    "            attended, _ = self.attention(hiddens)\n",
    "            if self.bin_mode : \n",
    "                vects  = self.out(attended).view(-1)\n",
    "                target = target.to(self.device).view(-1)\n",
    "                score += sum(torch.abs(target - self.act(vects)) < 0.5).item()\n",
    "            else : \n",
    "                log_probs = F.log_softmax(self.out(attended.squeeze(1)))\n",
    "                target    = target.to(self.device).view(-1)\n",
    "                score    += sum([target[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(target.size(0))])\n",
    "        return score * 100 / len(sentences)\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, tf_ratio = 0, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeSuccess(log_probs, targets) :\n",
    "            success = sum([self.ignore_index_out != targets[i].item() == log_probs[i].topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0))])\n",
    "            return success\n",
    "        \n",
    "        def computeLogProbs(batch, target, tf_ratio = 0, compute_accuracy = True) :\n",
    "            loss = 0\n",
    "            success = 0\n",
    "            forcing = (random.random() < tf_ratio)\n",
    "            # encode sentences\n",
    "            embeddings = self.word2vec_in.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # size (n_layers * num_directions, batch_size, hidden_dim)\n",
    "            # sum along directions\n",
    "            if self.context.bidirectional :\n",
    "                hidden = hidden.view(self.context.n_layers, 2, -1, self.context.hidden_dim)\n",
    "                hidden = torch.sum(hidden, dim = 1)               # size (n_layers, batch_size, hidden_dim)\n",
    "            # compute answers\n",
    "            word_index = self.word2vec_out.lang.getIndex('SOS')\n",
    "            word_index = Variable(torch.LongTensor([word_index])) # size (1)\n",
    "            word_index = word_index.expand(target.size(1))        # size (batch_size)\n",
    "            for t in range(target.size(0)) :\n",
    "                # compute word probs\n",
    "                log_prob, hidden = self.decoder.generateWord(hidden, word_index.unsqueeze(1).to(self.device))\n",
    "                # compute loss\n",
    "                loss += self.criterion(log_prob, target[t])\n",
    "                if compute_accuracy : success += computeSuccess(log_prob, target[t])\n",
    "                # apply teacher forcing\n",
    "                if forcing : word_index = target[t]                             # size (batch_size) \n",
    "                else       : word_index = log_prob.topk(1, dim = 1)[1].view(-1) # size (batch_size)\n",
    "            return loss, success       \n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, tf_ratio = 0, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            target = batch[1].to(self.device)\n",
    "            total = np.sum(target.data.cpu().numpy() != self.ignore_index_out)\n",
    "            loss, success = computeLogProbs(batch[0], target, tf_ratio, compute_accuracy)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return float(loss.item() / total), float(success * 100 / total)\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, tf_ratio, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, tf_ratio, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(qa[1].split(' ')) for qa in qa_trn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307122"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot = EncoderDecoder(device = torch.device(\"cpu\"),\n",
    "                         tokenizer = lambda s : normalizeString(s).split(' '),\n",
    "                         word2vec_in = word2vec_in,\n",
    "                         word2vec_out = word2vec_out,\n",
    "                         hidden_dim = 75, \n",
    "                         n_layers = 2,\n",
    "                         bound = 75,\n",
    "                         dropout = 0.1,\n",
    "                         optimizer = optim.SGD)\n",
    "\n",
    "chatbot.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (word2vec_in): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(784, 75)\n",
       "    )\n",
       "    (embedding): Embedding(784, 75)\n",
       "  )\n",
       "  (word2vec_out): Word2VecConnector(\n",
       "    (twin): Word2Vec(\n",
       "      (embedding): Embedding(898, 75)\n",
       "    )\n",
       "    (embedding): Embedding(898, 75)\n",
       "  )\n",
       "  (context): RecurrentEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (bigru): GRU(75, 75, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (word2vec): Word2VecConnector(\n",
       "      (twin): Word2Vec(\n",
       "        (embedding): Embedding(898, 75)\n",
       "      )\n",
       "      (embedding): Embedding(898, 75)\n",
       "    )\n",
       "    (gru): GRU(75, 75, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (out): Linear(in_features=75, out_features=897, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6257"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = chatbot.generatePackedSentences(qa_trn, batch_size = 32)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 12s (- 13m 15s) (100 1%) loss : 0.720  accuracy : 84.7 %\n",
      "0m 25s (- 12m 57s) (200 3%) loss : 0.720  accuracy : 85.2 %\n",
      "0m 37s (- 12m 32s) (300 4%) loss : 0.653  accuracy : 87.3 %\n",
      "0m 52s (- 12m 45s) (400 6%) loss : 0.757  accuracy : 84.4 %\n",
      "1m 4s (- 12m 24s) (500 7%) loss : 0.753  accuracy : 83.7 %\n",
      "1m 17s (- 12m 6s) (600 9%) loss : 0.655  accuracy : 85.7 %\n",
      "1m 29s (- 11m 50s) (700 11%) loss : 0.777  accuracy : 82.9 %\n",
      "1m 42s (- 11m 42s) (800 12%) loss : 0.912  accuracy : 80.0 %\n",
      "1m 55s (- 11m 25s) (900 14%) loss : 0.784  accuracy : 83.0 %\n",
      "2m 7s (- 11m 10s) (1000 15%) loss : 0.765  accuracy : 82.5 %\n",
      "2m 21s (- 11m 2s) (1100 17%) loss : 0.810  accuracy : 83.0 %\n",
      "2m 33s (- 10m 47s) (1200 19%) loss : 0.803  accuracy : 82.7 %\n",
      "2m 45s (- 10m 31s) (1300 20%) loss : 0.704  accuracy : 84.6 %\n",
      "2m 58s (- 10m 18s) (1400 22%) loss : 0.764  accuracy : 84.6 %\n",
      "3m 11s (- 10m 6s) (1500 23%) loss : 0.641  accuracy : 86.4 %\n",
      "3m 25s (- 9m 58s) (1600 25%) loss : 0.665  accuracy : 86.4 %\n",
      "3m 38s (- 9m 44s) (1700 27%) loss : 0.553  accuracy : 88.5 %\n",
      "3m 51s (- 9m 32s) (1800 28%) loss : 0.615  accuracy : 87.0 %\n",
      "4m 4s (- 9m 20s) (1900 30%) loss : 0.798  accuracy : 83.2 %\n",
      "4m 17s (- 9m 7s) (2000 31%) loss : 0.749  accuracy : 85.4 %\n",
      "4m 29s (- 8m 53s) (2100 33%) loss : 0.738  accuracy : 83.1 %\n",
      "4m 42s (- 8m 40s) (2200 35%) loss : 0.728  accuracy : 83.5 %\n",
      "4m 55s (- 8m 27s) (2300 36%) loss : 0.728  accuracy : 85.6 %\n",
      "5m 7s (- 8m 14s) (2400 38%) loss : 0.563  accuracy : 88.8 %\n",
      "5m 21s (- 8m 3s) (2500 39%) loss : 0.735  accuracy : 85.6 %\n",
      "5m 33s (- 7m 49s) (2600 41%) loss : 0.695  accuracy : 84.8 %\n",
      "5m 45s (- 7m 34s) (2700 43%) loss : 0.722  accuracy : 85.9 %\n",
      "5m 57s (- 7m 21s) (2800 44%) loss : 0.684  accuracy : 86.2 %\n",
      "6m 11s (- 7m 9s) (2900 46%) loss : 0.598  accuracy : 88.5 %\n",
      "6m 23s (- 6m 56s) (3000 47%) loss : 0.752  accuracy : 84.5 %\n",
      "6m 36s (- 6m 43s) (3100 49%) loss : 0.856  accuracy : 80.9 %\n",
      "6m 49s (- 6m 31s) (3200 51%) loss : 0.947  accuracy : 79.5 %\n",
      "7m 1s (- 6m 17s) (3300 52%) loss : 0.657  accuracy : 85.6 %\n",
      "7m 14s (- 6m 5s) (3400 54%) loss : 0.748  accuracy : 83.3 %\n",
      "7m 27s (- 5m 52s) (3500 55%) loss : 0.616  accuracy : 88.7 %\n",
      "7m 40s (- 5m 39s) (3600 57%) loss : 0.748  accuracy : 83.2 %\n",
      "7m 53s (- 5m 27s) (3700 59%) loss : 0.738  accuracy : 83.0 %\n",
      "8m 5s (- 5m 14s) (3800 60%) loss : 0.673  accuracy : 85.0 %\n",
      "8m 17s (- 5m 0s) (3900 62%) loss : 0.756  accuracy : 83.6 %\n",
      "8m 30s (- 4m 48s) (4000 63%) loss : 0.847  accuracy : 82.4 %\n",
      "8m 43s (- 4m 35s) (4100 65%) loss : 0.713  accuracy : 84.4 %\n",
      "8m 55s (- 4m 22s) (4200 67%) loss : 0.684  accuracy : 85.7 %\n",
      "9m 8s (- 4m 9s) (4300 68%) loss : 0.654  accuracy : 87.0 %\n",
      "9m 20s (- 3m 56s) (4400 70%) loss : 0.699  accuracy : 83.6 %\n",
      "9m 32s (- 3m 43s) (4500 71%) loss : 0.725  accuracy : 85.0 %\n",
      "9m 45s (- 3m 31s) (4600 73%) loss : 0.746  accuracy : 85.0 %\n",
      "9m 59s (- 3m 18s) (4700 75%) loss : 0.603  accuracy : 87.1 %\n",
      "10m 12s (- 3m 5s) (4800 76%) loss : 0.688  accuracy : 85.1 %\n",
      "10m 24s (- 2m 53s) (4900 78%) loss : 0.746  accuracy : 86.6 %\n",
      "10m 36s (- 2m 40s) (5000 79%) loss : 0.696  accuracy : 86.6 %\n",
      "10m 49s (- 2m 27s) (5100 81%) loss : 0.742  accuracy : 83.3 %\n",
      "11m 2s (- 2m 14s) (5200 83%) loss : 0.730  accuracy : 84.4 %\n",
      "11m 14s (- 2m 1s) (5300 84%) loss : 0.664  accuracy : 86.0 %\n",
      "11m 26s (- 1m 48s) (5400 86%) loss : 0.656  accuracy : 85.5 %\n",
      "11m 39s (- 1m 36s) (5500 87%) loss : 0.619  accuracy : 86.1 %\n",
      "11m 51s (- 1m 23s) (5600 89%) loss : 0.628  accuracy : 86.2 %\n",
      "12m 3s (- 1m 10s) (5700 91%) loss : 0.723  accuracy : 85.9 %\n",
      "12m 15s (- 0m 57s) (5800 92%) loss : 0.541  accuracy : 89.3 %\n",
      "12m 27s (- 0m 45s) (5900 94%) loss : 0.633  accuracy : 86.5 %\n",
      "12m 40s (- 0m 32s) (6000 95%) loss : 0.781  accuracy : 82.7 %\n",
      "12m 52s (- 0m 19s) (6100 97%) loss : 0.666  accuracy : 85.9 %\n",
      "13m 5s (- 0m 7s) (6200 99%) loss : 0.647  accuracy : 87.0 %\n"
     ]
    }
   ],
   "source": [
    "chatbot.fit(batches, epochs = 1, lr = 0.001, tf_ratio = 1,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.001, tf_ratio = 0.5,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.00025, tf_ratio = 0.5,  print_every = 100)\n",
    "chatbot.fit(batches, epochs = 1, lr = 0.0001, tf_ratio = 0.35,  print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "#torch.save(chatbot.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_II2_encoder_decoder.pth')\n",
    "\n",
    "# load\n",
    "#chatbot.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_II2_encoder_decoder.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceMotVar(motsVar, raw_sentence):\n",
    "    sentence = []\n",
    "    word_list = raw_sentence.split(' ')\n",
    "    for word in word_list :\n",
    "        if word in motsVar.keys() :\n",
    "            sentence.append(motsVar[word])\n",
    "        else :\n",
    "            sentence.append(word)\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "\n",
    "def repair(sentence) :\n",
    "    s = re.sub(\" ' \", \"'\", sentence)\n",
    "    s = re.sub(\" - \", \"-\", s)\n",
    "    s = re.sub(\" ,\", \",\", s)\n",
    "    s = re.sub(r'(?<=\\d) \\. (?=\\d)', '.', s)\n",
    "    s = re.sub(\" \\.\", \".\", s)\n",
    "    s = re.sub(\"\\( \", \"(\", s)\n",
    "    s = re.sub(\" \\)\", \")\", s)\n",
    "    s = s[0].upper() + s[1:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def InteractiveEvaluation(agent) :\n",
    "    print(\"Interactive mode (Press 'q' to exit)\")\n",
    "    while True :\n",
    "        text = input('User : ')\n",
    "        #print('User : {}'.format(text))\n",
    "        if text == 'eoc' or text == 'q' or text == 'quit' : \n",
    "            break\n",
    "        reponse = agent(text)\n",
    "        reponse = ReplaceMotVar(motsVar, reponse)\n",
    "        reponse = repair(reponse)\n",
    "        print('Bot  : {}'.format(reponse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode (Press 'q' to exit)\n",
      "User : coucou\n",
      "Bot  : Bonjour, comment puis-je vous aider ?\n",
      "User : quit\n"
     ]
    }
   ],
   "source": [
    "InteractiveEvaluation(chatbot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
