{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=orange>I - 3 </font>\n",
    "  Language Modeling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. <font color=orange>**Language Modeling**</font>\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. Sequence to sequence\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "1. Abstractive Summarization\n",
    "\n",
    "2. Question Answering\n",
    "\n",
    "3. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) |\n",
    "\n",
    "# Overview\n",
    "\n",
    "\n",
    "Exemples d'implémentation en PyTorch :\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "\n",
    "\n",
    "Différentes architectures sont décrites dans la litérature :\n",
    "\n",
    "- Regularizing and Optimizing LSTM Language Models - https://arxiv.org/pdf/1708.02182.pdf\n",
    "\n",
    "Un modèle linguistique est intérressant en soi, mais peut aussi servir pour le pré-entrainement de couches basses d'un modèle plus complexe :\n",
    "\n",
    "- Deep contextualized word representations - https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Improving Language Understanding by Generative Pre-Training - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "- Language Models are Unsupervised Multitask Learners - https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.5.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import gc\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DL4NLP = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_DL4NLP + '\\\\lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "df_GMB_extract = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050794, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx           word  pos\n",
       "0           1.0      Thousands  NNS\n",
       "1           1.0             of   IN\n",
       "2           1.0  demonstrators  NNS\n",
       "3           1.0           have  VBP\n",
       "4           1.0        marched  VBN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract.dropna(inplace = True)\n",
    "df_GMB_extract = df_GMB_extract[['sentence_idx', 'word', 'pos']]\n",
    "print(df_GMB_extract.shape)\n",
    "df_GMB_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus with words lowered and stripped\n",
    "corpus = df_GMB_extract.groupby(\"sentence_idx\").apply(lambda s: [w.lower().strip() for w in s[\"word\"].values.tolist()]).tolist()\n",
    "corpus = [[w for w in s if w != ''] for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I3_skipgram_gensim.model\")\n",
    "sg_gensim = Word2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27419\n"
     ]
    }
   ],
   "source": [
    "lang = Lang(corpus, min_count = 1)\n",
    "print(lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_gensim = Word2Vec(corpus, \n",
    "                     size = 100, \n",
    "                     window = 5, \n",
    "                     min_count = 1, \n",
    "                     negative = 20, \n",
    "                     iter = 100,\n",
    "                     sg = 1,\n",
    "                     workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I3_skipgram_gensim.model\")\n",
    "#sg_gensim.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(sg_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ordered vocab is the same for both the original and its wrapped objects\n",
    "# except the two last words 'PADDING_WORD' and 'UNK' added to the wrapped object\n",
    "list(word2vec.word2vec.wv.index2word) == list(word2vec.twin.lang.word2index)[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.models import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layer = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(\n",
    "            emb_dim = self.word2vec.out_dim, \n",
    "            hid_dim = hidden_dim, \n",
    "            n_layer = n_layer, \n",
    "            dropout = dropout, \n",
    "            bidirectional = False)\n",
    "        self.out       = nn.Linear(self.context.out_dim, self.word2vec.lang.n_words)\n",
    "        self.act       = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.criterion = nn.NLLLoss(size_average = False, weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, \n",
    "                sentence = '.', \n",
    "                hidden = None, \n",
    "                limit = 10, \n",
    "                color_code = '\\033[94m'):\n",
    "        # init variables\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            probs      = self.act(self.out(hidden[-1]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return \n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, lengths = [5, 10, 15]) :\n",
    "        sentences = [s[i: i+j] \\\n",
    "                     for s in sentences \\\n",
    "                     for j in lengths \\\n",
    "                     for i in range(len(s)-j)]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack1 = Variable(torch.LongTensor([s[-1] for s in pack0]))\n",
    "            pack0 = [s[:-1] for s in pack0]\n",
    "            lengths = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.word2vec.lang.getIndex('PADDING_WORD')))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            _, hidden  = self.context(embeddings, lengths = batch[1].to(self.device)) # WARNING : dim = (n_layers, batch_size, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hidden[-1]), dim = 1)   # dim = (batch_size, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            return sum([targets[i].item() == log_probs[i].data.topk(1)[1].item() for i in range(targets.size(0))]) * 100 / targets.size(0)\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = computeLogProbs(batch[0])\n",
    "            targets   = batch[1].to(self.device).view(-1)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / targets.size(0)), accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6175020"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = LanguageModel(device,\n",
    "                               tokenizer = lambda s : s.split(' '),\n",
    "                               word2vec = word2vec,\n",
    "                               hidden_dim = 200, \n",
    "                               n_layer = 3, \n",
    "                               dropout = 0.1,\n",
    "                               optimizer = optim.AdamW)\n",
    "\n",
    "language_model.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66035"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = language_model.generatePackedSentences(corpus, batch_size = 64, lengths = [5, 7, 9, 11, 13, 15])\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 27s (- 72m 12s) (250 0%) loss : 7.832  accuracy : 5.1 %\n",
      "0m 54s (- 71m 33s) (500 1%) loss : 7.453  accuracy : 5.2 %\n",
      "1m 21s (- 70m 51s) (750 1%) loss : 7.332  accuracy : 5.8 %\n",
      "1m 48s (- 70m 18s) (1000 2%) loss : 7.247  accuracy : 7.3 %\n",
      "2m 14s (- 69m 42s) (1250 3%) loss : 7.121  accuracy : 7.5 %\n",
      "2m 41s (- 69m 11s) (1500 3%) loss : 6.993  accuracy : 8.0 %\n",
      "3m 8s (- 68m 40s) (1750 4%) loss : 6.981  accuracy : 8.4 %\n",
      "3m 36s (- 68m 35s) (2000 5%) loss : 6.786  accuracy : 9.8 %\n",
      "4m 3s (- 68m 6s) (2250 5%) loss : 6.732  accuracy : 10.2 %\n",
      "4m 30s (- 67m 35s) (2500 6%) loss : 6.671  accuracy : 10.3 %\n",
      "4m 57s (- 67m 4s) (2750 6%) loss : 6.552  accuracy : 11.1 %\n",
      "5m 24s (- 66m 37s) (3000 7%) loss : 6.474  accuracy : 12.0 %\n",
      "5m 50s (- 66m 8s) (3250 8%) loss : 6.361  accuracy : 12.3 %\n",
      "6m 17s (- 65m 38s) (3500 8%) loss : 6.335  accuracy : 12.0 %\n",
      "6m 44s (- 65m 9s) (3750 9%) loss : 6.283  accuracy : 12.7 %\n",
      "7m 11s (- 64m 40s) (4000 10%) loss : 6.254  accuracy : 12.9 %\n",
      "7m 37s (- 64m 12s) (4250 10%) loss : 6.237  accuracy : 12.7 %\n",
      "8m 4s (- 63m 43s) (4500 11%) loss : 6.216  accuracy : 12.9 %\n",
      "8m 31s (- 63m 17s) (4750 11%) loss : 6.162  accuracy : 14.1 %\n",
      "8m 58s (- 62m 49s) (5000 12%) loss : 6.161  accuracy : 13.9 %\n",
      "9m 25s (- 62m 21s) (5250 13%) loss : 6.042  accuracy : 14.0 %\n",
      "9m 51s (- 61m 53s) (5500 13%) loss : 6.055  accuracy : 14.2 %\n",
      "10m 18s (- 61m 24s) (5750 14%) loss : 6.026  accuracy : 13.9 %\n",
      "10m 45s (- 60m 56s) (6000 15%) loss : 5.976  accuracy : 14.5 %\n",
      "11m 12s (- 60m 31s) (6250 15%) loss : 5.939  accuracy : 14.5 %\n",
      "11m 39s (- 60m 3s) (6500 16%) loss : 5.921  accuracy : 15.0 %\n",
      "12m 5s (- 59m 34s) (6750 16%) loss : 5.858  accuracy : 15.4 %\n",
      "12m 32s (- 59m 6s) (7000 17%) loss : 5.816  accuracy : 14.9 %\n",
      "12m 58s (- 58m 38s) (7250 18%) loss : 5.819  accuracy : 15.0 %\n",
      "13m 25s (- 58m 10s) (7500 18%) loss : 5.724  accuracy : 15.4 %\n",
      "13m 52s (- 57m 42s) (7750 19%) loss : 5.788  accuracy : 15.1 %\n",
      "14m 18s (- 57m 14s) (8000 20%) loss : 5.801  accuracy : 15.6 %\n",
      "14m 45s (- 56m 46s) (8250 20%) loss : 5.691  accuracy : 16.1 %\n",
      "15m 11s (- 56m 19s) (8500 21%) loss : 5.675  accuracy : 16.2 %\n",
      "15m 38s (- 55m 51s) (8750 21%) loss : 5.765  accuracy : 15.1 %\n",
      "16m 5s (- 55m 24s) (9000 22%) loss : 5.705  accuracy : 15.6 %\n",
      "16m 31s (- 54m 57s) (9250 23%) loss : 5.651  accuracy : 16.0 %\n",
      "16m 58s (- 54m 29s) (9500 23%) loss : 5.684  accuracy : 16.2 %\n",
      "17m 24s (- 54m 1s) (9750 24%) loss : 5.598  accuracy : 16.1 %\n",
      "17m 51s (- 53m 34s) (10000 25%) loss : 5.562  accuracy : 16.6 %\n",
      "18m 18s (- 53m 7s) (10250 25%) loss : 5.574  accuracy : 16.0 %\n",
      "18m 44s (- 52m 39s) (10500 26%) loss : 5.576  accuracy : 16.4 %\n",
      "19m 11s (- 52m 12s) (10750 26%) loss : 5.475  accuracy : 17.1 %\n",
      "19m 37s (- 51m 45s) (11000 27%) loss : 5.558  accuracy : 16.5 %\n",
      "20m 4s (- 51m 18s) (11250 28%) loss : 5.517  accuracy : 17.1 %\n",
      "20m 31s (- 50m 51s) (11500 28%) loss : 5.512  accuracy : 16.9 %\n",
      "20m 57s (- 50m 23s) (11750 29%) loss : 5.566  accuracy : 16.9 %\n",
      "21m 24s (- 49m 56s) (12000 30%) loss : 5.532  accuracy : 16.6 %\n",
      "21m 50s (- 49m 29s) (12250 30%) loss : 5.422  accuracy : 17.1 %\n",
      "22m 17s (- 49m 2s) (12500 31%) loss : 5.547  accuracy : 16.9 %\n",
      "22m 43s (- 48m 35s) (12750 31%) loss : 5.538  accuracy : 17.1 %\n",
      "23m 10s (- 48m 8s) (13000 32%) loss : 5.492  accuracy : 17.5 %\n",
      "23m 37s (- 47m 41s) (13250 33%) loss : 5.417  accuracy : 17.5 %\n",
      "24m 3s (- 47m 14s) (13500 33%) loss : 5.389  accuracy : 17.9 %\n",
      "24m 30s (- 46m 47s) (13750 34%) loss : 5.304  accuracy : 18.2 %\n",
      "24m 57s (- 46m 20s) (14000 35%) loss : 5.316  accuracy : 18.4 %\n",
      "25m 23s (- 45m 53s) (14250 35%) loss : 5.392  accuracy : 17.2 %\n",
      "25m 50s (- 45m 26s) (14500 36%) loss : 5.281  accuracy : 18.1 %\n",
      "26m 16s (- 44m 59s) (14750 36%) loss : 5.506  accuracy : 16.5 %\n",
      "26m 43s (- 44m 32s) (15000 37%) loss : 5.330  accuracy : 17.9 %\n",
      "27m 10s (- 44m 5s) (15250 38%) loss : 5.317  accuracy : 18.5 %\n",
      "27m 36s (- 43m 38s) (15500 38%) loss : 5.358  accuracy : 17.9 %\n",
      "28m 3s (- 43m 11s) (15750 39%) loss : 5.348  accuracy : 17.7 %\n",
      "28m 29s (- 42m 44s) (16000 40%) loss : 5.322  accuracy : 18.2 %\n",
      "28m 56s (- 42m 17s) (16250 40%) loss : 5.278  accuracy : 18.7 %\n",
      "29m 22s (- 41m 50s) (16500 41%) loss : 5.313  accuracy : 17.7 %\n",
      "29m 49s (- 41m 23s) (16750 41%) loss : 5.352  accuracy : 17.5 %\n",
      "30m 15s (- 40m 56s) (17000 42%) loss : 5.308  accuracy : 18.4 %\n",
      "30m 42s (- 40m 29s) (17250 43%) loss : 5.239  accuracy : 18.7 %\n",
      "31m 8s (- 40m 2s) (17500 43%) loss : 5.266  accuracy : 17.9 %\n",
      "31m 35s (- 39m 35s) (17750 44%) loss : 5.378  accuracy : 17.6 %\n",
      "32m 1s (- 39m 8s) (18000 45%) loss : 5.257  accuracy : 18.3 %\n",
      "32m 28s (- 38m 42s) (18250 45%) loss : 5.277  accuracy : 18.2 %\n",
      "32m 54s (- 38m 15s) (18500 46%) loss : 5.223  accuracy : 19.0 %\n",
      "33m 21s (- 37m 48s) (18750 46%) loss : 5.215  accuracy : 18.2 %\n",
      "33m 47s (- 37m 21s) (19000 47%) loss : 5.243  accuracy : 18.4 %\n",
      "34m 14s (- 36m 54s) (19250 48%) loss : 5.250  accuracy : 19.1 %\n",
      "34m 40s (- 36m 27s) (19500 48%) loss : 5.234  accuracy : 18.2 %\n",
      "35m 7s (- 36m 1s) (19750 49%) loss : 5.260  accuracy : 17.9 %\n",
      "35m 34s (- 35m 34s) (20000 50%) loss : 5.235  accuracy : 19.1 %\n",
      "36m 0s (- 35m 7s) (20250 50%) loss : 5.203  accuracy : 18.7 %\n",
      "36m 27s (- 34m 40s) (20500 51%) loss : 5.231  accuracy : 18.3 %\n",
      "36m 53s (- 34m 13s) (20750 51%) loss : 5.146  accuracy : 19.3 %\n",
      "37m 20s (- 33m 46s) (21000 52%) loss : 5.186  accuracy : 18.6 %\n",
      "37m 46s (- 33m 20s) (21250 53%) loss : 5.195  accuracy : 18.9 %\n",
      "38m 13s (- 32m 53s) (21500 53%) loss : 5.163  accuracy : 19.0 %\n",
      "38m 40s (- 32m 26s) (21750 54%) loss : 5.287  accuracy : 18.3 %\n",
      "39m 6s (- 31m 59s) (22000 55%) loss : 5.167  accuracy : 18.8 %\n",
      "39m 33s (- 31m 33s) (22250 55%) loss : 5.187  accuracy : 18.5 %\n",
      "39m 59s (- 31m 6s) (22500 56%) loss : 5.083  accuracy : 19.6 %\n",
      "40m 26s (- 30m 39s) (22750 56%) loss : 5.219  accuracy : 18.7 %\n",
      "40m 52s (- 30m 12s) (23000 57%) loss : 5.169  accuracy : 18.5 %\n",
      "41m 19s (- 29m 46s) (23250 58%) loss : 5.118  accuracy : 18.9 %\n",
      "41m 46s (- 29m 19s) (23500 58%) loss : 5.136  accuracy : 18.8 %\n",
      "42m 12s (- 28m 52s) (23750 59%) loss : 5.177  accuracy : 18.9 %\n",
      "42m 38s (- 28m 25s) (24000 60%) loss : 5.195  accuracy : 18.9 %\n",
      "43m 5s (- 27m 59s) (24250 60%) loss : 5.092  accuracy : 19.0 %\n",
      "43m 32s (- 27m 32s) (24500 61%) loss : 5.157  accuracy : 18.7 %\n",
      "43m 58s (- 27m 5s) (24750 61%) loss : 5.186  accuracy : 18.5 %\n",
      "44m 25s (- 26m 39s) (25000 62%) loss : 5.063  accuracy : 18.9 %\n",
      "44m 51s (- 26m 12s) (25250 63%) loss : 5.140  accuracy : 18.4 %\n",
      "45m 18s (- 25m 45s) (25500 63%) loss : 5.044  accuracy : 19.4 %\n",
      "45m 44s (- 25m 18s) (25750 64%) loss : 5.097  accuracy : 19.4 %\n",
      "46m 11s (- 24m 52s) (26000 65%) loss : 5.024  accuracy : 19.7 %\n",
      "46m 37s (- 24m 25s) (26250 65%) loss : 5.017  accuracy : 19.8 %\n",
      "47m 4s (- 23m 58s) (26500 66%) loss : 5.207  accuracy : 18.3 %\n",
      "47m 30s (- 23m 32s) (26750 66%) loss : 4.992  accuracy : 19.9 %\n",
      "47m 57s (- 23m 5s) (27000 67%) loss : 5.120  accuracy : 19.5 %\n",
      "48m 24s (- 22m 38s) (27250 68%) loss : 5.112  accuracy : 19.6 %\n",
      "48m 50s (- 22m 12s) (27500 68%) loss : 5.113  accuracy : 19.1 %\n",
      "49m 17s (- 21m 45s) (27750 69%) loss : 4.965  accuracy : 19.8 %\n",
      "49m 43s (- 21m 18s) (28000 70%) loss : 5.092  accuracy : 19.2 %\n",
      "50m 9s (- 20m 51s) (28250 70%) loss : 5.069  accuracy : 19.5 %\n",
      "50m 36s (- 20m 25s) (28500 71%) loss : 5.134  accuracy : 18.9 %\n",
      "51m 2s (- 19m 58s) (28750 71%) loss : 5.075  accuracy : 19.4 %\n",
      "51m 29s (- 19m 31s) (29000 72%) loss : 5.065  accuracy : 19.7 %\n",
      "51m 55s (- 19m 5s) (29250 73%) loss : 4.999  accuracy : 19.6 %\n",
      "52m 22s (- 18m 38s) (29500 73%) loss : 5.119  accuracy : 19.4 %\n",
      "52m 48s (- 18m 11s) (29750 74%) loss : 5.047  accuracy : 19.9 %\n",
      "53m 15s (- 17m 45s) (30000 75%) loss : 5.023  accuracy : 19.6 %\n",
      "53m 41s (- 17m 18s) (30250 75%) loss : 4.997  accuracy : 19.8 %\n",
      "54m 8s (- 16m 51s) (30500 76%) loss : 4.897  accuracy : 20.5 %\n",
      "54m 34s (- 16m 25s) (30750 76%) loss : 5.032  accuracy : 19.4 %\n",
      "55m 1s (- 15m 58s) (31000 77%) loss : 5.096  accuracy : 18.7 %\n",
      "55m 27s (- 15m 31s) (31250 78%) loss : 4.954  accuracy : 20.5 %\n",
      "55m 54s (- 15m 5s) (31500 78%) loss : 5.009  accuracy : 19.4 %\n",
      "56m 20s (- 14m 38s) (31750 79%) loss : 4.914  accuracy : 20.5 %\n",
      "56m 47s (- 14m 11s) (32000 80%) loss : 4.961  accuracy : 20.3 %\n",
      "57m 14s (- 13m 45s) (32250 80%) loss : 4.910  accuracy : 20.2 %\n",
      "57m 40s (- 13m 18s) (32500 81%) loss : 4.925  accuracy : 20.5 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58m 7s (- 12m 51s) (32750 81%) loss : 4.986  accuracy : 20.6 %\n",
      "58m 33s (- 12m 25s) (33000 82%) loss : 4.972  accuracy : 19.4 %\n",
      "59m 0s (- 11m 58s) (33250 83%) loss : 5.026  accuracy : 19.3 %\n",
      "59m 26s (- 11m 32s) (33500 83%) loss : 4.932  accuracy : 20.2 %\n",
      "59m 53s (- 11m 5s) (33750 84%) loss : 5.040  accuracy : 19.9 %\n",
      "60m 19s (- 10m 38s) (34000 85%) loss : 4.973  accuracy : 20.1 %\n",
      "60m 46s (- 10m 12s) (34250 85%) loss : 4.966  accuracy : 20.6 %\n",
      "61m 13s (- 9m 45s) (34500 86%) loss : 4.962  accuracy : 19.7 %\n",
      "61m 39s (- 9m 18s) (34750 86%) loss : 4.918  accuracy : 20.5 %\n",
      "62m 6s (- 8m 52s) (35000 87%) loss : 4.960  accuracy : 19.9 %\n",
      "62m 32s (- 8m 25s) (35250 88%) loss : 4.872  accuracy : 20.4 %\n",
      "62m 59s (- 7m 59s) (35500 88%) loss : 4.918  accuracy : 20.6 %\n",
      "63m 25s (- 7m 32s) (35750 89%) loss : 4.934  accuracy : 20.7 %\n",
      "63m 52s (- 7m 5s) (36000 90%) loss : 4.969  accuracy : 19.5 %\n",
      "64m 18s (- 6m 39s) (36250 90%) loss : 4.948  accuracy : 20.3 %\n",
      "64m 45s (- 6m 12s) (36500 91%) loss : 4.905  accuracy : 20.1 %\n",
      "65m 11s (- 5m 45s) (36750 91%) loss : 4.812  accuracy : 20.7 %\n",
      "65m 38s (- 5m 19s) (37000 92%) loss : 4.864  accuracy : 21.1 %\n",
      "66m 4s (- 4m 52s) (37250 93%) loss : 4.959  accuracy : 20.2 %\n",
      "66m 31s (- 4m 26s) (37500 93%) loss : 4.936  accuracy : 20.0 %\n",
      "66m 58s (- 3m 59s) (37750 94%) loss : 4.940  accuracy : 20.0 %\n",
      "67m 24s (- 3m 32s) (38000 95%) loss : 4.985  accuracy : 19.8 %\n",
      "67m 51s (- 3m 6s) (38250 95%) loss : 5.013  accuracy : 20.1 %\n",
      "68m 17s (- 2m 39s) (38500 96%) loss : 4.926  accuracy : 20.7 %\n",
      "68m 44s (- 2m 13s) (38750 96%) loss : 4.942  accuracy : 20.3 %\n",
      "69m 10s (- 1m 46s) (39000 97%) loss : 4.913  accuracy : 20.8 %\n",
      "69m 37s (- 1m 19s) (39250 98%) loss : 4.887  accuracy : 21.0 %\n",
      "70m 3s (- 0m 53s) (39500 98%) loss : 4.912  accuracy : 20.2 %\n",
      "70m 30s (- 0m 26s) (39750 99%) loss : 4.928  accuracy : 20.5 %\n",
      "70m 56s (- 0m 0s) (40000 100%) loss : 4.876  accuracy : 20.0 %\n",
      "epoch 1\n",
      "0m 24s (- 41m 43s) (250 0%) loss : 5.061  accuracy : 19.5 %\n",
      "0m 48s (- 41m 16s) (500 1%) loss : 5.139  accuracy : 19.8 %\n",
      "1m 12s (- 40m 54s) (750 2%) loss : 5.215  accuracy : 19.6 %\n",
      "1m 37s (- 40m 30s) (1000 3%) loss : 5.180  accuracy : 19.9 %\n",
      "2m 1s (- 40m 6s) (1250 4%) loss : 5.160  accuracy : 20.0 %\n",
      "2m 25s (- 39m 42s) (1500 5%) loss : 5.106  accuracy : 20.3 %\n",
      "2m 49s (- 39m 17s) (1750 6%) loss : 5.169  accuracy : 20.0 %\n",
      "3m 14s (- 38m 52s) (2000 7%) loss : 5.239  accuracy : 20.0 %\n",
      "3m 38s (- 38m 29s) (2250 8%) loss : 5.177  accuracy : 19.8 %\n",
      "4m 2s (- 38m 6s) (2500 9%) loss : 5.212  accuracy : 19.7 %\n",
      "4m 27s (- 37m 42s) (2750 10%) loss : 5.193  accuracy : 20.6 %\n",
      "4m 51s (- 37m 18s) (3000 11%) loss : 5.169  accuracy : 19.6 %\n",
      "5m 15s (- 36m 54s) (3250 12%) loss : 5.246  accuracy : 19.6 %\n",
      "5m 40s (- 36m 30s) (3500 13%) loss : 5.106  accuracy : 20.2 %\n",
      "6m 4s (- 36m 6s) (3750 14%) loss : 5.124  accuracy : 20.9 %\n",
      "6m 28s (- 35m 42s) (4000 15%) loss : 5.139  accuracy : 20.7 %\n",
      "6m 53s (- 35m 18s) (4250 16%) loss : 5.131  accuracy : 20.3 %\n",
      "7m 17s (- 34m 53s) (4500 17%) loss : 5.153  accuracy : 20.5 %\n",
      "7m 41s (- 34m 29s) (4750 18%) loss : 5.138  accuracy : 20.5 %\n",
      "8m 6s (- 34m 4s) (5000 19%) loss : 5.165  accuracy : 20.5 %\n",
      "8m 30s (- 33m 40s) (5250 20%) loss : 5.101  accuracy : 20.1 %\n",
      "8m 54s (- 33m 15s) (5500 21%) loss : 5.079  accuracy : 20.9 %\n",
      "9m 18s (- 32m 52s) (5750 22%) loss : 5.153  accuracy : 20.5 %\n",
      "9m 43s (- 32m 27s) (6000 23%) loss : 5.133  accuracy : 20.6 %\n",
      "10m 7s (- 32m 3s) (6250 24%) loss : 5.185  accuracy : 19.5 %\n",
      "10m 31s (- 31m 38s) (6500 24%) loss : 5.058  accuracy : 20.7 %\n",
      "10m 56s (- 31m 14s) (6750 25%) loss : 5.171  accuracy : 19.9 %\n",
      "11m 20s (- 30m 50s) (7000 26%) loss : 5.022  accuracy : 21.1 %\n",
      "11m 44s (- 30m 26s) (7250 27%) loss : 5.090  accuracy : 20.9 %\n",
      "12m 9s (- 30m 1s) (7500 28%) loss : 4.980  accuracy : 21.7 %\n",
      "12m 33s (- 29m 37s) (7750 29%) loss : 5.081  accuracy : 20.4 %\n",
      "12m 57s (- 29m 13s) (8000 30%) loss : 5.129  accuracy : 20.1 %\n",
      "13m 21s (- 28m 48s) (8250 31%) loss : 5.077  accuracy : 20.7 %\n",
      "13m 46s (- 28m 24s) (8500 32%) loss : 5.089  accuracy : 20.0 %\n",
      "14m 10s (- 28m 0s) (8750 33%) loss : 5.129  accuracy : 20.4 %\n",
      "14m 34s (- 27m 35s) (9000 34%) loss : 5.020  accuracy : 21.0 %\n",
      "14m 58s (- 27m 11s) (9250 35%) loss : 5.189  accuracy : 19.7 %\n",
      "15m 23s (- 26m 46s) (9500 36%) loss : 5.136  accuracy : 21.4 %\n",
      "15m 47s (- 26m 22s) (9750 37%) loss : 5.139  accuracy : 20.4 %\n",
      "16m 11s (- 25m 58s) (10000 38%) loss : 5.009  accuracy : 21.6 %\n",
      "16m 35s (- 25m 33s) (10250 39%) loss : 5.051  accuracy : 21.1 %\n",
      "17m 0s (- 25m 9s) (10500 40%) loss : 5.029  accuracy : 21.4 %\n",
      "17m 24s (- 24m 45s) (10750 41%) loss : 5.125  accuracy : 20.5 %\n",
      "17m 48s (- 24m 20s) (11000 42%) loss : 5.243  accuracy : 19.7 %\n",
      "18m 13s (- 23m 56s) (11250 43%) loss : 5.070  accuracy : 20.6 %\n",
      "18m 37s (- 23m 32s) (11500 44%) loss : 5.047  accuracy : 20.5 %\n",
      "19m 1s (- 23m 8s) (11750 45%) loss : 4.945  accuracy : 21.3 %\n",
      "19m 25s (- 22m 43s) (12000 46%) loss : 4.992  accuracy : 21.3 %\n",
      "19m 50s (- 22m 19s) (12250 47%) loss : 5.076  accuracy : 21.1 %\n",
      "20m 14s (- 21m 55s) (12500 48%) loss : 5.104  accuracy : 20.5 %\n",
      "20m 38s (- 21m 30s) (12750 48%) loss : 5.087  accuracy : 20.4 %\n",
      "21m 3s (- 21m 6s) (13000 49%) loss : 5.003  accuracy : 21.4 %\n",
      "21m 27s (- 20m 42s) (13250 50%) loss : 5.019  accuracy : 21.8 %\n",
      "21m 51s (- 20m 17s) (13500 51%) loss : 5.012  accuracy : 20.4 %\n",
      "22m 15s (- 19m 53s) (13750 52%) loss : 5.105  accuracy : 20.2 %\n",
      "22m 40s (- 19m 29s) (14000 53%) loss : 5.033  accuracy : 21.2 %\n",
      "23m 4s (- 19m 4s) (14250 54%) loss : 5.043  accuracy : 21.3 %\n",
      "23m 28s (- 18m 40s) (14500 55%) loss : 4.985  accuracy : 20.9 %\n",
      "23m 52s (- 18m 16s) (14750 56%) loss : 5.001  accuracy : 21.1 %\n",
      "24m 17s (- 17m 52s) (15000 57%) loss : 4.932  accuracy : 21.7 %\n",
      "24m 41s (- 17m 27s) (15250 58%) loss : 4.927  accuracy : 21.7 %\n",
      "25m 5s (- 17m 3s) (15500 59%) loss : 5.022  accuracy : 20.5 %\n",
      "25m 30s (- 16m 39s) (15750 60%) loss : 5.064  accuracy : 20.6 %\n",
      "25m 54s (- 16m 14s) (16000 61%) loss : 5.023  accuracy : 20.8 %\n",
      "26m 18s (- 15m 50s) (16250 62%) loss : 5.000  accuracy : 21.2 %\n",
      "26m 43s (- 15m 26s) (16500 63%) loss : 5.067  accuracy : 20.2 %\n",
      "27m 7s (- 15m 2s) (16750 64%) loss : 4.952  accuracy : 20.8 %\n",
      "27m 31s (- 14m 37s) (17000 65%) loss : 4.968  accuracy : 21.4 %\n",
      "27m 56s (- 14m 13s) (17250 66%) loss : 4.982  accuracy : 21.5 %\n",
      "28m 20s (- 13m 49s) (17500 67%) loss : 5.039  accuracy : 20.4 %\n",
      "28m 44s (- 13m 24s) (17750 68%) loss : 5.063  accuracy : 20.3 %\n",
      "29m 8s (- 13m 0s) (18000 69%) loss : 4.991  accuracy : 21.5 %\n",
      "29m 33s (- 12m 36s) (18250 70%) loss : 5.008  accuracy : 21.6 %\n",
      "29m 57s (- 12m 12s) (18500 71%) loss : 5.003  accuracy : 21.1 %\n",
      "30m 21s (- 11m 47s) (18750 72%) loss : 4.932  accuracy : 21.4 %\n",
      "30m 45s (- 11m 23s) (19000 72%) loss : 4.959  accuracy : 21.5 %\n",
      "31m 10s (- 10m 59s) (19250 73%) loss : 5.047  accuracy : 20.5 %\n",
      "31m 34s (- 10m 34s) (19500 74%) loss : 4.944  accuracy : 21.0 %\n",
      "31m 58s (- 10m 10s) (19750 75%) loss : 5.062  accuracy : 21.1 %\n",
      "32m 22s (- 9m 46s) (20000 76%) loss : 4.911  accuracy : 21.6 %\n",
      "32m 47s (- 9m 21s) (20250 77%) loss : 4.955  accuracy : 21.7 %\n",
      "33m 11s (- 8m 57s) (20500 78%) loss : 4.954  accuracy : 21.2 %\n",
      "33m 35s (- 8m 33s) (20750 79%) loss : 5.082  accuracy : 20.8 %\n",
      "33m 59s (- 8m 9s) (21000 80%) loss : 5.031  accuracy : 20.8 %\n",
      "34m 24s (- 7m 44s) (21250 81%) loss : 5.060  accuracy : 20.5 %\n",
      "34m 48s (- 7m 20s) (21500 82%) loss : 4.991  accuracy : 21.3 %\n",
      "35m 12s (- 6m 56s) (21750 83%) loss : 4.913  accuracy : 21.4 %\n",
      "35m 36s (- 6m 31s) (22000 84%) loss : 4.956  accuracy : 21.0 %\n",
      "36m 1s (- 6m 7s) (22250 85%) loss : 4.971  accuracy : 21.8 %\n",
      "36m 25s (- 5m 43s) (22500 86%) loss : 5.091  accuracy : 20.9 %\n",
      "36m 49s (- 5m 19s) (22750 87%) loss : 4.911  accuracy : 21.6 %\n",
      "37m 13s (- 4m 54s) (23000 88%) loss : 5.001  accuracy : 21.0 %\n",
      "37m 38s (- 4m 30s) (23250 89%) loss : 4.994  accuracy : 21.1 %\n",
      "38m 2s (- 4m 6s) (23500 90%) loss : 5.029  accuracy : 21.8 %\n",
      "38m 26s (- 3m 41s) (23750 91%) loss : 5.007  accuracy : 21.6 %\n",
      "38m 51s (- 3m 17s) (24000 92%) loss : 5.046  accuracy : 20.9 %\n",
      "39m 15s (- 2m 53s) (24250 93%) loss : 4.899  accuracy : 21.4 %\n",
      "39m 39s (- 2m 29s) (24500 94%) loss : 4.962  accuracy : 21.9 %\n",
      "40m 3s (- 2m 4s) (24750 95%) loss : 4.946  accuracy : 21.7 %\n",
      "40m 28s (- 1m 40s) (25000 96%) loss : 4.902  accuracy : 21.8 %\n",
      "40m 52s (- 1m 16s) (25250 96%) loss : 5.061  accuracy : 20.2 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41m 16s (- 0m 51s) (25500 97%) loss : 5.037  accuracy : 21.7 %\n",
      "41m 40s (- 0m 27s) (25750 98%) loss : 4.960  accuracy : 21.7 %\n",
      "42m 5s (- 0m 3s) (26000 99%) loss : 4.970  accuracy : 22.1 %\n"
     ]
    }
   ],
   "source": [
    "language_model.train()\n",
    "language_model.fit(batches[:40000], epochs = 1, lr = 0.001, print_every = 250)\n",
    "language_model.fit(batches[40000:], epochs = 1, lr = 0.0005, print_every = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 26s (- 70m 13s) (250 0%) loss : 4.761  accuracy : 22.0 %\n",
      "0m 53s (- 69m 48s) (500 1%) loss : 4.810  accuracy : 21.4 %\n",
      "1m 19s (- 69m 23s) (750 1%) loss : 4.814  accuracy : 21.5 %\n",
      "1m 46s (- 69m 1s) (1000 2%) loss : 4.856  accuracy : 22.6 %\n",
      "2m 12s (- 68m 32s) (1250 3%) loss : 4.846  accuracy : 22.4 %\n",
      "2m 39s (- 68m 5s) (1500 3%) loss : 4.813  accuracy : 22.6 %\n",
      "3m 5s (- 67m 40s) (1750 4%) loss : 4.851  accuracy : 21.9 %\n",
      "3m 32s (- 67m 13s) (2000 5%) loss : 4.798  accuracy : 22.9 %\n",
      "3m 58s (- 66m 45s) (2250 5%) loss : 4.836  accuracy : 22.4 %\n",
      "4m 25s (- 66m 18s) (2500 6%) loss : 4.860  accuracy : 22.2 %\n",
      "4m 51s (- 65m 52s) (2750 6%) loss : 4.777  accuracy : 23.4 %\n",
      "5m 18s (- 65m 28s) (3000 7%) loss : 4.794  accuracy : 23.0 %\n",
      "5m 45s (- 65m 1s) (3250 8%) loss : 4.722  accuracy : 23.2 %\n",
      "6m 11s (- 64m 34s) (3500 8%) loss : 4.773  accuracy : 23.2 %\n",
      "6m 38s (- 64m 9s) (3750 9%) loss : 4.817  accuracy : 22.6 %\n",
      "7m 4s (- 63m 43s) (4000 10%) loss : 4.779  accuracy : 22.6 %\n",
      "7m 31s (- 63m 15s) (4250 10%) loss : 4.810  accuracy : 23.5 %\n",
      "7m 57s (- 62m 47s) (4500 11%) loss : 4.905  accuracy : 21.8 %\n",
      "8m 24s (- 62m 21s) (4750 11%) loss : 4.892  accuracy : 22.5 %\n",
      "8m 50s (- 61m 55s) (5000 12%) loss : 4.900  accuracy : 22.4 %\n",
      "9m 17s (- 61m 28s) (5250 13%) loss : 4.849  accuracy : 22.3 %\n",
      "9m 43s (- 61m 2s) (5500 13%) loss : 4.865  accuracy : 22.7 %\n",
      "10m 10s (- 60m 35s) (5750 14%) loss : 4.821  accuracy : 22.5 %\n",
      "10m 36s (- 60m 8s) (6000 15%) loss : 4.832  accuracy : 22.6 %\n",
      "11m 3s (- 59m 42s) (6250 15%) loss : 4.787  accuracy : 22.9 %\n",
      "11m 30s (- 59m 16s) (6500 16%) loss : 4.823  accuracy : 23.3 %\n",
      "11m 56s (- 58m 49s) (6750 16%) loss : 4.736  accuracy : 23.2 %\n",
      "12m 23s (- 58m 23s) (7000 17%) loss : 4.756  accuracy : 23.0 %\n",
      "12m 49s (- 57m 56s) (7250 18%) loss : 4.761  accuracy : 23.4 %\n",
      "13m 16s (- 57m 29s) (7500 18%) loss : 4.716  accuracy : 23.7 %\n",
      "13m 42s (- 57m 2s) (7750 19%) loss : 4.764  accuracy : 22.7 %\n",
      "14m 8s (- 56m 35s) (8000 20%) loss : 4.787  accuracy : 22.7 %\n",
      "14m 35s (- 56m 9s) (8250 20%) loss : 4.698  accuracy : 23.6 %\n",
      "15m 1s (- 55m 42s) (8500 21%) loss : 4.711  accuracy : 23.1 %\n",
      "15m 28s (- 55m 16s) (8750 21%) loss : 4.831  accuracy : 22.9 %\n",
      "15m 55s (- 54m 49s) (9000 22%) loss : 4.788  accuracy : 23.0 %\n",
      "16m 21s (- 54m 23s) (9250 23%) loss : 4.753  accuracy : 22.7 %\n",
      "16m 47s (- 53m 56s) (9500 23%) loss : 4.766  accuracy : 23.3 %\n",
      "17m 14s (- 53m 29s) (9750 24%) loss : 4.743  accuracy : 23.2 %\n",
      "17m 40s (- 53m 2s) (10000 25%) loss : 4.713  accuracy : 23.4 %\n",
      "18m 7s (- 52m 36s) (10250 25%) loss : 4.697  accuracy : 23.2 %\n",
      "18m 33s (- 52m 9s) (10500 26%) loss : 4.729  accuracy : 22.9 %\n",
      "19m 0s (- 51m 43s) (10750 26%) loss : 4.637  accuracy : 23.8 %\n",
      "19m 27s (- 51m 17s) (11000 27%) loss : 4.771  accuracy : 22.7 %\n",
      "19m 53s (- 50m 50s) (11250 28%) loss : 4.718  accuracy : 23.0 %\n",
      "20m 20s (- 50m 24s) (11500 28%) loss : 4.689  accuracy : 23.1 %\n",
      "20m 46s (- 49m 57s) (11750 29%) loss : 4.791  accuracy : 22.8 %\n",
      "21m 13s (- 49m 30s) (12000 30%) loss : 4.740  accuracy : 23.0 %\n",
      "21m 39s (- 49m 4s) (12250 30%) loss : 4.619  accuracy : 23.8 %\n",
      "22m 6s (- 48m 37s) (12500 31%) loss : 4.774  accuracy : 23.0 %\n",
      "22m 32s (- 48m 11s) (12750 31%) loss : 4.789  accuracy : 23.8 %\n",
      "22m 59s (- 47m 44s) (13000 32%) loss : 4.762  accuracy : 23.5 %\n",
      "23m 25s (- 47m 18s) (13250 33%) loss : 4.646  accuracy : 23.6 %\n",
      "23m 52s (- 46m 52s) (13500 33%) loss : 4.686  accuracy : 23.3 %\n",
      "24m 19s (- 46m 25s) (13750 34%) loss : 4.593  accuracy : 24.2 %\n",
      "24m 45s (- 45m 59s) (14000 35%) loss : 4.633  accuracy : 24.0 %\n",
      "25m 12s (- 45m 32s) (14250 35%) loss : 4.670  accuracy : 23.3 %\n",
      "25m 38s (- 45m 6s) (14500 36%) loss : 4.564  accuracy : 24.2 %\n",
      "26m 5s (- 44m 39s) (14750 36%) loss : 4.813  accuracy : 22.4 %\n",
      "26m 32s (- 44m 13s) (15000 37%) loss : 4.658  accuracy : 23.6 %\n",
      "26m 58s (- 43m 46s) (15250 38%) loss : 4.653  accuracy : 24.1 %\n",
      "27m 24s (- 43m 20s) (15500 38%) loss : 4.697  accuracy : 23.5 %\n",
      "27m 51s (- 42m 53s) (15750 39%) loss : 4.651  accuracy : 23.4 %\n",
      "28m 17s (- 42m 26s) (16000 40%) loss : 4.666  accuracy : 23.8 %\n",
      "28m 44s (- 42m 0s) (16250 40%) loss : 4.616  accuracy : 24.1 %\n",
      "29m 10s (- 41m 33s) (16500 41%) loss : 4.657  accuracy : 24.0 %\n",
      "29m 37s (- 41m 6s) (16750 41%) loss : 4.694  accuracy : 22.6 %\n",
      "30m 3s (- 40m 40s) (17000 42%) loss : 4.670  accuracy : 23.5 %\n",
      "30m 30s (- 40m 13s) (17250 43%) loss : 4.582  accuracy : 23.8 %\n",
      "30m 56s (- 39m 47s) (17500 43%) loss : 4.678  accuracy : 23.4 %\n",
      "31m 23s (- 39m 20s) (17750 44%) loss : 4.756  accuracy : 22.4 %\n",
      "31m 49s (- 38m 53s) (18000 45%) loss : 4.611  accuracy : 23.9 %\n",
      "32m 15s (- 38m 27s) (18250 45%) loss : 4.639  accuracy : 23.8 %\n",
      "32m 42s (- 38m 0s) (18500 46%) loss : 4.648  accuracy : 23.9 %\n",
      "33m 8s (- 37m 33s) (18750 46%) loss : 4.618  accuracy : 23.5 %\n",
      "33m 35s (- 37m 7s) (19000 47%) loss : 4.615  accuracy : 23.9 %\n",
      "34m 1s (- 36m 40s) (19250 48%) loss : 4.657  accuracy : 23.6 %\n",
      "34m 28s (- 36m 14s) (19500 48%) loss : 4.625  accuracy : 23.9 %\n",
      "34m 54s (- 35m 47s) (19750 49%) loss : 4.643  accuracy : 23.2 %\n",
      "35m 21s (- 35m 21s) (20000 50%) loss : 4.646  accuracy : 23.6 %\n",
      "35m 47s (- 34m 54s) (20250 50%) loss : 4.644  accuracy : 23.7 %\n",
      "36m 14s (- 34m 28s) (20500 51%) loss : 4.648  accuracy : 23.4 %\n",
      "36m 40s (- 34m 1s) (20750 51%) loss : 4.561  accuracy : 24.5 %\n",
      "37m 7s (- 33m 35s) (21000 52%) loss : 4.624  accuracy : 23.3 %\n",
      "37m 33s (- 33m 8s) (21250 53%) loss : 4.616  accuracy : 23.6 %\n",
      "38m 0s (- 32m 42s) (21500 53%) loss : 4.606  accuracy : 23.6 %\n",
      "38m 27s (- 32m 15s) (21750 54%) loss : 4.721  accuracy : 22.6 %\n",
      "38m 53s (- 31m 49s) (22000 55%) loss : 4.604  accuracy : 23.6 %\n",
      "39m 20s (- 31m 22s) (22250 55%) loss : 4.628  accuracy : 23.4 %\n",
      "39m 46s (- 30m 56s) (22500 56%) loss : 4.523  accuracy : 24.7 %\n",
      "40m 13s (- 30m 29s) (22750 56%) loss : 4.651  accuracy : 22.9 %\n",
      "40m 39s (- 30m 3s) (23000 57%) loss : 4.628  accuracy : 23.0 %\n",
      "41m 6s (- 29m 36s) (23250 58%) loss : 4.587  accuracy : 23.8 %\n",
      "41m 32s (- 29m 10s) (23500 58%) loss : 4.594  accuracy : 23.5 %\n",
      "41m 59s (- 28m 43s) (23750 59%) loss : 4.663  accuracy : 23.1 %\n",
      "42m 25s (- 28m 16s) (24000 60%) loss : 4.643  accuracy : 23.6 %\n",
      "42m 52s (- 27m 50s) (24250 60%) loss : 4.549  accuracy : 24.0 %\n",
      "43m 18s (- 27m 23s) (24500 61%) loss : 4.599  accuracy : 23.7 %\n",
      "43m 45s (- 26m 57s) (24750 61%) loss : 4.706  accuracy : 22.8 %\n",
      "44m 11s (- 26m 30s) (25000 62%) loss : 4.531  accuracy : 23.6 %\n",
      "44m 38s (- 26m 4s) (25250 63%) loss : 4.617  accuracy : 23.4 %\n",
      "45m 4s (- 25m 37s) (25500 63%) loss : 4.518  accuracy : 23.7 %\n",
      "45m 31s (- 25m 11s) (25750 64%) loss : 4.580  accuracy : 24.0 %\n",
      "45m 57s (- 24m 44s) (26000 65%) loss : 4.506  accuracy : 24.6 %\n",
      "46m 24s (- 24m 18s) (26250 65%) loss : 4.511  accuracy : 24.5 %\n",
      "46m 50s (- 23m 51s) (26500 66%) loss : 4.711  accuracy : 22.8 %\n",
      "47m 17s (- 23m 25s) (26750 66%) loss : 4.479  accuracy : 24.2 %\n",
      "47m 43s (- 22m 58s) (27000 67%) loss : 4.604  accuracy : 23.5 %\n",
      "48m 10s (- 22m 32s) (27250 68%) loss : 4.607  accuracy : 23.7 %\n",
      "48m 36s (- 22m 5s) (27500 68%) loss : 4.619  accuracy : 23.4 %\n",
      "49m 3s (- 21m 39s) (27750 69%) loss : 4.463  accuracy : 24.5 %\n",
      "49m 29s (- 21m 12s) (28000 70%) loss : 4.608  accuracy : 23.7 %\n",
      "49m 56s (- 20m 46s) (28250 70%) loss : 4.578  accuracy : 24.2 %\n",
      "50m 22s (- 20m 19s) (28500 71%) loss : 4.644  accuracy : 23.4 %\n",
      "50m 49s (- 19m 53s) (28750 71%) loss : 4.560  accuracy : 24.4 %\n",
      "51m 15s (- 19m 26s) (29000 72%) loss : 4.568  accuracy : 24.3 %\n",
      "51m 42s (- 19m 0s) (29250 73%) loss : 4.538  accuracy : 24.0 %\n",
      "52m 8s (- 18m 33s) (29500 73%) loss : 4.648  accuracy : 23.6 %\n",
      "52m 35s (- 18m 7s) (29750 74%) loss : 4.547  accuracy : 24.2 %\n",
      "53m 1s (- 17m 40s) (30000 75%) loss : 4.525  accuracy : 24.1 %\n",
      "53m 28s (- 17m 14s) (30250 75%) loss : 4.516  accuracy : 24.3 %\n",
      "53m 55s (- 16m 47s) (30500 76%) loss : 4.436  accuracy : 24.5 %\n",
      "54m 21s (- 16m 21s) (30750 76%) loss : 4.549  accuracy : 24.0 %\n",
      "54m 48s (- 15m 54s) (31000 77%) loss : 4.605  accuracy : 23.1 %\n",
      "55m 14s (- 15m 28s) (31250 78%) loss : 4.504  accuracy : 24.3 %\n",
      "55m 41s (- 15m 1s) (31500 78%) loss : 4.550  accuracy : 24.2 %\n",
      "56m 7s (- 14m 35s) (31750 79%) loss : 4.457  accuracy : 24.3 %\n",
      "56m 33s (- 14m 8s) (32000 80%) loss : 4.488  accuracy : 24.9 %\n",
      "57m 0s (- 13m 41s) (32250 80%) loss : 4.459  accuracy : 24.4 %\n",
      "57m 26s (- 13m 15s) (32500 81%) loss : 4.455  accuracy : 24.8 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57m 53s (- 12m 48s) (32750 81%) loss : 4.543  accuracy : 24.5 %\n",
      "58m 20s (- 12m 22s) (33000 82%) loss : 4.530  accuracy : 23.9 %\n",
      "58m 46s (- 11m 55s) (33250 83%) loss : 4.557  accuracy : 23.3 %\n",
      "59m 13s (- 11m 29s) (33500 83%) loss : 4.510  accuracy : 24.1 %\n",
      "59m 39s (- 11m 2s) (33750 84%) loss : 4.581  accuracy : 23.6 %\n",
      "60m 6s (- 10m 36s) (34000 85%) loss : 4.551  accuracy : 24.2 %\n",
      "60m 32s (- 10m 9s) (34250 85%) loss : 4.515  accuracy : 24.8 %\n",
      "60m 59s (- 9m 43s) (34500 86%) loss : 4.524  accuracy : 23.5 %\n",
      "61m 25s (- 9m 16s) (34750 86%) loss : 4.483  accuracy : 24.6 %\n",
      "61m 52s (- 8m 50s) (35000 87%) loss : 4.521  accuracy : 23.6 %\n",
      "62m 18s (- 8m 23s) (35250 88%) loss : 4.443  accuracy : 24.4 %\n",
      "62m 45s (- 7m 57s) (35500 88%) loss : 4.508  accuracy : 24.6 %\n",
      "63m 11s (- 7m 30s) (35750 89%) loss : 4.478  accuracy : 24.6 %\n",
      "63m 38s (- 7m 4s) (36000 90%) loss : 4.536  accuracy : 23.7 %\n",
      "64m 4s (- 6m 37s) (36250 90%) loss : 4.521  accuracy : 23.8 %\n",
      "64m 31s (- 6m 11s) (36500 91%) loss : 4.501  accuracy : 23.9 %\n",
      "64m 57s (- 5m 44s) (36750 91%) loss : 4.404  accuracy : 25.0 %\n",
      "65m 24s (- 5m 18s) (37000 92%) loss : 4.424  accuracy : 25.4 %\n",
      "65m 50s (- 4m 51s) (37250 93%) loss : 4.525  accuracy : 24.1 %\n",
      "66m 17s (- 4m 25s) (37500 93%) loss : 4.500  accuracy : 24.0 %\n",
      "66m 44s (- 3m 58s) (37750 94%) loss : 4.512  accuracy : 24.0 %\n",
      "67m 10s (- 3m 32s) (38000 95%) loss : 4.561  accuracy : 24.5 %\n",
      "67m 36s (- 3m 5s) (38250 95%) loss : 4.606  accuracy : 23.9 %\n",
      "68m 3s (- 2m 39s) (38500 96%) loss : 4.515  accuracy : 24.3 %\n",
      "68m 29s (- 2m 12s) (38750 96%) loss : 4.509  accuracy : 24.2 %\n",
      "68m 56s (- 1m 46s) (39000 97%) loss : 4.495  accuracy : 24.9 %\n",
      "69m 23s (- 1m 19s) (39250 98%) loss : 4.478  accuracy : 24.4 %\n",
      "69m 49s (- 0m 53s) (39500 98%) loss : 4.516  accuracy : 24.6 %\n",
      "70m 16s (- 0m 26s) (39750 99%) loss : 4.520  accuracy : 24.1 %\n",
      "70m 42s (- 0m 0s) (40000 100%) loss : 4.468  accuracy : 24.0 %\n",
      "epoch 1\n",
      "0m 24s (- 41m 42s) (250 0%) loss : 4.701  accuracy : 22.4 %\n",
      "0m 48s (- 41m 23s) (500 1%) loss : 4.719  accuracy : 22.6 %\n",
      "1m 12s (- 40m 55s) (750 2%) loss : 4.801  accuracy : 22.6 %\n",
      "1m 37s (- 40m 30s) (1000 3%) loss : 4.742  accuracy : 22.8 %\n",
      "2m 1s (- 40m 7s) (1250 4%) loss : 4.747  accuracy : 23.0 %\n",
      "2m 25s (- 39m 42s) (1500 5%) loss : 4.688  accuracy : 23.5 %\n",
      "2m 49s (- 39m 18s) (1750 6%) loss : 4.751  accuracy : 22.9 %\n",
      "3m 14s (- 38m 54s) (2000 7%) loss : 4.832  accuracy : 22.6 %\n",
      "3m 38s (- 38m 30s) (2250 8%) loss : 4.772  accuracy : 23.0 %\n",
      "4m 2s (- 38m 5s) (2500 9%) loss : 4.830  accuracy : 22.6 %\n",
      "4m 27s (- 37m 40s) (2750 10%) loss : 4.819  accuracy : 22.9 %\n",
      "4m 51s (- 37m 16s) (3000 11%) loss : 4.776  accuracy : 22.9 %\n",
      "5m 15s (- 36m 52s) (3250 12%) loss : 4.880  accuracy : 21.3 %\n",
      "5m 39s (- 36m 28s) (3500 13%) loss : 4.745  accuracy : 23.2 %\n",
      "6m 4s (- 36m 3s) (3750 14%) loss : 4.778  accuracy : 23.3 %\n",
      "6m 28s (- 35m 39s) (4000 15%) loss : 4.761  accuracy : 23.2 %\n",
      "6m 52s (- 35m 15s) (4250 16%) loss : 4.783  accuracy : 22.8 %\n",
      "7m 16s (- 34m 51s) (4500 17%) loss : 4.800  accuracy : 23.0 %\n",
      "7m 41s (- 34m 26s) (4750 18%) loss : 4.778  accuracy : 23.3 %\n",
      "8m 5s (- 34m 1s) (5000 19%) loss : 4.826  accuracy : 23.2 %\n",
      "8m 29s (- 33m 37s) (5250 20%) loss : 4.760  accuracy : 22.5 %\n",
      "8m 53s (- 33m 12s) (5500 21%) loss : 4.730  accuracy : 24.2 %\n",
      "9m 17s (- 32m 48s) (5750 22%) loss : 4.816  accuracy : 22.8 %\n",
      "9m 42s (- 32m 24s) (6000 23%) loss : 4.821  accuracy : 23.1 %\n",
      "10m 6s (- 31m 59s) (6250 24%) loss : 4.868  accuracy : 22.1 %\n",
      "10m 30s (- 31m 35s) (6500 24%) loss : 4.738  accuracy : 23.1 %\n",
      "10m 54s (- 31m 11s) (6750 25%) loss : 4.845  accuracy : 22.7 %\n",
      "11m 19s (- 30m 47s) (7000 26%) loss : 4.689  accuracy : 23.7 %\n",
      "11m 43s (- 30m 22s) (7250 27%) loss : 4.788  accuracy : 23.2 %\n",
      "12m 7s (- 29m 58s) (7500 28%) loss : 4.684  accuracy : 24.1 %\n",
      "12m 32s (- 29m 34s) (7750 29%) loss : 4.755  accuracy : 23.2 %\n",
      "12m 56s (- 29m 10s) (8000 30%) loss : 4.821  accuracy : 22.7 %\n",
      "13m 20s (- 28m 46s) (8250 31%) loss : 4.767  accuracy : 22.8 %\n",
      "13m 45s (- 28m 21s) (8500 32%) loss : 4.780  accuracy : 22.6 %\n",
      "14m 9s (- 27m 57s) (8750 33%) loss : 4.819  accuracy : 22.9 %\n",
      "14m 33s (- 27m 33s) (9000 34%) loss : 4.733  accuracy : 23.4 %\n",
      "14m 57s (- 27m 9s) (9250 35%) loss : 4.906  accuracy : 22.1 %\n",
      "15m 22s (- 26m 44s) (9500 36%) loss : 4.835  accuracy : 23.3 %\n",
      "15m 46s (- 26m 20s) (9750 37%) loss : 4.833  accuracy : 22.8 %\n",
      "16m 10s (- 25m 56s) (10000 38%) loss : 4.727  accuracy : 23.6 %\n",
      "16m 34s (- 25m 32s) (10250 39%) loss : 4.758  accuracy : 23.3 %\n",
      "16m 59s (- 25m 8s) (10500 40%) loss : 4.743  accuracy : 24.1 %\n",
      "17m 23s (- 24m 43s) (10750 41%) loss : 4.841  accuracy : 22.9 %\n",
      "17m 47s (- 24m 19s) (11000 42%) loss : 4.961  accuracy : 21.8 %\n",
      "18m 11s (- 23m 54s) (11250 43%) loss : 4.781  accuracy : 23.1 %\n",
      "18m 36s (- 23m 30s) (11500 44%) loss : 4.774  accuracy : 23.0 %\n",
      "19m 0s (- 23m 6s) (11750 45%) loss : 4.674  accuracy : 23.7 %\n",
      "19m 24s (- 22m 42s) (12000 46%) loss : 4.703  accuracy : 23.7 %\n",
      "19m 49s (- 22m 18s) (12250 47%) loss : 4.788  accuracy : 23.7 %\n",
      "20m 13s (- 21m 53s) (12500 48%) loss : 4.843  accuracy : 22.3 %\n",
      "20m 37s (- 21m 29s) (12750 48%) loss : 4.814  accuracy : 22.9 %\n",
      "21m 1s (- 21m 5s) (13000 49%) loss : 4.737  accuracy : 23.9 %\n",
      "21m 26s (- 20m 41s) (13250 50%) loss : 4.754  accuracy : 23.9 %\n",
      "21m 50s (- 20m 16s) (13500 51%) loss : 4.744  accuracy : 23.5 %\n",
      "22m 14s (- 19m 52s) (13750 52%) loss : 4.849  accuracy : 22.8 %\n",
      "22m 38s (- 19m 28s) (14000 53%) loss : 4.777  accuracy : 23.5 %\n",
      "23m 3s (- 19m 3s) (14250 54%) loss : 4.813  accuracy : 23.2 %\n",
      "23m 27s (- 18m 39s) (14500 55%) loss : 4.718  accuracy : 23.4 %\n",
      "23m 51s (- 18m 15s) (14750 56%) loss : 4.740  accuracy : 23.3 %\n",
      "24m 15s (- 17m 51s) (15000 57%) loss : 4.679  accuracy : 23.7 %\n",
      "24m 40s (- 17m 26s) (15250 58%) loss : 4.669  accuracy : 24.0 %\n",
      "25m 4s (- 17m 2s) (15500 59%) loss : 4.755  accuracy : 22.8 %\n",
      "25m 28s (- 16m 38s) (15750 60%) loss : 4.794  accuracy : 23.1 %\n",
      "25m 53s (- 16m 14s) (16000 61%) loss : 4.749  accuracy : 23.3 %\n",
      "26m 17s (- 15m 49s) (16250 62%) loss : 4.731  accuracy : 23.6 %\n",
      "26m 41s (- 15m 25s) (16500 63%) loss : 4.800  accuracy : 22.3 %\n",
      "27m 5s (- 15m 1s) (16750 64%) loss : 4.693  accuracy : 23.4 %\n",
      "27m 30s (- 14m 36s) (17000 65%) loss : 4.730  accuracy : 23.4 %\n",
      "27m 54s (- 14m 12s) (17250 66%) loss : 4.731  accuracy : 23.8 %\n",
      "28m 18s (- 13m 48s) (17500 67%) loss : 4.805  accuracy : 22.7 %\n",
      "28m 42s (- 13m 24s) (17750 68%) loss : 4.808  accuracy : 22.8 %\n",
      "29m 7s (- 12m 59s) (18000 69%) loss : 4.760  accuracy : 23.4 %\n",
      "29m 31s (- 12m 35s) (18250 70%) loss : 4.757  accuracy : 23.6 %\n",
      "29m 55s (- 12m 11s) (18500 71%) loss : 4.750  accuracy : 23.5 %\n",
      "30m 20s (- 11m 47s) (18750 72%) loss : 4.688  accuracy : 24.0 %\n",
      "30m 44s (- 11m 22s) (19000 72%) loss : 4.721  accuracy : 23.5 %\n",
      "31m 8s (- 10m 58s) (19250 73%) loss : 4.805  accuracy : 23.0 %\n",
      "31m 32s (- 10m 34s) (19500 74%) loss : 4.715  accuracy : 23.2 %\n",
      "31m 57s (- 10m 10s) (19750 75%) loss : 4.803  accuracy : 23.2 %\n",
      "32m 21s (- 9m 45s) (20000 76%) loss : 4.673  accuracy : 23.7 %\n",
      "32m 45s (- 9m 21s) (20250 77%) loss : 4.727  accuracy : 23.6 %\n",
      "33m 10s (- 8m 57s) (20500 78%) loss : 4.718  accuracy : 23.7 %\n",
      "33m 34s (- 8m 33s) (20750 79%) loss : 4.857  accuracy : 23.0 %\n",
      "33m 58s (- 8m 8s) (21000 80%) loss : 4.795  accuracy : 23.2 %\n",
      "34m 22s (- 7m 44s) (21250 81%) loss : 4.807  accuracy : 22.8 %\n",
      "34m 47s (- 7m 20s) (21500 82%) loss : 4.764  accuracy : 23.5 %\n",
      "35m 11s (- 6m 55s) (21750 83%) loss : 4.668  accuracy : 23.8 %\n",
      "35m 35s (- 6m 31s) (22000 84%) loss : 4.736  accuracy : 23.3 %\n",
      "35m 59s (- 6m 7s) (22250 85%) loss : 4.743  accuracy : 23.5 %\n",
      "36m 23s (- 5m 43s) (22500 86%) loss : 4.862  accuracy : 23.0 %\n",
      "36m 48s (- 5m 18s) (22750 87%) loss : 4.705  accuracy : 24.1 %\n",
      "37m 12s (- 4m 54s) (23000 88%) loss : 4.790  accuracy : 23.4 %\n",
      "37m 36s (- 4m 30s) (23250 89%) loss : 4.784  accuracy : 22.7 %\n",
      "38m 0s (- 4m 6s) (23500 90%) loss : 4.816  accuracy : 23.3 %\n",
      "38m 25s (- 3m 41s) (23750 91%) loss : 4.795  accuracy : 24.0 %\n",
      "38m 49s (- 3m 17s) (24000 92%) loss : 4.836  accuracy : 22.9 %\n",
      "39m 13s (- 2m 53s) (24250 93%) loss : 4.664  accuracy : 23.7 %\n",
      "39m 37s (- 2m 28s) (24500 94%) loss : 4.743  accuracy : 23.9 %\n",
      "40m 2s (- 2m 4s) (24750 95%) loss : 4.732  accuracy : 23.6 %\n",
      "40m 26s (- 1m 40s) (25000 96%) loss : 4.686  accuracy : 24.0 %\n",
      "40m 50s (- 1m 16s) (25250 96%) loss : 4.820  accuracy : 22.3 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41m 15s (- 0m 51s) (25500 97%) loss : 4.835  accuracy : 23.8 %\n",
      "41m 39s (- 0m 27s) (25750 98%) loss : 4.752  accuracy : 23.5 %\n",
      "42m 3s (- 0m 3s) (26000 99%) loss : 4.754  accuracy : 23.7 %\n"
     ]
    }
   ],
   "source": [
    "language_model.train()\n",
    "language_model.fit(batches[:40000], epochs = 1, lr = 0.00025, print_every = 250)\n",
    "language_model.fit(batches[40000:], epochs = 1, lr = 0.0001, print_every = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(language_model.state_dict(), path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I3_language_model.pth')\n",
    "\n",
    "# load\n",
    "#language_model.load_state_dict(torch.load(path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I3_language_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iran \u001b[48;2;255;229;217m 's nuclear program , which is not to be used to develop nuclear weapons . \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# fastText gensim, n_layers = 3, dh = 50\n",
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
