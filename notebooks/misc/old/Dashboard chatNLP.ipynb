{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard chatNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des matières\n",
    "\n",
    "1. [Modèles](#modeles)\n",
    "\n",
    "|  | Sans mémoire | Avec mémoire à règles | Avec mémoire agnostique|\n",
    "|------|------|------|\n",
    "| **Sélectif** | [1.1.1](#ChatbotsSelectifsSansMemoire) | [1.2.1](#ChatbotsSelectifsAvecMemoireRegles) | [1.3.1](#ChatbotsSelectifsAvecMemoireAgnostique) |\n",
    "| **Génératif** | [1.1.2](#ChatbotsGeneratifsSansMemoire) | [1.2.2](#ChatbotsGeneratifsAvecMemoireRegles) | [1.3.2](#ChatbotsGeneratifsAvecMemoireAgnostique) |\n",
    "\n",
    "    \n",
    "\n",
    "2. [Modules](#modules)\n",
    "\n",
    "    2.1 [Encodeurs de texte](#encodeursDeTexte)\n",
    "        2.1.1 Encodeurs de mots\n",
    "        \n",
    "    2.2 [Modules d'attention simple](#attentionSimple)\n",
    "        2.2.1 Attention additive\n",
    "        2.2.2 Attention additive multi-tête\n",
    "        2.2.3 Attention additive multi-hopée\n",
    "        \n",
    "    2.3 [Modules d'attention hiérarchique](#attentionHierarchique)\n",
    "    \n",
    "    2.4 [Décodeurs](#decodeurs)\n",
    "        2.4.1 Décodeur sélectif\n",
    "        2.4.2 Décodeur génératif\n",
    "        2.4.3 Décodeur génératif à attention\n",
    "        2.4.4 Décodeur génératif à pointeur\n",
    "\n",
    "\n",
    "[Bas de page](#basDePage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du répertoire principal contenant la librairie, dans le quel on se déplace ensuite et où on génère un fichier README.txt avec une brève présentation de cette librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier chatNLP existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir chatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jb\\Desktop\\Scripts\\notebooks\\chatNLP\n"
     ]
    }
   ],
   "source": [
    "%cd chatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting README.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile README.txt\n",
    "\n",
    "\n",
    "Inspiration pour la construction de la librairie :\n",
    "    \n",
    "https://github.com/pytorch/fairseq\n",
    "https://github.com/allenai/allennlp\n",
    "https://www.dabeaz.com/modulepackage/ModulePackage.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation du répertoire courant en librairie Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "\n",
    "#import libNLP.modules\n",
    "#import libNLP.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeles\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modèles\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Génération du sous-répertoire _libNLP.models_ contenant l'ensemble des modèles de Deep Learning développés dans cette librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier models existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/__init__.py\n",
    "\n",
    "\n",
    "from .Chatbot import (Chatbot,\n",
    "                      CreateBot,\n",
    "                      BotTrainer)\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'Chatbot',\n",
    "    'CreateBot',\n",
    "    'BotTrainer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ChatbotsGeneratifsAvecMemoireAgnostique\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Chatbots génératifs à mémoire agnostique\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Chatbot.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from chatNLP.modules import (RecurrentWordsEncoder, \n",
    "                            \n",
    "                            AdditiveAttention,\n",
    "                            MultiHeadAttention,\n",
    "                            MultiHopedAttention,\n",
    "                            RecurrentHierarchicalAttention, \n",
    "                            \n",
    "                            WordsDecoder,\n",
    "                            AttnWordsDecoder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot(nn.Module):\n",
    "    \"\"\"Conversationnal agent with bi-GRU Encoder, taking as parameters at training time :\n",
    "    \n",
    "            -a complete dialogue of the form (with each content as string)\n",
    "    \n",
    "                    [['question 1', 'answer 1'],\n",
    "                     ['question 2', 'answer 2'],\n",
    "                             ..........\n",
    "                     ['current question', 'current answer']]\n",
    "     \n",
    "            -the current answer for teacher forcing, or None\n",
    "    \n",
    "    and at test time :\n",
    "    \n",
    "            -the current question as string\n",
    "    \n",
    "    Returns :\n",
    "     \n",
    "            -word indices of the generated answer, according to output language of the model\n",
    "            -attention weights of first attention layer, or None is no attention\n",
    "            -attention weights of second attention layer, or None is no attention\n",
    "    \"\"\"\n",
    "    def __init__(self, device, lang, encoder, attention, decoder):\n",
    "        super(Chatbot, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.lang = lang \n",
    "        self.device = device\n",
    "        self.n_level = attention.n_level if attention is not None else 1\n",
    "        self.memory_dim = encoder.output_dim\n",
    "        self.memory_length = 0\n",
    "        # modules        \n",
    "        self.encoder = encoder\n",
    "        self.attention = attention\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ---------------------- Technical methods -----------------------------\n",
    "    def loadSubModule(self, encoder = None, attention = None, decoder = None) :\n",
    "        if encoder is not None :\n",
    "            self.encoder = encoder\n",
    "        if attention is not None :\n",
    "            self.attention = attention\n",
    "        if decoder is not None :\n",
    "            self.decoder = decoder\n",
    "        return\n",
    "    \n",
    "    def freezeSubModule(self, encoder = False, attention = False, decoder = False) :\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = not encoder\n",
    "        for param in self.attention.parameters():\n",
    "            param.requires_grad = not attention\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad = not decoder\n",
    "        return\n",
    "    \n",
    "    def nbParametres(self) :\n",
    "        count = 0\n",
    "        for p in self.parameters():\n",
    "            if p.requires_grad == True :\n",
    "                count += p.data.nelement()\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def flatten(self, description) :\n",
    "        '''Baisse le nombre de niveaux de 1 dans la description'''\n",
    "        flatten = []\n",
    "        for line in description :\n",
    "            flatten += line\n",
    "        return flatten\n",
    "\n",
    "    \n",
    "    \n",
    "    # ------------------------ Text processing methods ---------------------------------\n",
    "    def variableFromSentence(self, sentence):\n",
    "        def normalizeString(sentence) :\n",
    "            '''Remove rare symbols from a string'''\n",
    "            def unicodeToAscii(s):\n",
    "                \"\"\"Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\"\"\"\n",
    "                return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "            sentence = unicodeToAscii(sentence.lower().strip())\n",
    "            sentence = re.sub(r\"[^a-zA-Z0-9?&\\%\\-\\_]+\", r\" \", sentence) \n",
    "            return sentence\n",
    "        sentence = normalizeString(sentence).split(' ') # a raw string transformed into a list of clean words\n",
    "        indexes=[]\n",
    "        unknowns = 0\n",
    "        for word in sentence:\n",
    "            if word not in self.lang.word2index.keys() and 'UNK' in self.lang.word2index.keys() :\n",
    "                indexes.append(self.lang.word2index['UNK'])\n",
    "            else :\n",
    "                indexes.append(self.lang.word2index[word])\n",
    "        indexes.append(self.lang.word2index['EOS'])                                \n",
    "        result = Variable(torch.LongTensor([[i] for i in indexes]))\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------------ Visualisation methods ---------------------------------\n",
    "    def flattenDialogue(self, dialogue):\n",
    "        flatten = []\n",
    "        for paire in dialogue :\n",
    "            flatten += paire\n",
    "        return [[int(word) for word in sentence.data.view(-1)] for sentence in flatten]\n",
    "    \n",
    "    def flattenWeights(self, weights) :\n",
    "        '''Baisse le nombre de niveaux de 1 dans les poids d'attention'''\n",
    "        flatten = []\n",
    "        for weight_layer in weights :\n",
    "            flatten.append(torch.cat(tuple(weight_layer.values()), dim = 2))\n",
    "        return flatten\n",
    "    \n",
    "    def formatWeights(self, dialogue, attn1_weights, attn2_weights) :\n",
    "        if self.n_level == 2 :\n",
    "            attn1_weights = self.flattenWeights(attn1_weights)\n",
    "        hops = self.attention.hops\n",
    "        l, L = len(dialogue), max([len(line) for line in dialogue])\n",
    "        Table = np.zeros((l, 1, L))\n",
    "        Liste = np.zeros((l, 1)) if attn2_weights is not None else None\n",
    "        count = 0\n",
    "        count_line = 0\n",
    "        for i, line in enumerate(dialogue) :\n",
    "            present = False\n",
    "            for j, word in enumerate(line) :\n",
    "                if word in self.lang.index2word.keys():\n",
    "                    present = True\n",
    "                    Table[i, 0, j] = sum([attn1_weights[k][0, 0, count].data for k in range(hops)])\n",
    "                    count += 1\n",
    "            if present and Liste is not None :\n",
    "                Liste[i] = sum([attn2_weights[k][count_line].data for k in range(hops)])\n",
    "                count_line += 1\n",
    "        return Table, Liste\n",
    "    \n",
    "    def showWeights(self, dialogue, attn1_weights, attn2_weights, maxi):\n",
    "        table, liste = self.formatWeights(dialogue[:-2], attn1_weights, attn2_weights)\n",
    "        l = table.shape[0]\n",
    "        L = table.shape[2]\n",
    "        fig = plt.figure(figsize = (l, L))\n",
    "        for i, line in enumerate(dialogue[:-2]):\n",
    "            ligne = [self.lang.index2word[int(word)] for word in line]\n",
    "            ax = fig.add_subplot(l, 1, i+1)\n",
    "            vals = table[i]\n",
    "            text = [' '] + ligne + [' ' for k in range(L-len(ligne))] if L>len(ligne) else [' '] + ligne\n",
    "            if liste is not None :\n",
    "                vals = np.concatenate((np.zeros((1, 1)) , vals), axis = 1)  \n",
    "                vals = np.concatenate((np.reshape(liste[i], (1, 1)) , vals), axis = 1)\n",
    "                turn = 'User' if i % 2 == 0 else 'Bot'\n",
    "                text = [turn] + [' '] + text\n",
    "            cax = ax.matshow(vals, vmin=0, vmax=maxi, cmap='YlOrBr')\n",
    "            ax.set_xticklabels(text, ha='left')\n",
    "            ax.set_yticklabels(' ')\n",
    "            ax.tick_params(axis=u'both', which=u'both',length=0, labelrotation = 30, labelright  = True)\n",
    "            ax.grid(b = False, which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            plt.subplots_adjust(hspace=0, wspace = 0.1)\n",
    "        plt.show()\n",
    "    \n",
    "    def showAttention(self, dialogue, n_col = 1, maxi = None):\n",
    "        answer, decoder_outputs, attn1_weights, attn2_weights = self.answerTrain(dialogue)\n",
    "        dialogue = self.flattenDialogue(dialogue)\n",
    "        if len(dialogue) > 1 :\n",
    "            self.showWeights(dialogue, attn1_weights, attn2_weights, maxi)\n",
    "        print('User : ', ' '.join([self.lang.index2word[int(word)] for word in dialogue[-2][:-1]]))\n",
    "        print('target : ', ' '.join([self.lang.index2word[int(word)] for word in dialogue[-1][:-1]]))\n",
    "        print('predic : ', ' '.join([self.lang.index2word[int(word)] for word in answer]))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------- Process methods ------------------------\n",
    "    def initMemory(self):\n",
    "        \"\"\"Initialize memory slots\"\"\"\n",
    "        self.memory = {}\n",
    "        self.memory_queries = {}\n",
    "        self.query_hidden = self.encoder.initHidden()\n",
    "        self.memory_length = 0\n",
    "        \n",
    "    def updateMemory(self, last_words, query_hidden):\n",
    "        \"\"\"Update memory with a list of word vectors 'last_words' and the last query vector 'last_query'\"\"\"\n",
    "        self.memory[self.memory_length] = last_words\n",
    "        self.memory_queries[self.memory_length] = query_hidden\n",
    "        self.query_hidden = query_hidden\n",
    "        self.memory_length += 1\n",
    "        \n",
    "    def readSentence(self, utterance):\n",
    "        \"\"\"Perform reading of an utterance, returning created word vectors\n",
    "           and last hidden states of teh encoder bi-GRU\n",
    "        \"\"\"\n",
    "        utterance = utterance.to(self.device)\n",
    "        last_words, query_hidden = self.encoder(utterance, self.query_hidden)\n",
    "        return last_words, query_hidden\n",
    "        \n",
    "    def readDialogue(self, dialogue):\n",
    "        \"\"\"Loop of readUtterance over a whole dialogue\n",
    "        \"\"\"\n",
    "        for i in range(len(dialogue)) :\n",
    "            for j in range(2):\n",
    "                utterance = dialogue[i][j]\n",
    "                last_words, query_hidden = self.readSentence(utterance)\n",
    "                self.updateMemory(last_words, query_hidden)\n",
    "   \n",
    "    def tracking(self, query_vector):\n",
    "        \"\"\"Détermine un vecteur d'attention sur les éléments du registre de l'agent,\n",
    "        sachant un vecteur 'very_last_hidden', et l'accole à ce vecteur \"\"\"\n",
    "        decision_vector, attn1_weights, attn2_weights = self.attention(words_memory = self.memory, \n",
    "                                                                       base_query = query_vector)\n",
    "        return decision_vector, attn1_weights, attn2_weights\n",
    "\n",
    "    def generateAnswer(self,last_words, query_vector, decision_vector, target_answer = None) :\n",
    "        \"\"\"Génère une réponse à partir d'un état caché initialisant le décodeur,\n",
    "        en utilisant une réponse cible pour un mode 'teacher forcing-like' si celle-ci est fournie \"\"\"\n",
    "        answer, decoder_outputs = self.decoder(last_words, query_vector, decision_vector, target_answer)\n",
    "        return answer, decoder_outputs\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ------------ 1st working mode : training mode ------------\n",
    "    def answerTrain(self, input, target_answer = None):\n",
    "        \"\"\"Parameters are a complete dialogue, containing the current query and answer, and of the form\n",
    "\n",
    "                    [['query 1', 'answer 1'],\n",
    "                     ['query 2', 'answer 2'],\n",
    "                             ..........\n",
    "                     ['current query', 'current answer']]\n",
    "\n",
    "           The model learns to generate the current answer. \n",
    "           Teacher forcing can be enabled by passing the ground answer though the 'target_answer' option. \n",
    "           Attention weights over words and past utterances can be provided with the 'provideAttention' option.\"\"\"\n",
    "        # 1) initiates memory instance\n",
    "        self.initMemory()\n",
    "        \n",
    "        # 2) reads historical part of dialogue (if applicable),\n",
    "        # word vectors and last hidden states of encoder bi-GRU are stored in memory\n",
    "        dialogue = input[:-1]\n",
    "        self.readDialogue(dialogue)\n",
    "        \n",
    "        # 3) reads current utterance,\n",
    "        # returns word vectors of query and query vector\n",
    "        query = input[-1][0]\n",
    "        last_words, query_hidden = self.readSentence(query)\n",
    "        q_hidden = query_hidden.view(1,1,-1)\n",
    "        \n",
    "        # 4) performs tracking\n",
    "        # returns decision vector\n",
    "        if self.attention is not None :\n",
    "            decision_vector, attn1_weights, attn2_weights = self.tracking(q_hidden)\n",
    "        else :\n",
    "            decision_vector = q_hidden\n",
    "            attn1_attention_weights = None\n",
    "            attn2_attention_weights = None\n",
    "            \n",
    "        # 5) response generation\n",
    "        # returns list of indices\n",
    "        answer, decoder_outputs = self.generateAnswer(last_words, q_hidden, decision_vector, target_answer)\n",
    "            \n",
    "        # 6) returns answer\n",
    "        return answer, decoder_outputs, attn1_weights, attn2_weights\n",
    "\n",
    "        \n",
    "        \n",
    "    # ------------ 2nd working mode : test mode ------------\n",
    "    def forward(self, input):\n",
    "        \"\"\"Parameters are a single current query as string, and the model learns to generate the current answer. \n",
    "           Attention weights over words and past utterances can be provided with the 'provideAttention' option.\"\"\"\n",
    "        \n",
    "        # 1) initiates memory and hidden states of encoder bi-GRU if conversation starts\n",
    "        if self.memory_length == 0 :\n",
    "            self.initMemory()\n",
    "            \n",
    "        # 2) reads current utterance,\n",
    "        # returns word vectors of query and query vector\n",
    "        sentence = self.variableFromSentence(input)\n",
    "        if sentence is None :\n",
    "            return \"Excusez-moi je n'ai pas compris\", None, None, None\n",
    "        else :\n",
    "            last_words, query_hidden = self.readSentence(sentence)\n",
    "            q_hidden = query_hidden.view(1,1,-1)\n",
    "\n",
    "            # 3) performs tracking\n",
    "            # returns decision vector\n",
    "            if self.attention is not None :\n",
    "                decision_vector, attn1_weights, attn2_weights = self.tracking(q_hidden)\n",
    "            else :\n",
    "                decision_vector = q_hidden\n",
    "                attn1_attention_weights = None\n",
    "                attn2_attention_weights = None\n",
    "\n",
    "            # 4) response generation\n",
    "            # returns list of indices\n",
    "            answer, decoder_outputs = self.generateAnswer(last_words, q_hidden, decision_vector)\n",
    "            \n",
    "            # 5) updates memory with current query and answer\n",
    "            self.updateMemory(last_words, query_hidden)\n",
    "            answer_var = Variable(torch.LongTensor([[i] for i in answer]))\n",
    "            last_words, query_hidden = self.readSentence(answer_var)\n",
    "            self.updateMemory(last_words, query_hidden)\n",
    "\n",
    "            # 6) returns answer\n",
    "            answer = ' '.join([self.lang.index2word[int(word)] for word in answer])\n",
    "            return answer, attn1_weights, attn2_weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def CreateBot(lang,                     ###\n",
    "              embedding_dim,              # --- Encoder options\n",
    "              hidden_dim,                 #\n",
    "              n_layers,                 ###\n",
    "\n",
    "              sentence_hidden_dim,      ###\n",
    "              hops,                       #\n",
    "              share,                      # --- Hierarchical encoder options\n",
    "              transf,                     #\n",
    "              dropout,                  ###\n",
    "              \n",
    "              attn_decoder_n_layers,    ### --- decoder options\n",
    "              \n",
    "              device\n",
    "             ):\n",
    "    '''Create an agent with specified dimensions and specificities'''\n",
    "    # 1) ----- encoding -----\n",
    "    embedding = nn.Embedding(lang.n_words, embedding_dim)\n",
    "    encoder = RecurrentWordsEncoder(device, embedding, hidden_dim, n_layers, dropout) # embedding, hidden_dim, n_layers = 1, dropout = 0\n",
    "    # 2) ----- attention -----\n",
    "    word_hidden_dim = encoder.output_dim\n",
    "    attention = RecurrentHierarchicalAttention(device,\n",
    "                                               word_hidden_dim,\n",
    "                                               sentence_hidden_dim, \n",
    "                                               base_query_dim = word_hidden_dim,\n",
    "                                               n_heads = 1,\n",
    "                                               hops = hops,\n",
    "                                               share = share,\n",
    "                                               transf = transf,\n",
    "                                               dropout = dropout)\n",
    "    # 3) ----- decoding -----\n",
    "    tracking_dim = attention.output_dim\n",
    "    if attn_decoder_n_layers >= 0 :\n",
    "        decoder = AttnWordsDecoder(device,\n",
    "                                   embedding,\n",
    "                                   decoder_hidden_dim,\n",
    "                                   dropout = dropout,\n",
    "                                   n_layers = attn_decoder_n_layers)\n",
    "    else :\n",
    "        decoder = WordsDecoder(device,\n",
    "                               embedding,                                   \n",
    "                               word_hidden_dim,\n",
    "                               tracking_dim,\n",
    "                               dropout = dropout)        \n",
    "    # 4) ----- model -----\n",
    "    chatbot = Chatbot(device, lang, encoder, attention, decoder)\n",
    "    chatbot = chatbot.to(device)\n",
    "    return chatbot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BotTrainer(object):\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 criterion= nn.NLLLoss(), \n",
    "                 optimizer = optim.SGD, \n",
    "                 clipping = 50,\n",
    "                 teacher_forcing_ratio = 0.5,  \n",
    "                 print_every=100):\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.clip = clipping\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.print_every = print_every# timer\n",
    "        \n",
    "        \n",
    "    def asMinutes(self, s):\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "    def timeSince(self, since, percent):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        es = s / (percent)\n",
    "        rs = es - s\n",
    "        return '%s (- %s)' % (self.asMinutes(s), self.asMinutes(rs))\n",
    "        \n",
    "        \n",
    "    def distance(self, agent_outputs, target_answer) :\n",
    "        \"\"\" Compute cumulated error between predicted output and ground answer.\"\"\"\n",
    "        loss = 0\n",
    "        loss_diff_mots = 0\n",
    "        agent_outputs_length = len(agent_outputs)\n",
    "        target_length = len(target_answer)\n",
    "        Max = max(agent_outputs_length, target_length)\n",
    "        Min = min(agent_outputs_length, target_length)   \n",
    "        for i in range(Min):\n",
    "            agent_output = agent_outputs[i]\n",
    "            target_word = target_answer[i]\n",
    "            loss += self.criterion(agent_output, target_word)\n",
    "            topv, topi = agent_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            if ni != target_word.data[0]:\n",
    "                loss_diff_mots += 1\n",
    "        if agent_outputs_length != target_length :\n",
    "            loss_diff_mots += Max - Min\n",
    "        return loss, loss_diff_mots\n",
    "        \n",
    "        \n",
    "    def trainLoop(self, agent, dialogue, target_answer, optimizer, learning_rate):\n",
    "        \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        target_length = len(target_answer)\n",
    "        target_answer = target_answer.to(self.device)\n",
    "        tf = target_answer if random.random() < self.teacher_forcing_ratio else None\n",
    "        answer, agent_outputs, attn1_attention_weights, attn2_attention_weights =  agent.answerTrain(dialogue, tf) \n",
    "        loss, loss_diff_mots = self.distance(agent_outputs, target_answer)        \n",
    "        loss.backward()\n",
    "        _ = torch.nn.utils.clip_grad_norm_(agent.parameters(), self.clip)\n",
    "        optimizer.step()\n",
    "        return loss.data[0] / target_length , loss_diff_mots\n",
    "        \n",
    "        \n",
    "    def train(self, agent, dialogues, n_iters = 10000, learning_rate=0.01, dic = None):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops.\"\"\"\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in agent.parameters() if param.requires_grad == True], lr=learning_rate)\n",
    "        print_loss_total = 0  \n",
    "        print_loss_diff_mots_total = 0\n",
    "        for iter in range(1, n_iters + 1):\n",
    "            if dic is not None :\n",
    "                j = int(random.choice(list(dic.keys())))\n",
    "                training_dialogue = dialogues[j]\n",
    "                i = random.choice(dic[j])\n",
    "                partie_dialogue = training_dialogue[:i+1]\n",
    "            else :\n",
    "                training_dialogue = random.choice(dialogues)\n",
    "                i = random.choice(range(len(training_dialogue)))\n",
    "                partie_dialogue = training_dialogue[:i+1]\n",
    "            #target_answer = variableFromSentence(agent.output_lang, training_dialogue[i][1])\n",
    "            target_answer = training_dialogue[i][1]\n",
    "            loss, loss_diff_mots = self.trainLoop(agent, partie_dialogue, target_answer, optimizer, learning_rate)\n",
    "            # quantité d'erreurs sur la réponse i\n",
    "            print_loss_total += loss\n",
    "            print_loss_diff_mots_total += loss_diff_mots       \n",
    "            if iter % self.print_every == 0:\n",
    "                print_loss_avg = print_loss_total / self.print_every\n",
    "                print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                print_loss_total = 0\n",
    "                print_loss_diff_mots_total = 0\n",
    "                print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg, print_loss_diff_mots_avg))\n",
    "                \n",
    "                \n",
    "    def ErrorCount(self, agent, dialogues):\n",
    "        bound = 10\n",
    "        ERRORS = [0 for i in range(bound +1)]\n",
    "        repartitionError = {}\n",
    "        for i in range(bound +1) :\n",
    "            repartitionError[i] = []\n",
    "        liste = []\n",
    "        for k, input_dialogue in enumerate(dialogues):\n",
    "            for l in range(len(input_dialogue)):\n",
    "                if len(input_dialogue[l][1])>0 :\n",
    "                    dialogue = input_dialogue[:l+1]\n",
    "                    #target_answer = variableFromSentence(agent.output_lang, input_dialogue[l][1])\n",
    "                    target_answer = input_dialogue[l][1]\n",
    "                    target_answer = target_answer.to(self.device)\n",
    "                    answer, agent_outputs, attn1_attention_weights, attn2_attention_weights = agent.answerTrain(dialogue)\n",
    "                    loss, loss_diff_mots = self.distance(agent_outputs, target_answer)\n",
    "                    if loss_diff_mots > bound :\n",
    "                        ERRORS = ERRORS + [0 for i in range(loss_diff_mots - bound)]\n",
    "                        for i in range(bound +1, loss_diff_mots +1) :\n",
    "                            repartitionError[i] = []\n",
    "                        bound  = loss_diff_mots\n",
    "                    ERRORS[loss_diff_mots] += 1\n",
    "                    if loss_diff_mots > 0 :\n",
    "                        liste.append([k, l, loss_diff_mots])\n",
    "        for triple in liste:\n",
    "            repartitionError[triple[2]].append(triple[:2])\n",
    "        print(\"The repartition of errors :\", ERRORS)\n",
    "        return repartitionError\n",
    "\n",
    "\n",
    "    def DialoguesWithErrors(self, agent, dialogues) :\n",
    "        '''Returns a dictionnary, with indices of dialogues and index of line in dialogue\n",
    "           where a mistake was made.\n",
    "        '''\n",
    "        start = time.time()\n",
    "        Sortie = {}\n",
    "        L = len(dialogues)\n",
    "        for i, dialogue in enumerate(dialogues) :\n",
    "            errs = []\n",
    "            for j in range(len(dialogue)) :\n",
    "                target_answer = dialogue[j][1]\n",
    "                target_answer = target_answer.to(self.device)\n",
    "                answer, agent_outputs, attn1_attention_weights, attn2_attention_weights = agent.answerTrain(dialogue[:j+1],\n",
    "                                                                                                            target_answer)\n",
    "                loss, loss_diff_mots = self.distance(agent_outputs, target_answer)\n",
    "                if loss_diff_mots > 0 :\n",
    "                    errs.append(j)\n",
    "            if errs != []:\n",
    "                Sortie[i] = errs\n",
    "            if (i+1) % self.print_every == 0:\n",
    "                print('%s (%d %d%%)' % (self.timeSince(start, (i+1) / L),\n",
    "                                             (i+1), (i+1) / L * 100))\n",
    "        return Sortie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Modules\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier modules existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/__init__.py\n",
    "\n",
    "from .Encoder_Words_Recurrent import RecurrentWordsEncoder\n",
    "\n",
    "from .Attention_Additive import AdditiveAttention\n",
    "from .Attention_MultiHead import MultiHeadAttention\n",
    "from .Attention_MultiHoped import MultiHopedAttention\n",
    "from .Attention_Hierarchical_Recurrent import RecurrentHierarchicalAttention\n",
    "\n",
    "from .Decoder_Classes import ClassDecoder\n",
    "from .Decoder_Words import WordsDecoder\n",
    "from .Decoder_Words_Attn import AttnWordsDecoder\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'RecurrentWordsEncoder',\n",
    "    \n",
    "    'AdditiveAttention',\n",
    "    'MultiHeadAttention',\n",
    "    'MultiHopedAttention',\n",
    "    'RecurrentHierarchicalAttention',\n",
    "    \n",
    "    'ClassDecoder',\n",
    "    'WordsDecoder',\n",
    "    'AttnWordsDecoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encodeursDeTexte\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encodeurs de texte\n",
    "\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encodeursDeMots\"></a>\n",
    "\n",
    "### 2.1.1 Encodeur de mots\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module **RecurrentWordsEncoder** encode une séquence de mots $w_1, ..., w_T$ en une séquence de vecteurs $h_1, ..., h_T$ en appliquant un plongement suivi d'une couche GRU bi-directionnelle. On peut représenter son fonctionnement par la figure suivante :\n",
    "\n",
    "\n",
    "![WordEncoder](figs/WordEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Encoder_Words_Recurrent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Encoder_Words_Recurrent.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class RecurrentWordsEncoder(nn.Module):\n",
    "    def __init__(self, device, embedding, hidden_dim, n_layers = 1, dropout = 0): \n",
    "        super(RecurrentWordsEncoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim           # dimension of hidden state of GRUs \n",
    "        self.dropout_p = dropout\n",
    "        self.n_layers = n_layers               # number of stacked GRU layers\n",
    "        self.output_dim = hidden_dim * 2       # dimension of outputed rep. of words and utterance\n",
    "        # parameters\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            embedding_dim = p.data.size()[1]\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.bigru = nn.GRU(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), \n",
    "                            bidirectional=True)\n",
    "\n",
    "        \n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(2 * self.n_layers, 1, self.hidden_dim)).to(self.device)\n",
    "\n",
    "    def forward(self, utterance, hidden = None):\n",
    "        embeddings = self.embedding(utterance)                          # dim = (input_length, 1, embedding_dim)\n",
    "        embeddings = self.dropout(embeddings)                           # dim = (input_length, 1, embedding_dim)\n",
    "        outputs, hidden = self.bigru(embeddings, hidden)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return outputs, hidden                                          # dim = (input_length, 1, hidden_dim * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionSimple\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Modules d'attention simple\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.1 Module d'attention additive\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![AttentionAdditive](figs/Attention_Additive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_Additive.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_Additive.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, query_dim, targets_dim, n_layers = 2): \n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.n_level = 1\n",
    "        self.query_dim = query_dim\n",
    "        self.targets_dim = targets_dim\n",
    "        self.output_dim = targets_dim\n",
    "        self.n_layers = n_layers\n",
    "        # parameters\n",
    "        self.attn_layer = nn.Linear(query_dim + targets_dim, targets_dim) if n_layers >= 1 else None\n",
    "        self.attn_layer2 = nn.Linear(targets_dim, targets_dim) if n_layers >= 2 else None\n",
    "        self.attn_v = nn.Linear(targets_dim, 1, bias = False) if n_layers >= 1 else None\n",
    "        self.act = F.softmax\n",
    "\n",
    "        \n",
    "    def forward(self, query = None, targets = None):\n",
    "        '''takes as parameters : \n",
    "                a query tensor conditionning the attention,     size = (1, minibatch_size, query_dim)\n",
    "                a tensor containing attention targets           size = (targets_length, minibatch_size, targets_dim)\n",
    "           returns : \n",
    "                the resulting tensor of the attention process,  size = (1, minibatch_size, targets_dim)\n",
    "                the attention weights,                          size = (1, targets_length)\n",
    "        '''\n",
    "        if targets is not None :\n",
    "            # concat method \n",
    "            if self.n_layers >= 1 :\n",
    "                poids = torch.cat((query.expand(targets.size(0), -1, -1), targets), 2) if query is not None else targets\n",
    "                poids = self.attn_layer(poids).tanh()                 # size (targets_length, minibatch_size, targets_dim)\n",
    "                if self.n_layers >= 2 :\n",
    "                    poids = self.attn_layer2(poids).tanh()            # size (targets_length, minibatch_size, targets_dim)\n",
    "                attn_weights = self.attn_v(poids)                     # size (targets_length, minibatch_size, 1)\n",
    "                attn_weights = torch.transpose(attn_weights, 0,1)     # size (minibatch_size, targets_length, 1)\n",
    "                targets = torch.transpose(targets, 0,1)               # size (minibatch_size, targets_length, targets_dim)\n",
    "            # dot method\n",
    "            else :\n",
    "                targets = torch.transpose(targets, 0,1)               # size (minibatch_size, targets_length, targets_dim)\n",
    "                query = torch.transpose(query, 0, 1)                  # size (minibatch_size, 1, query_dim)\n",
    "                query = torch.transpose(query, 1, 2)                  # size (minibatch_size, query_dim, 1)\n",
    "                attn_weights = torch.bmm(targets, query)              # size (minibatch_size, targets_length, 1)\n",
    "                \n",
    "            attn_weights = self.act(attn_weights, dim = 1)        # size (minibatch_size, targets_length, 1)\n",
    "            attn_weights = torch.transpose(attn_weights, 1,2)     # size (minibatch_size, 1, targets_length)\n",
    "            attn_applied = torch.bmm(attn_weights, targets)       # size (minibatch_size, 1, targets_dim)\n",
    "            attn_applied = torch.transpose(attn_applied, 0,1)     # size (1, minibatch_size, targets_dim)\n",
    "\n",
    "        else :\n",
    "            attn_applied = query\n",
    "            attn_weights = None\n",
    "        return attn_applied, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"attentionAdditiveMultitete\"></a>\n",
    "\n",
    "\n",
    "### 2.2.2 Module d'attention additive multi-tête\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_MultiHead.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_MultiHead.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Module performing additive attention over a sequence of vectors stored in\n",
    "       a memory block, conditionned by some vector. At instanciation it takes as imput :\n",
    "       \n",
    "                - query_dim : the dimension of the conditionning vector\n",
    "                - targets_dim : the dimension of vectors stored in memory\n",
    "                \n",
    "      Other ideas on Multi head attention on \n",
    "      https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py\n",
    "      https://github.com/tlatkowski/multihead-siamese-nets/blob/master/layers/attention.py\n",
    "    '''\n",
    "    def __init__(self, device, n_heads, query_dim, targets_dim, n_layers = 2): \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.n_level = 1\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        # parameters\n",
    "        self.attn_modules_list = nn.ModuleList([AdditiveAttention(query_dim, targets_dim, n_layers) for i in range(n_heads)])\n",
    "\n",
    "        \n",
    "    def forward(self, query = None, targets = None):\n",
    "        '''takes as parameters : \n",
    "                a query tensor conditionning the attention,     size = (1, n_heads, query_dim)\n",
    "                a tensor containing attention targets           size = (targets_length, n_heads, targets_dim)\n",
    "           returns : \n",
    "                the resulting tensor of the attention process,  size = (1, n_heads, targets_dim)\n",
    "                the attention weights,                          size = (n_heads, 1, targets_length)\n",
    "        '''\n",
    "        print(\"multihead attention\")\n",
    "        targets_length = targets.size(0)\n",
    "        targets_dim    = targets.size(2)\n",
    "        attn_applied   = Variable(torch.zeros(1, self.n_heads, targets_dim)).to(self.device)\n",
    "        attn_weights   = torch.zeros(self.n_heads, 1, targets_length).to(self.device)\n",
    "        for i, attn in enumerate(self.attn_modules_list) :\n",
    "            que = query[:, i, :] if query is not None else None\n",
    "            print(que.size())\n",
    "            tar = targets[:, i, :].unsqueeze(1)\n",
    "            print(tar.size())\n",
    "            attn_appl, attn_wghts = attn(que, tar)\n",
    "            print(attn_appl.size())\n",
    "            print(attn_wghts.size())\n",
    "            attn_applied[:, i, :] = attn_appl.squeeze(1)\n",
    "            attn_weights[i, :, :] = attn_wghts.squeeze(0)\n",
    "        return attn_applied, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionAdditiveMultihoped\"></a>\n",
    "\n",
    "\n",
    "### 2.2.3 Module d'attention additive multi-hopée\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_MultiHoped.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_MultiHoped.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class MultiHopedAttention(nn.Module):\n",
    "    '''Module performing additive attention over a sequence of vectors stored in\n",
    "       a memory block, conditionned by some vector. At instanciation it takes as imput :\n",
    "       \n",
    "                - query_dim : the dimension of the conditionning vector\n",
    "                - targets_dim : the dimension of vectors stored in memory\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 targets_dim,\n",
    "                 base_query_dim = 0,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(MultiHopedAttention, self).__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.targets_dim = targets_dim\n",
    "        self.output_dim = targets_dim\n",
    "        self.hops_query_dim = self.output_dim if hops > 1 else 0\n",
    "        self.query_dim = base_query_dim + self.hops_query_dim\n",
    "        \n",
    "        # structural coefficients\n",
    "        self.device = device\n",
    "        self.n_level = 1\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        self.transf = transf\n",
    "        self.dropout_p = dropout\n",
    "        if dropout > 0 :\n",
    "            self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # parameters\n",
    "        self.attn = AdditiveAttention(self.query_dim, self.targets_dim) \n",
    "        self.transf = nn.GRU(self.targets_dim, self.targets_dim) if transf else None\n",
    "        \n",
    "        \n",
    "    def initQuery(self): \n",
    "        if self.hops_query_dim > 0 :\n",
    "            return Variable(torch.zeros(1, 1, self.hops_query_dim)).to(self.device)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def update(self, hops_query, decision_vector):\n",
    "        if self.transf is not None :\n",
    "            _ , update = self.transf(decision_vector, hops_query)\n",
    "        else :\n",
    "            update = hops_query + decision_vector\n",
    "        return update\n",
    "    \n",
    "    \n",
    "    def forward(self, words_memory, base_query = None):\n",
    "        attn_weights_list = []\n",
    "        if self.hops > 1 and self.share :\n",
    "            hops_query = self.initQuery()\n",
    "        else :\n",
    "            hops_query = None\n",
    "            \n",
    "        for hop in range(self.hops) :\n",
    "            if base_query is not None and hops_query is not None :\n",
    "                query = torch.cat((base_query, hops_query), 2) # size (1, self.n_heads, self.query_dim)\n",
    "            elif base_query is not None :\n",
    "                query = base_query\n",
    "            elif hops_query is not None :\n",
    "                query = hops_query\n",
    "            else :\n",
    "                query = None\n",
    "            \n",
    "            decision_vector, attn_weights = self.attn(query, words_memory)\n",
    "            attn_weights_list.append(attn_weights)\n",
    "            if self.hops > 1 :\n",
    "                hops_query = self.update(hops_query, decision_vector) if hops_query is not None else decision_vector                          # size (L, n_classes, output_dim)\n",
    "            else :\n",
    "                hops_query = decision_vector\n",
    "  \n",
    "        \n",
    "        # output decision vector\n",
    "        return hops_query, attn_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionHierarchique\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Attention hiérarchique\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![HierarchicalAttention](figs/Hierarchical_Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_Hierarchical_Recurrent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_Hierarchical_Recurrent.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from . import AdditiveAttention, MultiHeadAttention\n",
    "\n",
    "\n",
    "class RecurrentHierarchicalAttention(nn.Module):\n",
    "    '''Ce module d'attention est :\n",
    "    \n",
    "    - hiérarchique avec bi-GRU entre chaque niveau d'attention\n",
    "    - multi-tête sur chaque niveau d'attention\n",
    "    - globalement multi-hopé, où il est possible d'effectuer plusieurs passes pour accumuler de l'information\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 word_hidden_dim, \n",
    "                 sentence_hidden_dim,\n",
    "                 base_query_dim = 0, \n",
    "                 n_heads = 1,\n",
    "                 n_layers = 1,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(RecurrentHierarchicalAttention, self).__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        self.sentence_input_dim = self.word_hidden_dim\n",
    "        self.sentence_hidden_dim = sentence_hidden_dim\n",
    "        self.context_vector_dim = sentence_hidden_dim * 2\n",
    "        self.output_dim = sentence_hidden_dim * 2\n",
    "        self.base_query_dim = base_query_dim\n",
    "        self.hops_query_dim = self.output_dim if hops > 1 else 0\n",
    "        self.query_dim = self.base_query_dim + self.hops_query_dim\n",
    "        \n",
    "        # structural coefficients\n",
    "        self.device = device\n",
    "        self.n_level = 2\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        self.dropout_p = dropout\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # first attention module\n",
    "        attn1_list = []\n",
    "        if share :\n",
    "            attn1 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.word_hidden_dim) if n_heads > 1 else \\\n",
    "                    AdditiveAttention(self.query_dim, self.word_hidden_dim) \n",
    "            for hop in range(hops):\n",
    "                attn1_list.append(attn1)\n",
    "            self.attn1 = nn.ModuleList(attn1_list)\n",
    "        else :\n",
    "            for hop in range(hops):\n",
    "                qd = self.query_dim if hop > 0 else self.base_query_dim\n",
    "                attn1 = MultiHeadAdditiveAttention(n_heads, qd, self.word_hidden_dim) if n_heads > 1 else \\\n",
    "                        AdditiveAttention(qd, self.word_hidden_dim) \n",
    "                attn1_list.append(attn1)\n",
    "            self.attn1 = nn.ModuleList(attn1_list)\n",
    "        \n",
    "        # intermediate encoder module\n",
    "        self.bigru = nn.GRU(self.sentence_input_dim, \n",
    "                            self.sentence_hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        # second attention module\n",
    "        attn2_list = []\n",
    "        if share :\n",
    "            attn2 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.context_vector_dim) if n_heads > 1 else \\\n",
    "                    AdditiveAttention(self.query_dim, self.context_vector_dim) \n",
    "            for hop in range(hops):\n",
    "                attn2_list.append(attn2)\n",
    "            self.attn2 = nn.ModuleList(attn2_list)\n",
    "        else :\n",
    "            for hop in range(hops):\n",
    "                qd = self.query_dim if hop > 0 else self.base_query_dim\n",
    "                attn2 = MultiHeadAdditiveAttention(n_heads, qd, self.context_vector_dim) if n_heads > 1 else \\\n",
    "                        AdditiveAttention(qd, self.context_vector_dim) \n",
    "                attn2_list.append(attn2)\n",
    "            self.attn2 = nn.ModuleList(attn2_list)\n",
    "        \n",
    "        # accumulation step\n",
    "        self.transf = nn.GRU(self.context_vector_dim, self.context_vector_dim) if transf else None\n",
    "\n",
    "\n",
    "    def initQuery(self): \n",
    "        if self.hops_query_dim > 0 :\n",
    "            return Variable(torch.zeros(1, self.n_heads, self.hops_query_dim)).to(self.device)\n",
    "        return None\n",
    "        \n",
    "                \n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(2 * self.n_layers, self.n_heads, self.sentence_hidden_dim)).to(self.device)\n",
    "        \n",
    "        \n",
    "    def singlePass(self, words_memory, query, attn1, attn2): \n",
    "        L = len(words_memory)\n",
    "        attn1_weights = {}\n",
    "        bigru_inputs = Variable(torch.zeros(L, self.n_heads, self.sentence_input_dim)).to(self.device)\n",
    "        # first attention layer\n",
    "        for i in range(L) :\n",
    "            targets = words_memory[i]                              # size (N_i, 1, 2*word_hidden_dim)\n",
    "            targets = targets.repeat(1, self.n_heads, 1)           # size (N_i, n_heads, 2*word_hidden_dim)\n",
    "            attn1_output, attn1_wghts = attn1(query, targets)\n",
    "            attn1_output = self.dropout(attn1_output)\n",
    "            attn1_weights[i] = attn1_wghts\n",
    "            bigru_inputs[i] = attn1_output.squeeze(0)              # size (n_heads, 2*word_hidden_dim)\n",
    "        # intermediate biGRU\n",
    "        bigru_hidden = self.initHidden()\n",
    "        attn2_inputs, bigru_hidden = self.bigru(bigru_inputs, bigru_hidden)  # size (L, n_heads, 2*word_hidden_dim)\n",
    "        # second attention layer\n",
    "        attn2_inputs = self.dropout(attn2_inputs)\n",
    "        decision_vector, attn2_weights = attn2(query = query, targets = attn2_inputs)\n",
    "        attn2_weights = attn2_weights.view(-1)\n",
    "        decision_vector = self.dropout(decision_vector)\n",
    "        # output decision vector\n",
    "        return decision_vector, attn1_weights, attn2_weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, hops_query, decision_vector):\n",
    "        if self.transf is not None :\n",
    "            _ , update = self.transf(decision_vector, hops_query)\n",
    "        else :\n",
    "            update = hops_query + decision_vector\n",
    "        return update\n",
    "        \n",
    "        \n",
    "    def forward(self, words_memory, base_query = None):\n",
    "        '''takes as parameters : \n",
    "                a tensor containing words_memory vectors        dim = (words_memory_length, word_hidden_dim)\n",
    "                a tensor containing past queries                dim = (words_memory_length, query_dim)\n",
    "           returns : \n",
    "                the resulting decision vector                   dim = (1, 1, query_dim)\n",
    "                the weights of first attention layer (dict)     \n",
    "                the weights of second attention layer (dict)\n",
    "        '''\n",
    "        attn1_weights_list = []\n",
    "        attn2_weights_list = []\n",
    "        if len(words_memory) > 0 :\n",
    "            if base_query is not None :\n",
    "                base_query = base_query.repeat(1, self.n_heads, 1)\n",
    "            if self.hops > 1 and self.share :\n",
    "                hops_query = self.initQuery()\n",
    "            else :\n",
    "                hops_query = None\n",
    "\n",
    "            for hop in range(self.hops) :\n",
    "                if base_query is not None and hops_query is not None :\n",
    "                    query = torch.cat((base_query, hops_query), 2) # size (1, self.n_heads, self.query_dim)\n",
    "                elif base_query is not None :\n",
    "                    query = base_query\n",
    "                elif hops_query is not None :\n",
    "                    query = hops_query\n",
    "                else :\n",
    "                    query = None\n",
    "                decision_vector, attn1_weights, attn2_weights = self.singlePass(words_memory, \n",
    "                                                                                query, \n",
    "                                                                                self.attn1[hop], \n",
    "                                                                                self.attn2[hop])\n",
    "                attn1_weights_list.append(attn1_weights)\n",
    "                attn2_weights_list.append(attn2_weights)\n",
    "                if self.hops > 1 and hops_query is not None :\n",
    "                    hops_query = self.update(hops_query, decision_vector)  # size (L, self.n_heads, self.output_dim)\n",
    "                else :\n",
    "                    hops_query = decision_vector\n",
    "        else :\n",
    "            hops_query = base_query\n",
    "        # output decision vector\n",
    "        return hops_query, attn1_weights_list, attn2_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Modules de décodage\n",
    "\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeursSelectifs\"></a>\n",
    "\n",
    "\n",
    "### 2.4.1 Décodeur sélectif\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Classes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Classes.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_dim, n_classes) :\n",
    "        super(ClassDecoder, self).__init__() \n",
    "        self.version = 'class'\n",
    "        self.n_classes = n_classes\n",
    "        self.classes_decoder = nn.Linear(text_dim, n_classes)\n",
    "\n",
    "    def forward(self, text_vector, train_mode = False):\n",
    "        classes_vector = self.classes_decoder(text_vector).view(-1)\n",
    "        if train_mode :\n",
    "            return classes_vector\n",
    "        else :\n",
    "            classes = F.softmax(classes_vector) \n",
    "            topv, topi = classes.data.topk(1)\n",
    "            result = topi[0][0].numpy()\n",
    "            return result   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurGeneratif\"></a>\n",
    "\n",
    "\n",
    "### 2.4.2 Décodeur génératif\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![Decoder](figs/Decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class WordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, device, embedding, hidden_dim, tracking_dim, dropout = 0.1):\n",
    "        super(WordsDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tracking_dim = tracking_dim\n",
    "        # modules\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            lang_size     = p.data.size(0)\n",
    "            embedding_dim = p.data.size(1)\n",
    "        self.reduce = nn.Linear(tracking_dim, hidden_dim)\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, lang_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def generateWord(self, final_vector, hidden, current_word_index):\n",
    "        # update hidden state\n",
    "        current_word = self.embedding(current_word_index).view(1,1,-1)\n",
    "        embedded = torch.cat((final_vector, current_word), dim = 2)\n",
    "        #embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # generate next word\n",
    "        vector = self.out(hidden).squeeze(0)\n",
    "        log_proba = F.log_softmax(vector, dim = 1)\n",
    "        return log_proba, hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, last_words, query_vector, decision_vector, target_answer = None) :\n",
    "        bound = 25\n",
    "        log_probas = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        final_vector = self.reduce(decision_vector).tanh() + query_vector\n",
    "        final_vector = self.dropout(final_vector)\n",
    "        current_word_index = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        hidden = final_vector\n",
    "        for di in range(bound) :\n",
    "            log_proba, hidden = self.generateWord(final_vector, hidden, current_word_index)\n",
    "            topv, topi = log_proba.data.topk(1)\n",
    "            log_probas.append(log_proba)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            if ni == 1 : # EOS_token\n",
    "                break\n",
    "            elif target_answer is not None : # Teacher forcing\n",
    "                answer.append(ni)\n",
    "                if di < target_answer.size(0) :\n",
    "                    current_word_index = target_answer[di].to(self.device)\n",
    "                else :\n",
    "                    break\n",
    "            else :\n",
    "                answer.append(ni)\n",
    "                current_word_index = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "        return answer, log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurGeneratifAttention\"></a>\n",
    "\n",
    "\n",
    "### 2.4.3 Décodeur génératif à attention\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words_Attn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words_Attn.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class AttnWordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, device, embedding, hidden_dim, n_layers = 0, dropout = 0.1):\n",
    "        super(AttnWordsDecoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        # modules\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            lang_size     = p.data.size()[0]\n",
    "            embedding_dim = p.data.size()[1]\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_dim, hidden_dim)\n",
    "        self.attn = AdditiveAttention(hidden_dim, hidden_dim, n_layers) \n",
    "        self.concat = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, lang_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def generateWord(self, last_words, decision_vector, hidden, current_word_index):\n",
    "        # update hidden state\n",
    "        current_word = self.embedding(current_word_index).view(1,1,-1)\n",
    "        #current_word = self.dropout(current_word)\n",
    "        embedded = torch.cat((current_word, decision_vector), dim = 2)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # generate next word\n",
    "        attn, attn_weights = self.attn(hidden, last_words)\n",
    "        vector = self.concat(torch.cat((hidden, attn), dim = 2)).tanh()\n",
    "        vector = self.out(vector).squeeze(0)\n",
    "        log_proba = F.log_softmax(vector, dim = 1)\n",
    "        return log_proba, hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, last_words, query_vector, decision_vector, target_answer = None) :\n",
    "        bound = 25\n",
    "        log_probas = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        decision_vector = self.dropout(decision_vector)\n",
    "        current_word_index = Variable(torch.LongTensor([[0]])).to(self.device) # SOS_token\n",
    "        last_words = self.dropout(last_words)\n",
    "        hidden = self.dropout(query_vector)\n",
    "        for di in range(bound) :\n",
    "            log_proba, hidden = self.generateWord(last_words, decision_vector, hidden, current_word_index)\n",
    "            topv, topi = log_proba.data.topk(1)\n",
    "            log_probas.append(log_proba)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            if ni == 1 : # EOS_token\n",
    "                break\n",
    "            elif target_answer is not None : # Teacher forcing\n",
    "                answer.append(ni)\n",
    "                if di < target_answer.size(0) :\n",
    "                    current_word_index = target_answer[di].to(self.device)\n",
    "                else :\n",
    "                    break\n",
    "            else :\n",
    "                answer.append(ni)\n",
    "                current_word_index = Variable(torch.LongTensor([[ni]])).to(self.device)\n",
    "        return answer, log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour dans le répertoire courant du tableau de bord :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jb\\Desktop\\Scripts\\notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "<a id=\"basDePage\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
