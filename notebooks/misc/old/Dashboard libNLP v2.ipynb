{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tableau de bord pour la librairie libNLP\n",
    "\n",
    "Ce tableau de bord possède deux fonctions :\n",
    "\n",
    "1/ Une fonctionalité _tutorielle_ : La librairie est entièrement détaillée, avec une explication et une illustration pour chaque module et modèle.\n",
    "\n",
    "2/ Une fonctionalité d'_édition_ : Chaque modification dans une cellule de ce notebook est appliquée au module correspondant dans la librairie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des matières\n",
    "\n",
    "1. [Modèles](#modeles)\n",
    "\n",
    "    1.1 [Classifieurs de documents](#ClassifieursDeDocuments)\n",
    "    \n",
    "    1.2 [Convertisseurs de texte en distribution](#Convertisseurs)\n",
    "    \n",
    "\n",
    "2. [Modules](#modules)\n",
    "\n",
    "    2.1 [Encodeurs de texte](#encodeursDeTexte)\n",
    "        2.1.1 Encodeurs de mots\n",
    "        2.1.2 Encodeur de texte\n",
    "        \n",
    "    2.2 [Modules d'attention simple](#attentionSimple)\n",
    "        2.2.1 Attention additive\n",
    "        2.2.2 Attention additive multi-tête\n",
    "        2.2.3 Attention additive multi-hopée\n",
    "        \n",
    "    2.3 [Modules d'attention hiérarchique](#attentionHierarchique)\n",
    "    \n",
    "    2.4 [Décodeurs](#decodeurs)\n",
    "        2.4.1 Décodeurs sélectifs\n",
    "        2.4.1 Décodeurs génératifs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du répertoire principal contenant la librairie, dans le quel on se déplace ensuite et où on génère un fichier README.txt avec une brève présentation de cette librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier libNLP_v2 existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir libNLP_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jb\\Desktop\\Scripts\\notebooks\\libNLP_v2\n"
     ]
    }
   ],
   "source": [
    "%cd libNLP_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting README.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile README.txt\n",
    "\n",
    "\n",
    "Inspiration pour la construction de la librairie :\n",
    "https://github.com/pytorch/fairseq\n",
    "https://github.com/allenai/allennlp\n",
    "https://www.dabeaz.com/modulepackage/ModulePackage.pdf\n",
    "\n",
    "\n",
    "Dans le tutorial complet on doit avoir :\n",
    "\n",
    "- en intro, la représentation de la lib_NLP en graphe\n",
    "- une table des matières avec liens hypertexte. Structure :\n",
    "    1° Modèles :\n",
    "        - classifieur de documents\n",
    "    2° Modules :\n",
    "        - encodage\n",
    "        - attention\n",
    "        - encodage hiérarchique\n",
    "        - décodage\n",
    "\n",
    "- option d'écriture de chaque module dans le fichier .py correspondant dans la librairie\n",
    "\n",
    "- faire un Setup pour l'import de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation du répertoire courant en librairie Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile __init__.py\n",
    "\n",
    "#import libNLP.modules\n",
    "#import libNLP.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeles\"></a>\n",
    "\n",
    "\n",
    "# 1 Modèles\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Génération du sous-répertoire _libNLP.models_ contenant l'ensemble des modèles de Deep Learning développés dans cette librairie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier models existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/__init__.py\n",
    "\n",
    "\n",
    "from .Text_Classifier import (TextClassifier, \n",
    "                              TextClassifierCreator, \n",
    "                              TextClassifierTrainer)\n",
    "from .Root_Converter import RootConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ClassifieursDeDocuments\"></a>\n",
    "\n",
    "## 1.1 Classifieurs de documents\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Le modèle principal de classification de documents est le module **TextClassifier**, basé sur le modèle _Hierarchical Attention Network_ et constitué de trois sous-modules (stockés dans _libNLP.modules_) :\n",
    "\n",
    "- Un _Text Encoder_ afin de transformer chaque mot d'un texte en un vecteur contextualisé\n",
    "- Une _Attention Hiérarchique_ pour extraire l'information contenue dans le texte encodé\n",
    "- Un _Class Decoder_ afin de générer une classe de sortie basé sur le résultat de l'attention\n",
    "\n",
    "Ce modèle est accompagné d'une méthode **TextClassifierCreator** afin de générer un classifieur basé sur la classe TextClassifier, ainsi que d'un module **TextClassifierTrainer** permettant d'entrainer ce classifieur sur un jeu de données.\n",
    "\n",
    "![Multilabel_Classifier](figs/Attention_multiple_residuelle.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Text_Classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Text_Classifier.py\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from libNLP_v2.modules import (\n",
    "                            RecurrentWordsEncoder, \n",
    "                            TextEncoder, \n",
    "                            HierarchicalTextEncoder,\n",
    "                            \n",
    "                            AdditiveAttention,\n",
    "                            MultiHeadAttention,\n",
    "                            MultiHopedAttention,\n",
    "                            RecurrentHierarchicalAttention, \n",
    "                            \n",
    "                            ClassDecoder,\n",
    "                            MultiTaskClassDecoder,\n",
    "                            MultilabelDecoder,\n",
    "                            MultilabelDecoderV2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, device, text_encoder, attention, classes_decoder) :\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.n_level = attention.n_level if attention is not None else 1\n",
    "        # modules        \n",
    "        self.text_encoder = text_encoder\n",
    "        self.attention = attention\n",
    "        self.classes_decoder = classes_decoder\n",
    "    \n",
    "    \n",
    "    # ---------------------- Technical methods -----------------------------\n",
    "    def loadSubModule(self, text_encoder = None, attention = None, classes_decoder = None) :\n",
    "        if text_encoder is not None :\n",
    "            self.text_encoder = text_encoder\n",
    "        if attention is not None :\n",
    "            self.attention = attention\n",
    "        if classes_decoder is not None :\n",
    "            self.classes_decoder = classes_decoder\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def freezeSubModule(self, text_encoder = False, attention = False, classes_decoder = False) :\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = not text_encoder\n",
    "        for param in self.attention.parameters():\n",
    "            param.requires_grad = not attention\n",
    "        for param in self.classes_decoder.parameters():\n",
    "            param.requires_grad = not classes_decoder\n",
    "        return\n",
    "    \n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        count = 0\n",
    "        for p in self.parameters():\n",
    "            if p.requires_grad == True :\n",
    "                count += p.data.nelement()\n",
    "        return count\n",
    "    \n",
    "    \n",
    "    def flatten(self, description) :\n",
    "        '''Baisse le nombre de niveaux de 1 dans la description'''\n",
    "        flatten = []\n",
    "        for line in description :\n",
    "            flatten += line\n",
    "        return flatten\n",
    "        \n",
    "        \n",
    "    # ------------------------- Working modes --------------------------------\n",
    "    def answerTrain(self, description, rand = 0):\n",
    "        '''Applies text encoding and attention computation followed by a last linear layer \n",
    "           with output dimension the number of possible classes. Evaluation is directly \n",
    "           performed on this vector though the corresponding trainer module.\n",
    "        '''\n",
    "        if self.n_level == 1 :\n",
    "            # 1) put description into lvl_1 format\n",
    "            description = self.flatten(description)\n",
    "            \n",
    "            # 2) apply text encoding\n",
    "            words_memory, sentences_memory = self.text_encoder(description, rand)\n",
    "            \n",
    "            # 3) perform attention when applicable\n",
    "            if self.attention is not None :\n",
    "                text_vector, attn1_weights = self.attention(words_memory)\n",
    "            else :\n",
    "                text_vector = sentences_memory\n",
    "                attn1_weights = None\n",
    "            attn2_weights = None\n",
    "            \n",
    "        elif self.n_level == 2 :\n",
    "            # 1) apply text encoding\n",
    "            words_memory, sentences_memory = self.text_encoder(description, rand)\n",
    "            \n",
    "            # 2) perform attention when applicable\n",
    "            text_vector, attn1_weights, attn2_weights = self.attention(words_memory)\n",
    "            \n",
    "        classes_vector = self.classes_decoder(text_vector, train_mode = True)\n",
    "        return classes_vector, attn1_weights, attn2_weights \n",
    "\n",
    "        \n",
    "    def forward(self, description):\n",
    "        '''Applies text encoding and attention computation followed by a linear layer\n",
    "           and a softmax transformation. When a class decoder is loaded, returns the\n",
    "           index of most probable class. When a multilabel class decoder is loaded, \n",
    "           returns the list of most probable 0 - 1 labels.\n",
    "        '''\n",
    "        if self.n_level == 1 :\n",
    "            # 1) put description into lvl_1 format\n",
    "            description = self.flatten(description)\n",
    "            \n",
    "            # 2) apply text encoding\n",
    "            words_memory, sentences_memory = self.text_encoder(description, rand = 0)\n",
    "            \n",
    "            # 3) perform attention when applicable\n",
    "            if self.attention is not None :\n",
    "                text_vector, attn1_weights = self.attention(words_memory)\n",
    "            else :\n",
    "                text_vector = sentences_memory\n",
    "                attn1_weights = None\n",
    "            attn2_weights = None\n",
    "            \n",
    "        elif self.n_level == 2 :\n",
    "            # 1) apply text encoding\n",
    "            words_memory, sentences_memory = self.text_encoder(description, rand = 0)\n",
    "            \n",
    "            # 2) perform attention when applicable\n",
    "            text_vector, attn1_weights, attn2_weights = self.attention(words_memory)\n",
    "            \n",
    "        classe, probas = self.classes_decoder(text_vector)\n",
    "        return classe, probas, attn1_weights, attn2_weights \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ------------------------ Visualisation methods ---------------------------------\n",
    "    def flattenWeights(self, weights) :\n",
    "        '''Baisse le nombre de niveaux de 1 dans les poids d'attention'''\n",
    "        flatten = []\n",
    "        for weight_layer in weights :\n",
    "            flatten.append(torch.cat(tuple(weight_layer.values()), dim = 2))\n",
    "        return flatten\n",
    "    \n",
    "    \n",
    "    def formatWeights(self, description, attn1_weights, attn2_weights) :\n",
    "        if self.n_level == 2 :\n",
    "            attn1_weights = self.flattenWeights(attn1_weights)\n",
    "        hops = self.attention.hops\n",
    "        l, L = len(description), max([len(line) for line in description])\n",
    "        Table = np.zeros((l, 1, L))\n",
    "        Liste = np.zeros((l, 1)) if attn2_weights is not None else None\n",
    "        count = 0\n",
    "        count_line = 0\n",
    "        for i, line in enumerate(description) :\n",
    "            present = False\n",
    "            for j, word in enumerate(line) :\n",
    "                if word in self.text_encoder.lang.word2index.keys():\n",
    "                    present = True\n",
    "                    Table[i, 0, j] = sum([attn1_weights[k][0, 0, count].data for k in range(hops)])\n",
    "                    count += 1\n",
    "            if present and Liste is not None :\n",
    "                Liste[i] = sum([attn2_weights[k][count_line].data for k in range(hops)])\n",
    "                count_line += 1\n",
    "        return Table, Liste\n",
    "    \n",
    "    \n",
    "    def showAttention(self, description, target, liste, fig_size = 'auto', maxi = None):\n",
    "        classe, probas, attn1_weights, attn2_weights = self.forward(description)\n",
    "        if target is not None :\n",
    "            print('target : ', liste[int(target[0])])\n",
    "        print('predic : ', liste[int(classe)])\n",
    "        table, liste = self.formatWeights(description, attn1_weights, attn2_weights)\n",
    "        l = table.shape[0] if fig_size == 'auto' else fig_size[0]\n",
    "        L = table.shape[2] if fig_size == 'auto' else fig_size[1]\n",
    "        fig = plt.figure(figsize = (l, L))\n",
    "        for i, line in enumerate(description):\n",
    "            ax = fig.add_subplot(l, 1, i+1)\n",
    "            vals = table[i]\n",
    "            text = [' '] + line + [' ' for k in range(L-len(line))] if L>len(line) else [' '] + line\n",
    "            if liste is not None :\n",
    "                vals = np.concatenate((np.zeros((1, 1)) , vals), axis = 1)  \n",
    "                vals = np.concatenate((np.reshape(liste[i], (1, 1)) , vals), axis = 1)\n",
    "                text = [' '] + [' '] + text\n",
    "                \n",
    "            cax = ax.matshow(vals, vmin=0, vmax=maxi, cmap='YlOrBr')\n",
    "            ax.set_xticklabels(text, ha='left')\n",
    "            ax.set_yticklabels(' ')\n",
    "            ax.tick_params(axis=u'both', which=u'both',length=0, labelrotation = 30, labelright  = True)\n",
    "            ax.grid(b = False, which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            plt.subplots_adjust(wspace = 0.5, top = 1.2, bottom = 0)\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "def TextClassifierCreator(lang,                     ###\n",
    "                          embedding_dim,              #\n",
    "                          hidden_dim,                 # --- Encoder options\n",
    "                          n_layers,                 ###\n",
    "\n",
    "                          sentence_hidden_dim,      ###\n",
    "                          hops,                       #\n",
    "                          share,                      # --- Hierarchical encoder options\n",
    "                          transf,                     #\n",
    "                          dropout,                  ###\n",
    "\n",
    "                          n_labels,                 ### --- Decoder options\n",
    "                          decoder_version,            #\n",
    "                          \n",
    "                          device                      #\n",
    "                         ):\n",
    "    '''Create an agent with specified dimensions and specificities\n",
    "    '''\n",
    "    contextualization = True if hidden_dim > 0 else False\n",
    "    hierarchical = True if sentence_hidden_dim > 0 else False\n",
    "\n",
    "    # 1) ----- encoding -----\n",
    "    embedding = nn.Embedding(lang.n_words, embedding_dim)\n",
    "    encoder = RecurrentWordsEncoder(device, \n",
    "                                    embedding, \n",
    "                                    hidden_dim, \n",
    "                                    n_layers, \n",
    "                                    dropout) # embedding, hidden_dim, n_layers = 1, dropout = 0\n",
    "    text_encoder = HierarchicalTextEncoder(device, lang, encoder) if hierarchical else TextEncoder(device, lang, encoder)\n",
    "    word_hidden_dim = encoder.output_dim\n",
    "\n",
    "    \n",
    "    # 2) ----- attention -----\n",
    "    if decoder_version == 'class' :\n",
    "        n_heads = 1\n",
    "    elif decoder_version == 'multilabel_1' :\n",
    "        n_heads = 1\n",
    "    elif decoder_version == 'multilabel_2' :\n",
    "        n_heads = n_labels\n",
    "    if hierarchical :\n",
    "        query_dim = word_hidden_dim if hops > 1 else 0\n",
    "        attention = RecurrentHierarchicalAttention(device,\n",
    "                                                   word_hidden_dim,\n",
    "                                                   sentence_hidden_dim, \n",
    "                                                   query_dim = query_dim,\n",
    "                                                   n_heads = n_heads,\n",
    "                                                   n_layers = n_layers,\n",
    "                                                   hops = hops,\n",
    "                                                   share = share,\n",
    "                                                   transf = transf,\n",
    "                                                   dropout = dropout)\n",
    "    else :\n",
    "        attention = MultiHopedAttention(targets_dim = word_hidden_dim,\n",
    "                                        base_query_dim = 0,\n",
    "                                        hops = hops,\n",
    "                                        share = share,\n",
    "                                        transf = transf,\n",
    "                                        dropout = dropout)\n",
    "        \n",
    "    # 3) ----- decoding -----\n",
    "    text_dim = attention.output_dim\n",
    "    if decoder_version == 'class' :\n",
    "        if type(n_labels) == int :\n",
    "            classes_decoder = ClassDecoder(text_dim, n_labels)\n",
    "        elif type(n_labels) == list :\n",
    "            classes_decoder = MultiTaskClassDecoder(text_dim, n_labels, weight_list)\n",
    "    elif decoder_version == 'multilabel_1' :\n",
    "        classes_decoder = MultilabelDecoder(text_dim, n_labels)\n",
    "    elif decoder_version == 'multilabel_2' :\n",
    "        classes_decoder = MultilabelDecoderV2(text_dim, n_labels)\n",
    "    \n",
    "    # 4) ----- model -----\n",
    "    text_cl = TextClassifier(device, text_encoder, attention, classes_decoder)\n",
    "    text_cl = text_cl.to(device)\n",
    "    \n",
    "    return text_cl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextClassifierTrainer(object):\n",
    "    def __init__(self, \n",
    "                 criterion = nn.NLLLoss(), #nn.BCEWithLogitsLoss(), #nn.BCELoss(), \n",
    "                 optimizer = optim.SGD,\n",
    "                 print_every=100):\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.print_every = print_every\n",
    "        \n",
    "        \n",
    "    def asMinutes(self, s):\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "    def timeSince(self, since, percent):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        es = s / (percent)\n",
    "        rs = es - s\n",
    "        return '%s (- %s)' % (self.asMinutes(s), self.asMinutes(rs))        \n",
    "        \n",
    "        \n",
    "    def distance(self, agent, agent_output, target) :\n",
    "        \"\"\" Compute cumulated error between predicted output and ground answer.\"\"\"\n",
    "        tar = Variable(torch.LongTensor(target.reshape(-1))).to(agent.device)\n",
    "        out = F.log_softmax(agent_output).view(1, -1) if agent.classes_decoder.version == 'class' else agent_output\n",
    "        loss = self.criterion(out, tar)\n",
    "        if agent.classes_decoder.version == 'class' :\n",
    "            classes = F.softmax(agent_output) \n",
    "            topv, topi = classes.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            loss_diff = 1 if ni != target.item() else 0\n",
    "        else :\n",
    "            classes = F.sigmoid(agent_output)\n",
    "            loss_diff = 0\n",
    "            for i in range(len(target)):\n",
    "                loss_diff += 1 if abs(classes[i]-target[i]) >= 0.5 else 0\n",
    "        return loss, loss_diff\n",
    "    \n",
    "    \n",
    "    def distanceTot(self, agent, agent_output, target) :\n",
    "        \"\"\" Compute cumulated error between predicted output and ground answer.\"\"\"\n",
    "        if agent.classes_decoder.version == 'class' and type(agent_output) == list :\n",
    "            loss = 0\n",
    "            for i in range(len(target)) :\n",
    "                lossbis, loss_diffbis = self.distance(agent, agent_output[i], target[i])\n",
    "                loss = loss + agent.classes_decoder.weight_list[i]*lossbis\n",
    "                if i == 0:\n",
    "                    loss_diff = loss_diffbis\n",
    "            return loss, loss_diff\n",
    "            \n",
    "        else :\n",
    "            return self.distance(agent, agent_output, target)\n",
    "        \n",
    "        \n",
    "    def trainLoop(self, agent, description_batch, target_batch, optimizer, learning_rate, rand):\n",
    "        \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        target_length = len(target_batch[0])\n",
    "        loss = 0\n",
    "        loss_diff = 0\n",
    "        for description, target in zip(description_batch, target_batch) :\n",
    "            agent_output, attn1_weights_list, attn2_weights_list = agent.answerTrain(description, rand)\n",
    "            loss2, loss_diff2 = self.distanceTot(agent, agent_output, target)\n",
    "            loss = loss + loss2\n",
    "            loss_diff += loss_diff2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.data[0], loss_diff\n",
    "        \n",
    "        \n",
    "    def train(self, agent, \n",
    "              descriptions, \n",
    "              targets, \n",
    "              n_iters = 100, \n",
    "              n_epochs = None,\n",
    "              mini_batch_size = 1,\n",
    "              learning_rate=0.01,\n",
    "              dic = None,\n",
    "              rand = 0,\n",
    "              random_state = 42\n",
    "             ):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops.\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in agent.parameters() if param.requires_grad == True], lr=learning_rate)\n",
    "        \n",
    "        language = set(agent.text_encoder.lang.word2index.keys())\n",
    "        print_loss_total = 0  \n",
    "        print_loss_diff_mots_total = 0\n",
    "        L = len(descriptions)\n",
    "        if n_epochs is None :\n",
    "            for iter in range(1, n_iters + 1):\n",
    "                description_batch = []\n",
    "                target_batch = []\n",
    "                if dic is not None :\n",
    "                    while len(description_batch) < mini_batch_size :\n",
    "                        j = np.random.choice(range(len(dic)))\n",
    "                        if dic[j] != [] :\n",
    "                            i = np.random.choice(dic[j])\n",
    "                            description = descriptions[i]\n",
    "                            #if self.test(description, language) :\n",
    "                            description_batch.append(description)\n",
    "                            target_batch.append(targets[i]) \n",
    "                else :\n",
    "                    while len(description_batch) < mini_batch_size:\n",
    "                        i = np.random.choice(range(L))\n",
    "                        description = descriptions[i]\n",
    "                        #if self.test(description, language) :\n",
    "                        description_batch.append(description)\n",
    "                        target_batch.append(targets[i])\n",
    "\n",
    "                loss, loss_diff_mots = self.trainLoop(agent, description_batch, target_batch, optimizer, learning_rate, rand)\n",
    "                # quantité d'erreurs sur la réponse i\n",
    "                print_loss_total += loss\n",
    "                print_loss_diff_mots_total += loss_diff_mots       \n",
    "                if iter % (self.print_every / mini_batch_size) == 0:\n",
    "                    print_loss_avg = print_loss_total * mini_batch_size / self.print_every\n",
    "                    print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                    print_loss_total = 0\n",
    "                    print_loss_diff_mots_total = 0\n",
    "                    print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, iter / n_iters),\n",
    "                                                 iter * mini_batch_size, iter / n_iters * 100, \n",
    "                                                      print_loss_avg, print_loss_diff_mots_avg))\n",
    "        \n",
    "        else :\n",
    "            Liste = [k for k in range(L)]\n",
    "            for epoch in range(1, n_epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(Liste)\n",
    "                for i in range(int(L/mini_batch_size)) :\n",
    "                    description_batch = []\n",
    "                    target_batch = []\n",
    "                    for j in Liste[i*mini_batch_size: (i+1)*mini_batch_size]:\n",
    "                        description = descriptions[j]\n",
    "                        #if self.test(description, language) :\n",
    "                        description_batch.append(description)\n",
    "                        target_batch.append(targets[j])\n",
    "                        \n",
    "                    loss, loss_diff_mots = self.trainLoop(agent, description_batch, target_batch, optimizer, learning_rate, rand)\n",
    "                    # quantité d'erreurs sur la réponse i\n",
    "                    print_loss_total += loss\n",
    "                    print_loss_diff_mots_total += loss_diff_mots \n",
    "                    if i> 0 and i % (self.print_every / mini_batch_size) == 0:\n",
    "                        print_loss_avg = print_loss_total * mini_batch_size / self.print_every\n",
    "                        print_loss_diff_mots_avg = print_loss_diff_mots_total / self.print_every\n",
    "                        print_loss_total = 0\n",
    "                        print_loss_diff_mots_total = 0\n",
    "                        print('%s (%d %d%%) %.4f %.2f' % (self.timeSince(start, i / int(L/mini_batch_size)),\n",
    "                                                     i * mini_batch_size, i / int(L/mini_batch_size) * 100, \n",
    "                                                          print_loss_avg, print_loss_diff_mots_avg))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Convertisseurs\"></a>\n",
    "\n",
    "## 1.2 Convertisseurs de texte en distribution\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/Root_Converter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/Root_Converter.py\n",
    "\n",
    "# Pacific\n",
    "\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker #, FuncFormatter\n",
    "#%matplotlib inline\n",
    "from termcolor import colored\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class RootConverter(nn.Module):\n",
    "    '''Transforms a sentence into a distribution over words of a given document'''\n",
    "    def __init__(self, lang, embedding_dim, pre_entrainement = None, freeze = False, softmax = False) :\n",
    "        super(RootConverter, self).__init__() \n",
    "        \n",
    "        self.lang = lang\n",
    "        self.lang_size = lang.n_words          # size of word vocabulary\n",
    "        self.embedding_dim = embedding_dim     # dimension of embedded words\n",
    "        self.softmax = F.softmax if softmax else None\n",
    "        self.embedding = nn.Embedding(self.lang_size, embedding_dim)\n",
    "        if pre_entrainement is not None :\n",
    "            self.embedding.load_state_dict({'weight': torch.Tensor(pre_entrainement)})\n",
    "        if freeze :\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "            \n",
    "    def indexesFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a given sentence into a list of indices according to a given language'''\n",
    "        indexes=[]\n",
    "        unknowns = 0\n",
    "        for word in sentence:\n",
    "            p = random.random()\n",
    "            if word not in self.lang.word2index.keys() :\n",
    "                pass\n",
    "                #indexes.append(UNK_token) #pass\n",
    "            elif p >= rand :\n",
    "                indexes.append(self.lang.word2index[word])\n",
    "            elif p < rand :\n",
    "                e = random.choice([1, 2, 3])\n",
    "                if e == 1 :  # doesn't put any word\n",
    "                    pass\n",
    "                elif e == 2 :# hide word with UNK_Token\n",
    "                    indexes.append(UNK_token)\n",
    "                else :       # put word followed with randomly selected other word\n",
    "                    indexes.append(self.lang.word2index[word])\n",
    "                    indexes.append(random.choice(list(range(self.lang.n_words))))\n",
    "\n",
    "        # remove exceeding words, exept first word and the two last words or symbols\n",
    "        if max_length is not None :\n",
    "            print(max_length)\n",
    "            while len(indexes) > max_length:\n",
    "                indexes.pop(random.randint(1,len(indexes)-2))\n",
    "        return indexes\n",
    "\n",
    "\n",
    "    def variableFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a sentence into a torch variable, containing a list of indices according\n",
    "           to a given language.\n",
    "        '''\n",
    "        indexes = self.indexesFromSentence(sentence, max_length, rand)                                 \n",
    "        result = Variable(torch.LongTensor(indexes).view(-1, 1)) if len(indexes) != 0 else None\n",
    "        return result\n",
    "\n",
    "\n",
    "    def variableFromDescription(self, description, max_length = None, rand = 0):\n",
    "        '''Turn a whole dialogue into a list of torch tensors, according to an input\n",
    "           and an output languages'''\n",
    "        sortie = []\n",
    "        for line in description :\n",
    "            variable = self.variableFromSentence(line, max_length, rand)\n",
    "            if variable is not None :\n",
    "                sortie.append(variable)\n",
    "        return sortie\n",
    "    \n",
    "    \n",
    "    def flatten(self, description) :\n",
    "        for i, line in enumerate(description) :\n",
    "            if i == 0:\n",
    "                flatten = line\n",
    "            else :\n",
    "                flatten = torch.cat((flatten, line), dim = 0)\n",
    "        return flatten\n",
    "\n",
    "\n",
    "    def lendes(self, description) :\n",
    "        length = 0\n",
    "        for line in description :\n",
    "            length += len(line)\n",
    "        return length\n",
    "    \n",
    "    \n",
    "    def complete(self, description, weights) :\n",
    "        final_weights = []\n",
    "        count = 0\n",
    "        for i in range(len(description)) :\n",
    "            line = description[i]\n",
    "            w = torch.zeros(len(line))\n",
    "            for j, word in enumerate(line) :\n",
    "                if word in self.lang.word2index.keys():\n",
    "                    w[j] = weights[0, count]\n",
    "                    count += 1\n",
    "            final_weights.append(w)\n",
    "        return final_weights\n",
    "                     \n",
    "            \n",
    "        \n",
    "    def convert(self, description, sentence):\n",
    "        \n",
    "        # acquisition\n",
    "        var = self.variableFromDescription(description)\n",
    "        var = self.flatten(var)\n",
    "        var = self.embedding(var)                                      #dim = (description_length, 1, embedding_dim)\n",
    "        \n",
    "        # calcul de distribution par mot du résumé\n",
    "        if self.softmax is None :\n",
    "            var = F.normalize(var, p = 2, dim = 2)                     # <---- normalize on last dimension\n",
    "        var = torch.transpose(var, 0, 1)                               #dim = (1, description_length, embedding_dim)\n",
    "        var = torch.transpose(var, 1, 2)                               #dim = (1, embedding_dim, description_length)\n",
    "        \n",
    "        query = self.embedding(self.variableFromSentence(sentence))    #dim = (sentence_length, 1, embedding_dim)\n",
    "        if self.softmax is None :\n",
    "            query = F.normalize(query, p = 2, dim = 2)                 # <---- normalize on last dimension\n",
    "        query = torch.transpose(query, 0, 1)                           #dim = (1, sentence_length, embedding_dim)\n",
    "        \n",
    "        weights = torch.bmm(query, var)                                # size (1, sentence_length, description_length)\n",
    "        if self.softmax is not None :\n",
    "            weights = self.softmax(weights, dim = 2)\n",
    "            \n",
    "        # calcul de distribution par phrase du résumé\n",
    "        weights, _ = torch.max(weights, dim = 1)                       # size (1, description_length)\n",
    "        #weights = torch.sqrt(weights)                                 # size (1, description_length)\n",
    "        # ou torch.sum(weights, dim = 1)\n",
    "        weights = weights/sum(weights.data[0]) # <--- probability distribution over words of the description\n",
    "\n",
    "        final_weights = self.complete(description, weights)\n",
    "        return final_weights\n",
    "    \n",
    "    \n",
    "    def computeSentenceAttention(self, description, sentence):\n",
    "        attn_weights = self.convert(description, sentence)\n",
    "        l = len(description)\n",
    "        sentence_attn = np.zeros((l))\n",
    "        for i in range(l) :\n",
    "            sentence_attn[i] = torch.sum(attn_weights[i]).data[0] #torch.mean\n",
    "        return attn_weights, sentence_attn\n",
    "            \n",
    "    \n",
    "    \n",
    "    def showConvertion(self, description, sentence, maxi = None):\n",
    "        attn_weights, sentence_attn = self.computeSentenceAttention(description, sentence)\n",
    "        if maxi is None :\n",
    "            maxi = max([torch.max(attn_weights[i]).data[0] for i in range(len(attn_weights))])\n",
    "        count = 0\n",
    "        l = len(description)\n",
    "        L = max([len(line) for line in description])\n",
    "        fig = plt.figure(figsize = [L+1, l]) #\n",
    "        for i, line in enumerate(description):\n",
    "            vals = np.zeros((1, L))\n",
    "            for j in range(len(line)) :\n",
    "                vals[0, j] = attn_weights[i][j].data[0]\n",
    "            #vals = attn_weights[0, count : count + b].unsqueeze(0).numpy()\n",
    "            ax = fig.add_subplot(l, 1, i+1)\n",
    "            cax = ax.matshow(vals, vmin=0, vmax=maxi, cmap='YlOrBr') #, extent = (0, l, 0, 0.3)) # 'bone_r'\n",
    "            #fig.colorbar(cax)\n",
    "            text = [' '] + line + [' ' for k in range(L-len(line))] if L>len(line) else [' '] + line\n",
    "            ax.set_xticklabels(text, ha='left')\n",
    "            sc = str(sentence_attn[i]*100)[:5]\n",
    "            ax.set_yticklabels(' ' + sc)\n",
    "            ax.tick_params(axis = 'x', which = u'both',length = 0, labelrotation = 30, labelright = True)\n",
    "            ax.tick_params(axis = 'y', which = u'both',length = 0, labelrotation = 0)\n",
    "            ax.grid(b = False, which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "            plt.subplots_adjust(wspace = 0.5, top = 1.2, bottom = 0)\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def select(self, sentence_attn, liste_nwords, ratio = 3/4, percentage = None):\n",
    "        if percentage is not None :\n",
    "            listo = list(sentence_attn)\n",
    "            lister = list(sentence_attn)\n",
    "            retenus = []\n",
    "            count = 0\n",
    "            while count <= percentage and listo != [] :\n",
    "                maxi = max(listo)\n",
    "                i = lister.index(maxi)\n",
    "                if count + liste_nwords[i] <= percentage :\n",
    "                    retenus.append(i)\n",
    "                listo.remove(maxi)\n",
    "                count += liste_nwords[i]\n",
    "            return retenus\n",
    "        else :\n",
    "            listo = list(sentence_attn)\n",
    "            lister = list(sentence_attn)\n",
    "            #print(tot)\n",
    "            retenus = []\n",
    "            count = 0\n",
    "            while count < ratio and listo != [] :\n",
    "                maxi = max(listo)\n",
    "                i = lister.index(maxi)\n",
    "                #if count + maxi <= 3*tot/4 :\n",
    "                retenus.append(i)\n",
    "                listo.remove(maxi)\n",
    "                count += maxi\n",
    "                #print(count)\n",
    "            return retenus\n",
    "    \n",
    "    \n",
    "    def showSelection(self, description, sentence, ratio = 3/4, percentage = None):\n",
    "        attn_weights, sentence_attn = self.computeSentenceAttention(description, sentence)\n",
    "        nwords = [len(line) for line in description]\n",
    "        nwords = [el*100/sum(nwords) for el in nwords]\n",
    "        retenus = self.select(sentence_attn, nwords, ratio, percentage)\n",
    "        for i, line in enumerate(description) :\n",
    "            if i in retenus :\n",
    "                print(' '.join(line))\n",
    "            else :\n",
    "                print(colored(' '.join(line), 'blue'))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>\n",
    "\n",
    "\n",
    "\n",
    "# 2 Modules\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Un sous-r‚pertoire ou un fichier modules existe d‚j….\n"
     ]
    }
   ],
   "source": [
    "%mkdir modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/__init__.py\n",
    "\n",
    "from .Encoder_Words_Recurrent import RecurrentWordsEncoder\n",
    "from .Encoder_Text import TextEncoder, HierarchicalTextEncoder\n",
    "\n",
    "from .Attention_Additive import AdditiveAttention\n",
    "from .Attention_MultiHead import MultiHeadAttention\n",
    "from .Attention_MultiHoped import MultiHopedAttention\n",
    "from .Attention_Hierarchical_Recurrent import RecurrentHierarchicalAttention\n",
    "\n",
    "from .Decoder_Multilabel import MultilabelDecoder, MultilabelDecoderV2 \n",
    "from .Decoder_Class import ClassDecoder, MultiTaskClassDecoder\n",
    "from .Decoder_Words import WordsDecoder\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'RecurrentWordsEncoder',\n",
    "    'TextEncoder',\n",
    "    'HierarchicalTextEncoder',\n",
    "    \n",
    "    'AdditiveAttention',\n",
    "    'MultiHeadAttention',\n",
    "    'MultiHopedAttention',\n",
    "    'RecurrentHierarchicalAttention',\n",
    "    \n",
    "    'MultilabelDecoder',\n",
    "    'MultilabelDecoderV2',\n",
    "    'ClassDecoder',\n",
    "    'MultiTaskClassDecoder',\n",
    "    'WordsDecoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encodeursDeTexte\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Encodeurs de texte\n",
    "\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encodeursDeMots\"></a>\n",
    "\n",
    "### 2.1.1 Encodeur de mots\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module **RecurrentWordsEncoder** encode une séquence de mots $w_1, ..., w_T$ en une séquence de vecteurs $h_1, ..., h_T$ en appliquant un plongement suivi d'une couche GRU bi-directionnelle. On peut représenter son fonctionnement par la figure suivante :\n",
    "\n",
    "\n",
    "![WordEncoder](figs/WordEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Encoder_Words_Recurrent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Encoder_Words_Recurrent.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class RecurrentWordsEncoder(nn.Module):\n",
    "    def __init__(self, device, embedding, hidden_dim, n_layers = 1, dropout = 0): \n",
    "        super(RecurrentWordsEncoder, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim           # dimension of hidden state of GRUs \n",
    "        self.dropout_p = dropout\n",
    "        self.n_layers = n_layers               # number of stacked GRU layers\n",
    "        self.output_dim = hidden_dim * 2       # dimension of outputed rep. of words and utterance\n",
    "        # parameters\n",
    "        self.embedding = embedding\n",
    "        for p in embedding.parameters() :\n",
    "            embedding_dim = p.data.size(1)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.bigru = nn.GRU(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), \n",
    "                            bidirectional=True)\n",
    "\n",
    "        \n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(2 * self.n_layers, 1, self.hidden_dim)).to(self.device)\n",
    "\n",
    "    def forward(self, utterance, hidden = None):\n",
    "        embeddings = self.embedding(utterance)                          # dim = (input_length, 1, embedding_dim)\n",
    "        embeddings = self.dropout(embeddings)                           # dim = (input_length, 1, embedding_dim)\n",
    "        outputs, hidden = self.bigru(embeddings, hidden)\n",
    "        outputs = self.dropout(outputs)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return outputs, hidden                                          # dim = (input_length, 1, hidden_dim * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encodeurDeTexte\"></a>\n",
    "\n",
    "### 2.1.2 Encodeur de texte\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "Le module **TextEncoder** applique un module WordsEncoder pour l'encodage de séquences sur chaque phrase d'un document, en passant l'état caché du GRU bi-directionnel en fin d'encodage d'une phrase pour l'initiation de l'encodage de la phrase suivante. Ceci peut être représenté par la figure :\n",
    "\n",
    "\n",
    "![TextEncoder](figs/TextEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Encoder_Text.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Encoder_Text.py\n",
    "\n",
    "# Modins\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,device, lang, word_encoder):\n",
    "        super(TextEncoder, self).__init__()\n",
    "    \n",
    "        self.device = device\n",
    "        self.words_memory_dim = word_encoder.output_dim\n",
    "        self.lang = lang      \n",
    "        self.word_encoder = word_encoder\n",
    "        \n",
    "        \n",
    "    def indexesFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a given sentence into a list of indices according to a given language'''\n",
    "        indexes=[]\n",
    "        unknowns = 0\n",
    "        for word in sentence:\n",
    "            p = random.random()\n",
    "            if word not in self.lang.word2index.keys() :\n",
    "                if 'UNK' in self.lang.word2index.keys() :\n",
    "                    indexes.append(self.lang.word2index['UNK'])\n",
    "                else :\n",
    "                    pass\n",
    "            elif p >= rand :\n",
    "                indexes.append(self.lang.word2index[word])\n",
    "            elif p < rand :\n",
    "                e = random.choice([1, 2, 3])\n",
    "                if e == 1 :  # doesn't put any word\n",
    "                    pass\n",
    "                elif e == 2 :# hide word with UNK_Token\n",
    "                    indexes.append(self.lang.word2index['UNK'])\n",
    "                else :       # put word followed with randomly selected other word\n",
    "                    indexes.append(self.lang.word2index[word])\n",
    "                    indexes.append(random.choice(list(range(self.lang.n_words))))\n",
    "\n",
    "        # remove exceeding words, exept first word and the two last words or symbols\n",
    "        if max_length is not None :\n",
    "            #print(max_length)\n",
    "            while len(indexes) > max_length:\n",
    "                indexes.pop(random.randint(1,len(indexes)-2))\n",
    "        return indexes\n",
    "\n",
    "\n",
    "    def variableFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a sentence into a torch variable, containing a list of indices according\n",
    "           to a given language.\n",
    "        '''\n",
    "        indexes = self.indexesFromSentence(sentence, max_length, rand)                                 \n",
    "        result = Variable(torch.LongTensor(indexes).view(-1, 1)) if len(indexes) != 0 else None\n",
    "        result = result.to(self.device)\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def forward(self, input, rand = 0):\n",
    "        \"\"\"Parameters are a single current query as string, and the model learns to generate the current answer. \n",
    "           Attention weights over words and past utterances can be provided with the 'provideAttention' option.\n",
    "        \"\"\"\n",
    "        description = self.variableFromSentence(input, rand = rand)\n",
    "        words_memory = {}\n",
    "        sentences_memory = {}\n",
    "        query_hidden = self.word_encoder.initHidden()\n",
    "        \n",
    "        words_memory, query_hidden = self.word_encoder(description, query_hidden)\n",
    "        sentences_memory = query_hidden\n",
    "            \n",
    "        return words_memory.unsqueeze(1), sentences_memory \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HierarchicalTextEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, lang, word_encoder):\n",
    "        super(HierarchicalTextEncoder, self).__init__()\n",
    "    \n",
    "        self.device = device\n",
    "        self.words_memory_dim = word_encoder.output_dim\n",
    "        self.lang = lang      \n",
    "        self.word_encoder = word_encoder\n",
    "        \n",
    "        \n",
    "    def indexesFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a given sentence into a list of indices according to a given language'''\n",
    "        indexes=[]\n",
    "        unknowns = 0\n",
    "        for word in sentence:\n",
    "            p = random.random()\n",
    "            if word not in self.lang.word2index.keys() :\n",
    "                if 'UNK' in self.lang.word2index.keys() :\n",
    "                    indexes.append(self.lang.word2index['UNK'])\n",
    "                else :\n",
    "                    pass\n",
    "            elif p >= rand :\n",
    "                indexes.append(self.lang.word2index[word])\n",
    "            elif p < rand :\n",
    "                e = random.choice([1, 2, 3])\n",
    "                if e == 1 :  # doesn't put any word\n",
    "                    pass\n",
    "                elif e == 2 :# hide word with UNK_Token\n",
    "                    indexes.append(self.lang.word2index['UNK'])\n",
    "                else :       # put word followed with randomly selected other word\n",
    "                    indexes.append(self.lang.word2index[word])\n",
    "                    indexes.append(random.choice(list(range(self.lang.n_words))))\n",
    "\n",
    "        # remove exceeding words, exept first word and the two last words or symbols\n",
    "        if max_length is not None :\n",
    "            print(max_length)\n",
    "            while len(indexes) > max_length:\n",
    "                indexes.pop(random.randint(1,len(indexes)-2))\n",
    "        return indexes\n",
    "\n",
    "\n",
    "    def variableFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a sentence into a torch variable, containing a list of indices according\n",
    "           to a given language.\n",
    "        '''\n",
    "        indexes = self.indexesFromSentence(sentence, max_length, rand)                                 \n",
    "        result = Variable(torch.LongTensor(indexes).view(-1, 1)) if len(indexes) != 0 else None\n",
    "        return result\n",
    "\n",
    "\n",
    "    def variablesFromDescription(self, description, max_length = None, rand = 0):\n",
    "        '''Turn a whole dialogue into a list of torch tensors, according to an input\n",
    "           and an output languages'''\n",
    "        sortie = []\n",
    "        for line in description :\n",
    "            variable = self.variableFromSentence(line, max_length, rand)\n",
    "            if variable is not None :\n",
    "                variable = variable.to(self.device)\n",
    "                sortie.append(variable)\n",
    "        return sortie\n",
    "        \n",
    "        \n",
    "    def forward(self, input, rand = 0):\n",
    "        \"\"\"Parameters are a single current query as string, and the model learns to generate the current answer. \n",
    "           Attention weights over words and past utterances can be provided with the 'provideAttention' option.\n",
    "        \"\"\"\n",
    "        description = self.variablesFromDescription(input, rand = rand)\n",
    "        words_memory = {}\n",
    "        sentences_memory = {}\n",
    "        query_hidden = self.word_encoder.initHidden()\n",
    "        \n",
    "        for i in range(len(description)) :\n",
    "            utterance = description[i]\n",
    "            words_hiddens, query_hidden = self.word_encoder(utterance, query_hidden)\n",
    "            words_memory[i] = words_hiddens\n",
    "            sentences_memory[i] = query_hidden\n",
    "        \n",
    "            \n",
    "        return words_memory, sentences_memory  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modulesAttention\"></a>\n",
    "\n",
    "\n",
    "## 2.2 Modules d'attention simple\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "\n",
    "<a id=\"attentionSimple\"></a>\n",
    "\n",
    "\n",
    "### 2.2.1 Module d'attention additive\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![AttentionAdditive](figs/Attention_Additive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_Additive.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_Additive.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, query_dim, targets_dim, n_layers = 1): \n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.n_level = 1\n",
    "        self.query_dim = query_dim\n",
    "        self.targets_dim = targets_dim\n",
    "        self.output_dim = targets_dim\n",
    "        self.n_layers = n_layers\n",
    "        # parameters\n",
    "        self.attn_layer = nn.Linear(query_dim + targets_dim, targets_dim) if n_layers >= 1 else None\n",
    "        self.attn_layer2 = nn.Linear(targets_dim, targets_dim) if n_layers >= 2 else None\n",
    "        self.attn_v = nn.Linear(targets_dim, 1, bias = False) if n_layers >= 1 else None\n",
    "        self.act = F.softmax\n",
    "\n",
    "        \n",
    "    def forward(self, query = None, targets = None):\n",
    "        '''takes as parameters : \n",
    "                a query tensor conditionning the attention,     size = (1, minibatch_size, query_dim)\n",
    "                a tensor containing attention targets           size = (targets_length, minibatch_size, targets_dim)\n",
    "           returns : \n",
    "                the resulting tensor of the attention process,  size = (1, minibatch_size, targets_dim)\n",
    "                the attention weights,                          size = (1, targets_length)\n",
    "        '''\n",
    "        if targets is not None :\n",
    "            # concat method \n",
    "            if self.n_layers >= 1 :\n",
    "                poids = torch.cat((query.expand(targets.size(0), -1, -1), targets), 2) if query is not None else targets\n",
    "                poids = self.attn_layer(poids).tanh()                 # size (targets_length, minibatch_size, targets_dim)\n",
    "                if self.n_layers >= 2 :\n",
    "                    poids = self.attn_layer2(poids).tanh()            # size (targets_length, minibatch_size, targets_dim)\n",
    "                attn_weights = self.attn_v(poids)                     # size (targets_length, minibatch_size, 1)\n",
    "                attn_weights = torch.transpose(attn_weights, 0,1)     # size (minibatch_size, targets_length, 1)\n",
    "                targets = torch.transpose(targets, 0,1)               # size (minibatch_size, targets_length, targets_dim)\n",
    "            # dot method\n",
    "            else :\n",
    "                targets = torch.transpose(targets, 0,1)               # size (minibatch_size, targets_length, targets_dim)\n",
    "                query = torch.transpose(query, 0, 1)                  # size (minibatch_size, 1, query_dim)\n",
    "                query = torch.transpose(query, 1, 2)                  # size (minibatch_size, query_dim, 1)\n",
    "                attn_weights = torch.bmm(targets, query)              # size (minibatch_size, targets_length, 1)\n",
    "                \n",
    "            attn_weights = self.act(attn_weights, dim = 1)        # size (minibatch_size, targets_length, 1)\n",
    "            attn_weights = torch.transpose(attn_weights, 1,2)     # size (minibatch_size, 1, targets_length)\n",
    "            attn_applied = torch.bmm(attn_weights, targets)       # size (minibatch_size, 1, targets_dim)\n",
    "            attn_applied = torch.transpose(attn_applied, 0,1)     # size (1, minibatch_size, targets_dim)\n",
    "\n",
    "        else :\n",
    "            attn_applied = query\n",
    "            attn_weights = None\n",
    "        return attn_applied, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"attentionAdditiveMultitete\"></a>\n",
    "\n",
    "\n",
    "### 2.2.2 Module d'attention additive multi-tête\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_MultiHead.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_MultiHead.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Module performing additive attention over a sequence of vectors stored in\n",
    "       a memory block, conditionned by some vector. At instanciation it takes as imput :\n",
    "       \n",
    "                - query_dim : the dimension of the conditionning vector\n",
    "                - targets_dim : the dimension of vectors stored in memory\n",
    "                \n",
    "      Other ideas on Multi head attention on \n",
    "      https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py\n",
    "      https://github.com/tlatkowski/multihead-siamese-nets/blob/master/layers/attention.py\n",
    "    '''\n",
    "    def __init__(self, device, n_heads, query_dim, targets_dim, n_layers = 2): \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # relevant quantities\n",
    "        self.device = device\n",
    "        self.n_level = 1\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        # parameters\n",
    "        self.attn_modules_list = nn.ModuleList([AdditiveAttention(query_dim, targets_dim, n_layers) for i in range(n_heads)])\n",
    "\n",
    "        \n",
    "    def forward(self, query = None, targets = None):\n",
    "        '''takes as parameters : \n",
    "                a query tensor conditionning the attention,     size = (1, n_heads, query_dim)\n",
    "                a tensor containing attention targets           size = (targets_length, n_heads, targets_dim)\n",
    "           returns : \n",
    "                the resulting tensor of the attention process,  size = (1, n_heads, targets_dim)\n",
    "                the attention weights,                          size = (n_heads, 1, targets_length)\n",
    "        '''\n",
    "        print(\"multihead attention\")\n",
    "        targets_length = targets.size(0)\n",
    "        targets_dim    = targets.size(2)\n",
    "        attn_applied   = Variable(torch.zeros(1, self.n_heads, targets_dim)).to(self.device)\n",
    "        attn_weights   = torch.zeros(self.n_heads, 1, targets_length).to(self.device)\n",
    "        for i, attn in enumerate(self.attn_modules_list) :\n",
    "            que = query[:, i, :] if query is not None else None\n",
    "            print(que.size())\n",
    "            tar = targets[:, i, :].unsqueeze(1)\n",
    "            print(tar.size())\n",
    "            attn_appl, attn_wghts = attn(que, tar)\n",
    "            print(attn_appl.size())\n",
    "            print(attn_wghts.size())\n",
    "            attn_applied[:, i, :] = attn_appl.squeeze(1)\n",
    "            attn_weights[i, :, :] = attn_wghts.squeeze(0)\n",
    "        return attn_applied, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attentionAdditiveMultihoped\"></a>\n",
    "\n",
    "\n",
    "### 2.2.3 Module d'attention additive multi-hopée\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_MultiHoped.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_MultiHoped.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from . import AdditiveAttention\n",
    "\n",
    "\n",
    "class MultiHopedAttention(nn.Module):\n",
    "    '''Module performing additive attention over a sequence of vectors stored in\n",
    "       a memory block, conditionned by some vector. At instanciation it takes as imput :\n",
    "       \n",
    "                - query_dim : the dimension of the conditionning vector\n",
    "                - targets_dim : the dimension of vectors stored in memory\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 targets_dim,\n",
    "                 base_query_dim = 0,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(MultiHopedAttention, self).__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.targets_dim = targets_dim\n",
    "        self.output_dim = targets_dim\n",
    "        self.hops_query_dim = self.output_dim if hops > 1 else 0\n",
    "        self.query_dim = base_query_dim + self.hops_query_dim\n",
    "        \n",
    "        # structural coefficients\n",
    "        self.device = device\n",
    "        self.n_level = 1\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        self.transf = transf\n",
    "        self.dropout_p = dropout\n",
    "        if dropout > 0 :\n",
    "            self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # parameters\n",
    "        self.attn = AdditiveAttention(self.query_dim, self.targets_dim) \n",
    "        self.transf = nn.GRU(self.targets_dim, self.targets_dim) if transf else None\n",
    "        \n",
    "        \n",
    "    def initQuery(self): \n",
    "        if self.hops_query_dim > 0 :\n",
    "            return Variable(torch.zeros(1, 1, self.hops_query_dim)).to(self.device)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def update(self, hops_query, decision_vector):\n",
    "        if self.transf is not None :\n",
    "            _ , update = self.transf(decision_vector, hops_query)\n",
    "        else :\n",
    "            update = hops_query + decision_vector\n",
    "        return update\n",
    "    \n",
    "    \n",
    "    def forward(self, words_memory, base_query = None):\n",
    "        attn_weights_list = []\n",
    "        if self.hops > 1 and self.share :\n",
    "            hops_query = self.initQuery()\n",
    "        else :\n",
    "            hops_query = None\n",
    "            \n",
    "        for hop in range(self.hops) :\n",
    "            if base_query is not None and hops_query is not None :\n",
    "                query = torch.cat((base_query, hops_query), 2) # size (1, self.n_heads, self.query_dim)\n",
    "            elif base_query is not None :\n",
    "                query = base_query\n",
    "            elif hops_query is not None :\n",
    "                query = hops_query\n",
    "            else :\n",
    "                query = None\n",
    "            \n",
    "            decision_vector, attn_weights = self.attn(query, words_memory)\n",
    "            attn_weights_list.append(attn_weights)\n",
    "            if self.hops > 1 :\n",
    "                hops_query = self.update(hops_query, decision_vector) if hops_query is not None else decision_vector                          # size (L, n_classes, output_dim)\n",
    "            else :\n",
    "                hops_query = decision_vector\n",
    "  \n",
    "        \n",
    "        # output decision vector\n",
    "        return hops_query, attn_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"attentionHierarchique\"></a>\n",
    "\n",
    "\n",
    "## 2.3 Attention hiérarchique\n",
    "\n",
    "[Retour à la table des matières](#plan)\n",
    "\n",
    "![HierarchicalAttention](figs/Hierarchical_Attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Attention_Hierarchical_Recurrent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Attention_Hierarchical_Recurrent.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from . import AdditiveAttention, MultiHeadAttention\n",
    "\n",
    "\n",
    "class RecurrentHierarchicalAttention(nn.Module):\n",
    "    '''Ce module d'attention est :\n",
    "    \n",
    "    - hiérarchique avec bi-GRU entre chaque niveau d'attention\n",
    "    - multi-tête sur chaque niveau d'attention\n",
    "    - globalement multi-hopé, où il est possible d'effectuer plusieurs passes pour accumuler de l'information\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 device,\n",
    "                 word_hidden_dim, \n",
    "                 sentence_hidden_dim,\n",
    "                 query_dim = 0, \n",
    "                 n_heads = 1,\n",
    "                 n_layers = 1,\n",
    "                 hops = 1,\n",
    "                 share = True,\n",
    "                 transf = False,\n",
    "                 dropout = 0\n",
    "                ):\n",
    "        super(RecurrentHierarchicalAttention, self).__init__()\n",
    "        \n",
    "        # dimensions\n",
    "        self.query_dim = query_dim\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        self.sentence_input_dim = self.word_hidden_dim\n",
    "        self.sentence_hidden_dim = sentence_hidden_dim\n",
    "        self.context_vector_dim = sentence_hidden_dim * 2\n",
    "        self.output_dim = sentence_hidden_dim * 2\n",
    "        \n",
    "        # structural coefficients\n",
    "        self.device = device\n",
    "        self.n_level = 2\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.hops = hops\n",
    "        self.share = share\n",
    "        self.dropout_p = dropout\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # first attention module\n",
    "        attn1_list = []\n",
    "        if share :\n",
    "            attn1 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.word_hidden_dim) if n_heads > 1 else \\\n",
    "                    AdditiveAttention(self.query_dim, self.word_hidden_dim) \n",
    "            for hop in range(hops):\n",
    "                attn1_list.append(attn1)\n",
    "            self.attn1 = nn.ModuleList(attn1_list)\n",
    "        else :\n",
    "            for hop in range(hops):\n",
    "                attn1 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.word_hidden_dim) if n_heads > 1 else \\\n",
    "                        AdditiveAttention(self.query_dim, self.word_hidden_dim) \n",
    "                attn1_list.append(attn1)\n",
    "            self.attn1 = nn.ModuleList(attn1_list)\n",
    "        \n",
    "        # intermediate encoder module\n",
    "        self.bigru = nn.GRU(self.sentence_input_dim, \n",
    "                            self.sentence_hidden_dim, \n",
    "                            n_layers,\n",
    "                            dropout=(0 if n_layers == 1 else dropout), \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        # second attention module\n",
    "        attn2_list = []\n",
    "        if share :\n",
    "            attn2 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.context_vector_dim) if n_heads > 1 else \\\n",
    "                    AdditiveAttention(self.query_dim, self.context_vector_dim) \n",
    "            for hop in range(hops):\n",
    "                attn2_list.append(attn2)\n",
    "            self.attn2 = nn.ModuleList(attn2_list)\n",
    "        else :\n",
    "            for hop in range(hops):\n",
    "                attn2 = MultiHeadAdditiveAttention(n_heads, self.query_dim, self.context_vector_dim) if n_heads > 1 else \\\n",
    "                        AdditiveAttention(self.query_dim, self.context_vector_dim) \n",
    "                attn2_list.append(attn2)\n",
    "            self.attn2 = nn.ModuleList(attn2_list)\n",
    "        \n",
    "        # accumulation step\n",
    "        self.transf = nn.Linear(self.output_dim, self.output_dim, bias = False) if transf \\\n",
    "                      or self.hops > 1 else None\n",
    "\n",
    "\n",
    "    def initQuery(self): \n",
    "        if self.query_dim > 0 :\n",
    "            return Variable(torch.zeros(1, self.n_heads, self.query_dim)).to(self.device)\n",
    "        return None\n",
    "        \n",
    "                \n",
    "    def initHidden(self): \n",
    "        return Variable(torch.zeros(2 * self.n_layers, self.n_heads, self.sentence_hidden_dim)).to(self.device)\n",
    "        \n",
    "        \n",
    "    def singlePass(self, words_memory, query, attn1, attn2): \n",
    "        L = len(words_memory)\n",
    "        attn1_weights = {}\n",
    "        bigru_inputs = Variable(torch.zeros(L, self.n_heads, self.sentence_input_dim)).to(self.device)\n",
    "        # first attention layer\n",
    "        for i in range(L) :\n",
    "            targets = words_memory[i]                              # size (N_i, 1, 2*word_hidden_dim)\n",
    "            targets = targets.repeat(1, self.n_heads, 1)           # size (N_i, n_heads, 2*word_hidden_dim)\n",
    "            attn1_output, attn1_wghts = attn1(query, targets)\n",
    "            attn1_output = self.dropout(attn1_output)\n",
    "            attn1_weights[i] = attn1_wghts\n",
    "            bigru_inputs[i] = attn1_output.squeeze(0)              # size (n_heads, 2*word_hidden_dim)\n",
    "        # intermediate biGRU\n",
    "        bigru_hidden = self.initHidden()\n",
    "        attn2_inputs, bigru_hidden = self.bigru(bigru_inputs, bigru_hidden)  # size (L, n_heads, 2*word_hidden_dim)\n",
    "        # second attention layer\n",
    "        attn2_inputs = self.dropout(attn2_inputs)\n",
    "        decision_vector, attn2_weights = attn2(query = query, targets = attn2_inputs)\n",
    "        attn2_weights = attn2_weights.view(-1)\n",
    "        decision_vector = self.dropout(decision_vector)\n",
    "        # output decision vector\n",
    "        return decision_vector, attn1_weights, attn2_weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update(self, query, decision_vector):\n",
    "        if self.transf is not None :\n",
    "            update = query + self.transf(decision_vector) if query is not None else self.transf(decision_vector)\n",
    "        else :\n",
    "            update = query + decision_vector if query is not None else decision_vector\n",
    "        return update\n",
    "        \n",
    "        \n",
    "    def forward(self, words_memory, query = None):\n",
    "        '''takes as parameters : \n",
    "                a tensor containing words_memory vectors        dim = (words_memory_length, word_hidden_dim)\n",
    "                a tensor containing past queries                dim = (words_memory_length, query_dim)\n",
    "           returns : \n",
    "                the resulting decision vector                   dim = (1, 1, query_dim)\n",
    "                the weights of first attention layer (dict)     \n",
    "                the weights of second attention layer (dict)\n",
    "        '''\n",
    "        attn1_weights_list = []\n",
    "        attn2_weights_list = []\n",
    "        if len(words_memory) > 0 :\n",
    "            if query is not None :\n",
    "                query = query.repeat(1, self.n_heads, 1)\n",
    "            elif self.hops > 1 :\n",
    "                query = self.initQuery()\n",
    "            \n",
    "            for hop in range(self.hops) :\n",
    "                decision_vector, attn1_weights, attn2_weights = self.singlePass(words_memory, \n",
    "                                                                                query, \n",
    "                                                                                self.attn1[hop], \n",
    "                                                                                self.attn2[hop])\n",
    "                attn1_weights_list.append(attn1_weights)\n",
    "                attn2_weights_list.append(attn2_weights)\n",
    "                \n",
    "                query = self.update(query, decision_vector)  # size (L, self.n_heads, self.output_dim)\n",
    "                query = self.dropout(query)\n",
    "\n",
    "        # output decision vector\n",
    "        return query, attn1_weights_list, attn2_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeurs\"></a>\n",
    "\n",
    "\n",
    "# 2.4 Modules de décodage\n",
    "\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeursSelectifs\"></a>\n",
    "\n",
    "\n",
    "### 2.4.1 Décodeurs sélectifs\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Multilabel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Multilabel.py\n",
    "\n",
    "# Modins\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultilabelDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_dim, n_labels) :\n",
    "        super(MultilabelDecoder, self).__init__() \n",
    "        self.version = 'multilabel_1'\n",
    "        self.n_labels = n_labels\n",
    "        self.classes_decoder = nn.Linear(text_dim, n_labels)\n",
    "\n",
    "    def forward(self, text_vector, train_mode = False):\n",
    "        classes_vector = self.classes_decoder(text_vector).view(-1)\n",
    "        if train_mode :\n",
    "            return classes_vector\n",
    "        else :\n",
    "            classes = F.sigmoid(classes_vector)\n",
    "            result = []\n",
    "            for el in classes.data :\n",
    "                a = 1 if el > 0.5 else 0\n",
    "                result.append(a)\n",
    "            return result  \n",
    "        \n",
    "        \n",
    "\n",
    "class MultilabelDecoderV2(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_dim, n_labels) :\n",
    "        super(MultilabelDecoderV2, self).__init__() \n",
    "        self.version = 'multilabel_2'\n",
    "        self.n_labels = n_labels\n",
    "        self.classes_decoder = nn.ModuleList([nn.Linear(text_dim, 1) for i in range(n_labels)]) \n",
    "\n",
    "    def forward(self, text_vector, train_mode = False):\n",
    "        classes_vector = Variable(torch.zeros(self.n_labels))\n",
    "        for i, dec in enumerate(self.classes_decoder) :\n",
    "            classes_vector[i] = dec(text_vector[:, i, :].unsqueeze(1)).view(-1)\n",
    "        if train_mode :\n",
    "            return classes_vector\n",
    "        else :\n",
    "            classes = F.sigmoid(classes_vector)\n",
    "            result = []\n",
    "            for el in classes.data :\n",
    "                a = 1 if el > 0.5 else 0\n",
    "                result.append(a)\n",
    "            return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Class.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Class.py\n",
    "\n",
    "# Modins\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ClassDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_dim, n_classes) :\n",
    "        super(ClassDecoder, self).__init__() \n",
    "        self.version = 'class'\n",
    "        self.n_classes = n_classes\n",
    "        self.classes_decoder = nn.Linear(text_dim, n_classes)\n",
    "\n",
    "    def forward(self, text_vector, train_mode = False):\n",
    "        classes_vector = self.classes_decoder(text_vector).view(-1)\n",
    "        if train_mode :\n",
    "            return classes_vector\n",
    "        else :\n",
    "            probas = F.softmax(classes_vector) \n",
    "            topv, topi = probas.data.topk(1)\n",
    "            result = topi[0][0].cpu().numpy()\n",
    "            return result, probas\n",
    "        \n",
    "        \n",
    "        \n",
    "class MultiTaskClassDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_dim, n_class_list, weight_list = None) :\n",
    "        super(MultiTaskClassDecoder, self).__init__() \n",
    "        self.version = 'class'\n",
    "        self.decoder_list = nn.ModuleList([ClassDecoder(text_dim, N) for N in n_class_list])\n",
    "        self.weight_list = weight_list if weight_list is not None else [1 for i in range(len(n_class_list))]\n",
    "\n",
    "    def forward(self, text_vector, train_mode = False):\n",
    "        classes_vector_list = [mod(text_vector, train_mode) for mod in self.decoder_list]\n",
    "        return classes_vector_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decodeursGeneratifs\"></a>\n",
    "\n",
    "\n",
    "### 2.4.2 Décodeurs génératifs\n",
    "\n",
    "[Retour à la table des matières](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modules/Decoder_Words.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modules/Decoder_Words.py\n",
    "\n",
    "# Modins\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class WordsDecoder(nn.Module):\n",
    "    '''Transforms a vector into a sequence of words'''\n",
    "    def __init__(self, text_dim, \n",
    "                 output_dim, \n",
    "                 lang, \n",
    "                 pre_entrainement = None, \n",
    "                 freeze = False,\n",
    "                 teacher_forcing_ratio = 0.5,\n",
    "                 dropout = 0.1):\n",
    "        \n",
    "        super(WordsDecoder, self).__init__()\n",
    "        \n",
    "        # relevant quantities\n",
    "        self.lang = lang\n",
    "        self.text_dim = text_dim\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        # modules\n",
    "        self.embedding = nn.Embedding(lang.n_words, output_dim)\n",
    "        if pre_entrainement is not None : \n",
    "            self.embedding.load_state_dict({'weight': torch.Tensor(pre_entrainement)})\n",
    "        if freeze :\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.gru = nn.GRU(output_dim + text_dim, text_dim)\n",
    "        self.out = nn.Linear(text_dim, lang.n_words)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def indexesFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a given sentence into a list of indices according to a given language'''\n",
    "        indexes=[]\n",
    "        unknowns = 0\n",
    "        for word in sentence:\n",
    "            p = random.random()\n",
    "            if word not in self.lang.word2index.keys() :\n",
    "                pass\n",
    "                #indexes.append(UNK_token) #pass\n",
    "            elif p >= rand :\n",
    "                indexes.append(self.lang.word2index[word])\n",
    "            elif p < rand :\n",
    "                e = random.choice([1, 2, 3])\n",
    "                if e == 1 :  # doesn't put any word\n",
    "                    pass\n",
    "                elif e == 2 :# hide word with UNK_Token\n",
    "                    indexes.append(UNK_token)\n",
    "                else :       # put word followed with randomly selected other word\n",
    "                    indexes.append(self.lang.word2index[word])\n",
    "                    indexes.append(random.choice(list(range(self.lang.n_words))))\n",
    "        indexes.append(self.lang.word2index['EOS'])\n",
    "\n",
    "        # remove exceeding words, exept first word and the two last words or symbols\n",
    "        if max_length is not None :\n",
    "            print(max_length)\n",
    "            while len(indexes) > max_length:\n",
    "                indexes.pop(random.randint(1,len(indexes)-2))\n",
    "        return indexes\n",
    "\n",
    "\n",
    "    def variableFromSentence(self, sentence, max_length = None, rand = 0):\n",
    "        '''Turn a sentence into a torch variable, containing a list of indices according\n",
    "           to a given language.\n",
    "        '''\n",
    "        indexes = self.indexesFromSentence(sentence, max_length, rand)                                 \n",
    "        result = Variable(torch.LongTensor(indexes).view(-1, 1)) if len(indexes) != 0 else None\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def generateWord(self, text_vector, current_word_index, decoder_hidden_vector):\n",
    "        '''takes parameters : \n",
    "        \n",
    "                the index of last decoded word as input                  \n",
    "                the query vector                                dim = (1, 1, decision_dim)\n",
    "                the last hidden decoder state                   dim = (1, 1, decision_dim)\n",
    "        \n",
    "           returns : \n",
    "        \n",
    "                the index of next decoded word\n",
    "                the updated hidden decoder state                 dim = (1, 1, decision_dim)\n",
    "        '''\n",
    "        current_word = self.embedding(current_word_index)\n",
    "        embedded = torch.cat((current_word, text_vector), dim = 2)\n",
    "        #embedded = self.dropout(embedded)\n",
    "        for i in range(self.n_layers):\n",
    "            output, decoder_hidden_vector = self.gru(embedded, decoder_hidden_vector)\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, decoder_hidden_vector\n",
    "    \n",
    "    \n",
    "    def forward(self, text_vector, target_answer = None) :\n",
    "        \"\"\"Génère une réponse à partir d'un état caché initialisant le décodeur,\n",
    "        en utilisant une réponse cible pour un mode 'teacher forcing-like' si celle-ci est fournie \"\"\"\n",
    "        bound = 50\n",
    "        decoder_outputs = []\n",
    "        answer = []\n",
    "        di = 0\n",
    "        SOS_token = self.lang.word2index['SOS']\n",
    "        EOS_token = self.lang.word2index['EOS']\n",
    "        if target_answer is not None :\n",
    "            target_answer = target_answer[0]\n",
    "            target_answer = self.variableFromSentence(target_answer)\n",
    "        target_answer = target_answer if random.random() < self.teacher_forcing_ratio else None\n",
    "        current_word_index = Variable(torch.LongTensor([[SOS_token]]))\n",
    "        decoder_hidden_vector = text_vector\n",
    "        decoder_hidden_vector = self.dropout(decoder_hidden_vector)\n",
    "        for di in range(bound) :\n",
    "            output, decoder_hidden_vector = self.generateWord(text_vector, \n",
    "                                                              current_word_index, \n",
    "                                                              decoder_hidden_vector)\n",
    "            topv, topi = output.data.topk(1)\n",
    "            decoder_outputs.append(output)\n",
    "            ni = topi[0][0] # index of current generated word\n",
    "            answer.append(ni)\n",
    "            if ni == EOS_token :\n",
    "                break\n",
    "            elif target_answer is not None : # Teacher forcing\n",
    "                if di < target_answer.size()[0] :\n",
    "                    current_word_index = target_answer[di].view(1,-1)  \n",
    "                else :\n",
    "                    break\n",
    "            else :\n",
    "                current_word_index = Variable(torch.LongTensor([[ni]]))\n",
    "                \n",
    "        # transform 'answer' into a sentence with self.lang\n",
    " \n",
    "        return answer, decoder_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retour dans le répertoire courant du tableau de bord :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jb\\Desktop\\Scripts\\notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Retour à la table des matières](#plan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
