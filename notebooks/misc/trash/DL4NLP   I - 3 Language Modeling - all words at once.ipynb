{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=orange>I - 3 </font>\n",
    "  Language Modeling\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. <font color=orange>**Language Modeling**</font>\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. Sequence to sequence\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "1. Abstractive Summarization\n",
    "\n",
    "2. Question Answering\n",
    "\n",
    "3. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) |\n",
    "\n",
    "# Overview\n",
    "\n",
    "\n",
    "Exemples d'implémentation en PyTorch :\n",
    "\n",
    "- https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "\n",
    "\n",
    "Différentes architectures sont décrites dans la litérature :\n",
    "\n",
    "- Regularizing and Optimizing LSTM Language Models - https://arxiv.org/pdf/1708.02182.pdf\n",
    "\n",
    "Un modèle linguistique est intérressant en soi, mais peut aussi servir pour le pré-entrainement de couches basses d'un modèle plus complexe :\n",
    "\n",
    "- Deep contextualized word representations - https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Improving Language Understanding by Generative Pre-Training - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "- Language Models are Unsupervised Multitask Learners - https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.5.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import gc\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DL4NLP = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_DL4NLP + '\\\\lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une liste de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "df_GMB_extract = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050794, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx           word  pos\n",
       "0           1.0      Thousands  NNS\n",
       "1           1.0             of   IN\n",
       "2           1.0  demonstrators  NNS\n",
       "3           1.0           have  VBP\n",
       "4           1.0        marched  VBN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract.dropna(inplace = True)\n",
    "df_GMB_extract = df_GMB_extract[['sentence_idx', 'word', 'pos']]\n",
    "print(df_GMB_extract.shape)\n",
    "df_GMB_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus with words lowered and stripped\n",
    "corpus = df_GMB_extract.groupby(\"sentence_idx\").apply(lambda s: [w.lower().strip() for w in s[\"word\"].values.tolist()]).tolist()\n",
    "corpus = [[w for w in s if w != ''] for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "We consider here a FastText model trained following the Skip-Gram training objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I3_skipgram_gensim.model\")\n",
    "sg_gensim = Word2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27419\n"
     ]
    }
   ],
   "source": [
    "lang = Lang(corpus, min_count = 1)\n",
    "print(lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_gensim = Word2Vec(corpus, \n",
    "                     size = 100, \n",
    "                     window = 5, \n",
    "                     min_count = 1, \n",
    "                     negative = 20, \n",
    "                     iter = 100,\n",
    "                     sg = 1,\n",
    "                     workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I3_skipgram_gensim.model\")\n",
    "#sg_gensim.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2VecConnector(sg_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ordered vocab is the same for both the original and its wrapped objects\n",
    "# except the two last words 'PADDING_WORD' and 'UNK' added to the wrapped object\n",
    "list(word2vec.word2vec.wv.index2word) == list(word2vec.twin.lang.word2index)[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "The contextualization layer transforms a sequences of word vectors into another one, of same length, where each output vector corresponds to a new version of each input vector that is contextualized with respect to neighboring vectors.\n",
    "\n",
    "\n",
    "This module consists of a bi-directional _Gated Recurrent Unit_ (GRU) that supports packed sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.models import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layer = 1, \n",
    "                 dropout = 0, \n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(\n",
    "            emb_dim = self.word2vec.out_dim, \n",
    "            hid_dim = hidden_dim, \n",
    "            n_layer = n_layer, \n",
    "            dropout = dropout, \n",
    "            bidirectional = False)\n",
    "        self.out = nn.Linear(self.context.out_dim, self.word2vec.lang.n_words)\n",
    "        self.act = F.softmax\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.criterion = nn.NLLLoss(\n",
    "            reduction    = 'mean',\n",
    "            ignore_index = self.ignore_index,\n",
    "            weight       = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def forward(self, \n",
    "                sentence = '.', \n",
    "                hidden = None, \n",
    "                limit = 10, \n",
    "                color_code = '\\033[94m'):\n",
    "        # init variables\n",
    "        words  = self.tokenizer(sentence)\n",
    "        result = words + [color_code]\n",
    "        hidden, count, stop = None, 0, False\n",
    "        while not stop :\n",
    "            # compute probs\n",
    "            embeddings = self.word2vec(words, self.device)\n",
    "            _, hidden  = self.context(embeddings, lengths = None, hidden = hidden) # size (n_layers, batch_size, hid_dim)\n",
    "            probs      = self.act(self.out(hidden[-1]), dim = 1).view(-1)\n",
    "            # get predicted word\n",
    "            topv, topi = probs.data.topk(1)\n",
    "            words = [self.word2vec.lang.index2word[topi.item()]]\n",
    "            result += words\n",
    "            # stopping criterion\n",
    "            count += 1\n",
    "            if count == limit or words == [limit] or count == 50 : stop = True\n",
    "        print(' '.join(result + ['\\033[0m']))\n",
    "        return \n",
    "    \n",
    "    def generatePackedSentences(self, sentences, batch_size = 32, max_length = 15) :\n",
    "        sentences = [s[i: i+max_length] for s in sentences for i in range(0, len(s)- max_length)]\n",
    "        sentences = [s for s in sentences if len(s) > 1]\n",
    "        sentences.sort(key = lambda s: len(s), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack0 = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in s] for s in pack0]\n",
    "            pack0 = [[w for w in words if w is not None] for words in pack0]\n",
    "            pack0.sort(key = len, reverse = True)\n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1)) # size (batch_size, max_length)\n",
    "            pack1 = pack0[:, 1:]                                      # size (batch_size, max_length-1) \n",
    "            pack0 = pack0[:,:-1]                                      # size (batch_size, max_length-1) \n",
    "            lengths = torch.tensor([len(p) for p in pack0])           # size (batch_size) \n",
    " \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "    \n",
    "    def fit(self, batches, iters = None, epochs = None, lr = 0.025, min_background_length = 3, random_state = 42,\n",
    "              print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            lengths    = batch[1].to(self.device)\n",
    "            hiddens, _ = self.context(embeddings, lengths = lengths) # size (batch_size, max_length-1, hid_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)   # size (batch_size, max_length-1, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            mask1 = (targets.data != self.ignore_index).int() #~ flips all bools to opposite value\n",
    "            mask2 = (targets.data == log_probs.data.topk(1, dim = 1)[1].squeeze(1)).int()\n",
    "            good  = torch.sum(mask1 * mask2).item()\n",
    "            alls  = torch.sum(mask1).item()\n",
    "            return good, alls\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return\n",
    "\n",
    "        def trainLoop(batch, min_background_length, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            if min_background_length < batch[1].size(-1) :\n",
    "                log_probs = computeLogProbs(batch[0]).transpose(1, 2) # size (batch_size, lang_size, max_length-1)\n",
    "                log_probs = log_probs[:, :, min_background_length:]   # size (batch_size, lang_size, max_length-min_background_length-1)\n",
    "                targets   = batch[1].to(self.device)                  # size (batch_size, max_length-1)\n",
    "                targets   = targets[:, min_background_length:]        # size (batch_size, max_length-min_background_length-1)\n",
    "                loss      = self.criterion(log_probs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                good, alls = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "                return loss.item(), good, alls\n",
    "            else :\n",
    "                return 0, 0, 0\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_good = 0\n",
    "        tot_alls = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, good, alls = trainLoop(batch, min_background_length, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_good += good \n",
    "                tot_alls += alls\n",
    "                if iter % print_every == 0 : \n",
    "                    tot_acc = tot_good * 100 / tot_alls\n",
    "                    printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "                    tot_loss = 0  \n",
    "                    tot_good = 0\n",
    "                    tot_alls = 0\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, good, alls = trainLoop(batch, min_background_length, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_good += good \n",
    "                    tot_alls += alls\n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_acc = tot_good * 100 / tot_alls\n",
    "                        printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "                        tot_loss = 0  \n",
    "                        tot_good = 0\n",
    "                        tot_alls = 0\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4525620"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model = LanguageModel(device,\n",
    "                               tokenizer = lambda s : s.split(' '),\n",
    "                               word2vec = word2vec,\n",
    "                               hidden_dim = 150, \n",
    "                               n_layer = 3, \n",
    "                               dropout = 0.1,\n",
    "                               optimizer = optim.AdamW)\n",
    "\n",
    "language_model.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270059"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = language_model.generatePackedSentences(corpus, batch_size = 2, max_length = 15)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.7750e-02, -1.5767e-02,  1.2777e-01,  8.3767e-02, -1.2256e-01,\n",
      "         -6.6195e-02,  5.0019e-02,  1.3278e-02,  5.4525e-02, -9.6136e-02,\n",
      "         -1.0278e-01, -4.0381e-02, -1.2332e-01, -6.7921e-02, -4.0722e-02,\n",
      "          2.5816e-02,  1.0844e-01,  3.6596e-02, -8.1626e-02,  1.5819e-01,\n",
      "          6.6519e-02,  5.6303e-02,  5.9060e-02,  1.2683e-01, -1.9040e-02,\n",
      "          5.5987e-02,  0.0000e+00,  0.0000e+00, -2.0169e-01, -1.0295e-01,\n",
      "          2.4276e-02, -7.3571e-02, -6.2827e-02, -3.9476e-02, -1.0112e-03,\n",
      "         -1.1361e-01, -7.2010e-02,  3.2149e-02,  1.1524e-01,  6.5980e-02,\n",
      "         -7.8823e-02,  4.9361e-02, -1.6162e-02,  0.0000e+00, -5.8017e-02,\n",
      "          4.6166e-02,  1.2452e-01,  4.0334e-02, -1.0646e-01, -2.2557e-02,\n",
      "         -5.3657e-02, -1.7797e-02,  1.5746e-01,  4.3862e-02, -3.3072e-02,\n",
      "          6.2544e-02,  1.9251e-01,  0.0000e+00,  1.9553e-01,  1.2036e-01,\n",
      "          1.0195e-01,  8.6168e-02,  0.0000e+00,  0.0000e+00,  1.2080e-02,\n",
      "          1.8458e-02, -1.3271e-01,  1.1099e-01, -1.3148e-01, -5.1676e-02,\n",
      "         -7.5678e-02,  7.4100e-02, -3.3878e-03, -1.3861e-01, -3.3791e-02,\n",
      "          1.6894e-01, -6.1257e-02, -3.8985e-02,  0.0000e+00,  1.6182e-02,\n",
      "          0.0000e+00,  1.7128e-01,  2.5849e-02,  4.2135e-03,  0.0000e+00,\n",
      "          8.6407e-02,  3.8622e-02, -2.4880e-02,  0.0000e+00, -6.3103e-02,\n",
      "         -5.2698e-02,  1.1657e-01,  5.3730e-02,  0.0000e+00, -1.8595e-03,\n",
      "         -1.3443e-01,  2.0679e-01, -5.5692e-02, -4.0457e-02, -5.3389e-02,\n",
      "         -1.5069e-01, -2.7656e-02,  6.0307e-03,  4.2286e-02, -3.7292e-02,\n",
      "          2.4237e-02,  1.2579e-01, -5.2267e-02,  8.2672e-02, -1.7751e-02,\n",
      "         -9.7136e-02,  3.7441e-02, -4.8756e-02,  6.0061e-02,  2.9453e-02,\n",
      "         -6.5206e-02,  4.7949e-02, -1.5246e-02, -7.2525e-02,  0.0000e+00,\n",
      "         -1.4649e-02, -2.7849e-02, -1.2464e-01,  8.6001e-02, -3.3798e-02,\n",
      "          1.7599e-01,  1.3711e-02, -5.8987e-03,  1.0176e-01,  9.2144e-02,\n",
      "         -1.2304e-01,  1.2088e-01, -1.6624e-02, -1.6514e-01, -1.3768e-02,\n",
      "          0.0000e+00, -3.7903e-02, -7.5840e-02,  5.5403e-03,  0.0000e+00,\n",
      "          0.0000e+00, -6.1164e-02, -1.4649e-01, -1.1839e-02,  4.1160e-03,\n",
      "          6.9696e-02,  8.8967e-02, -7.3171e-02,  1.6621e-01,  0.0000e+00],\n",
      "        [ 7.0350e-02, -4.0260e-02,  1.2021e-01,  1.0836e-02, -1.3287e-01,\n",
      "         -3.5704e-02,  1.6001e-02,  0.0000e+00,  3.9710e-02, -9.5886e-02,\n",
      "         -1.3677e-01,  0.0000e+00, -1.0034e-01, -8.1436e-02, -1.0109e-03,\n",
      "         -2.3061e-02,  7.2428e-02,  4.9172e-02, -4.6002e-02,  1.1224e-01,\n",
      "          8.5108e-02,  8.3343e-02,  6.6494e-02,  1.2032e-01, -1.0235e-03,\n",
      "          3.2395e-02,  1.1540e-01,  5.4391e-02, -2.3110e-01, -1.1847e-01,\n",
      "          1.2710e-02, -8.3413e-02, -4.0339e-02,  1.2826e-02, -4.7379e-02,\n",
      "         -1.2343e-01, -1.0803e-01,  5.1519e-02,  7.9840e-02,  0.0000e+00,\n",
      "         -6.9229e-02,  0.0000e+00,  0.0000e+00, -1.9223e-02, -4.4751e-02,\n",
      "          5.3128e-02,  1.7821e-01,  4.8514e-02, -1.0708e-01, -3.1402e-02,\n",
      "          0.0000e+00, -7.6825e-04,  1.2825e-01,  2.6551e-02,  2.6518e-03,\n",
      "          7.5825e-02,  2.3759e-01,  3.7728e-04,  1.7799e-01,  1.1586e-01,\n",
      "          1.1423e-01,  1.9186e-02, -1.3069e-01, -3.7144e-03,  0.0000e+00,\n",
      "          3.8942e-03, -1.6313e-01,  0.0000e+00, -1.6662e-01, -7.2154e-02,\n",
      "         -7.8835e-02,  1.1312e-01,  6.8710e-03, -1.1759e-01, -1.3583e-02,\n",
      "          0.0000e+00, -5.6366e-02, -3.2218e-02,  1.4146e-02,  4.9773e-02,\n",
      "          4.4380e-02,  1.3021e-01, -2.6991e-02,  4.4311e-02, -1.3366e-01,\n",
      "          9.0751e-02,  0.0000e+00,  8.6577e-03, -1.6171e-01, -4.7729e-02,\n",
      "          0.0000e+00,  7.5358e-02,  4.7750e-02,  4.3683e-04,  1.6471e-03,\n",
      "         -1.2699e-01,  1.9275e-01, -6.7167e-03, -4.1082e-02, -6.0451e-02,\n",
      "         -1.5421e-01,  8.4462e-03,  1.4152e-03,  4.9569e-02, -2.7787e-02,\n",
      "          0.0000e+00,  0.0000e+00, -8.6600e-02,  1.3232e-01, -8.9821e-03,\n",
      "         -7.3022e-02,  4.8936e-02, -3.3570e-02,  3.7473e-02,  0.0000e+00,\n",
      "          0.0000e+00,  5.3198e-02,  1.8686e-02, -3.7247e-02, -1.3032e-01,\n",
      "         -2.8337e-02, -3.9082e-02, -1.1924e-01,  4.7338e-02, -6.8633e-03,\n",
      "          1.7674e-01,  3.0197e-02, -1.1032e-04,  1.0454e-01,  1.2424e-01,\n",
      "          0.0000e+00,  1.6818e-01, -1.7467e-02,  0.0000e+00, -4.9721e-02,\n",
      "          2.3562e-02, -3.0212e-02, -9.9894e-02,  0.0000e+00,  5.5130e-02,\n",
      "          2.1858e-02,  0.0000e+00, -1.4668e-01,  9.2646e-04, -1.9082e-02,\n",
      "          9.7087e-02,  6.1139e-02, -8.7825e-02,  1.7817e-01,  9.0819e-02]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[ 7.7750e-02, -1.5767e-02,  1.2777e-01,  8.3767e-02, -1.2256e-01,\n",
      "          0.0000e+00,  5.0019e-02,  1.3278e-02,  5.4525e-02, -9.6136e-02,\n",
      "         -1.0278e-01,  0.0000e+00, -1.2332e-01, -6.7921e-02, -4.0722e-02,\n",
      "          2.5816e-02,  1.0844e-01,  0.0000e+00,  0.0000e+00,  1.5819e-01,\n",
      "          0.0000e+00,  5.6303e-02,  5.9060e-02,  1.2683e-01, -1.9040e-02,\n",
      "          5.5987e-02,  1.2184e-01,  2.3018e-02,  0.0000e+00, -1.0295e-01,\n",
      "          2.4276e-02, -7.3571e-02, -6.2827e-02, -3.9476e-02, -1.0112e-03,\n",
      "         -1.1361e-01, -7.2010e-02,  3.2149e-02,  1.1524e-01,  6.5980e-02,\n",
      "         -7.8823e-02,  0.0000e+00, -1.6162e-02,  2.8803e-03, -5.8017e-02,\n",
      "          4.6166e-02,  1.2452e-01,  4.0334e-02, -1.0646e-01, -2.2557e-02,\n",
      "         -5.3657e-02, -1.7797e-02,  1.5746e-01,  4.3862e-02, -3.3072e-02,\n",
      "          6.2544e-02,  1.9251e-01, -7.5472e-04,  1.9553e-01,  1.2036e-01,\n",
      "          0.0000e+00,  8.6168e-02, -8.0121e-02,  2.7205e-03,  1.2080e-02,\n",
      "          1.8458e-02, -1.3271e-01,  1.1099e-01, -1.3148e-01, -5.1676e-02,\n",
      "         -7.5678e-02,  7.4100e-02, -3.3878e-03, -1.3861e-01, -3.3791e-02,\n",
      "          1.6894e-01, -6.1257e-02, -3.8985e-02, -2.5579e-02,  0.0000e+00,\n",
      "          9.2553e-02,  1.7128e-01,  2.5849e-02,  4.2135e-03, -1.3898e-01,\n",
      "          8.6407e-02,  0.0000e+00, -2.4880e-02, -1.4933e-01, -6.3103e-02,\n",
      "         -5.2698e-02,  1.1657e-01,  5.3730e-02,  2.6895e-02, -1.8595e-03,\n",
      "         -1.3443e-01,  2.0679e-01, -5.5692e-02, -4.0457e-02, -5.3389e-02,\n",
      "         -1.5069e-01, -2.7656e-02,  6.0307e-03,  4.2286e-02, -3.7292e-02,\n",
      "          2.4237e-02,  1.2579e-01, -5.2267e-02,  8.2672e-02, -1.7751e-02,\n",
      "         -9.7136e-02,  0.0000e+00, -4.8756e-02,  6.0061e-02,  0.0000e+00,\n",
      "         -6.5206e-02,  0.0000e+00, -1.5246e-02, -7.2525e-02, -1.3232e-01,\n",
      "         -1.4649e-02, -2.7849e-02, -1.2464e-01,  8.6001e-02, -3.3798e-02,\n",
      "          1.7599e-01,  1.3711e-02, -5.8987e-03,  1.0176e-01,  9.2144e-02,\n",
      "         -1.2304e-01,  1.2088e-01,  0.0000e+00, -1.6514e-01, -1.3768e-02,\n",
      "          2.7132e-02,  0.0000e+00, -7.5840e-02,  5.5403e-03,  5.5579e-02,\n",
      "          7.4394e-03, -6.1164e-02, -1.4649e-01, -1.1839e-02,  4.1160e-03,\n",
      "          6.9696e-02,  8.8967e-02, -7.3171e-02,  1.6621e-01,  8.0521e-02],\n",
      "        [ 7.0350e-02, -4.0260e-02,  1.2021e-01,  1.0836e-02, -1.3287e-01,\n",
      "         -3.5704e-02,  1.6001e-02,  2.5709e-02,  3.9710e-02, -9.5886e-02,\n",
      "         -1.3677e-01,  0.0000e+00, -1.0034e-01, -8.1436e-02, -1.0109e-03,\n",
      "         -2.3061e-02,  7.2428e-02,  4.9172e-02, -4.6002e-02,  1.1224e-01,\n",
      "          8.5108e-02,  8.3343e-02,  6.6494e-02,  1.2032e-01, -1.0235e-03,\n",
      "          3.2395e-02,  1.1540e-01,  5.4391e-02, -2.3110e-01, -1.1847e-01,\n",
      "          1.2710e-02, -8.3413e-02, -4.0339e-02,  1.2826e-02, -4.7379e-02,\n",
      "         -1.2343e-01, -1.0803e-01,  5.1519e-02,  7.9840e-02,  6.8924e-02,\n",
      "         -6.9229e-02,  4.4065e-02,  0.0000e+00, -1.9223e-02, -4.4751e-02,\n",
      "          5.3128e-02,  1.7821e-01,  4.8514e-02, -1.0708e-01, -3.1402e-02,\n",
      "         -5.5761e-02, -7.6825e-04,  1.2825e-01,  2.6551e-02,  2.6518e-03,\n",
      "          7.5825e-02,  2.3759e-01,  3.7728e-04,  1.7799e-01,  0.0000e+00,\n",
      "          1.1423e-01,  1.9186e-02, -1.3069e-01, -3.7144e-03,  0.0000e+00,\n",
      "          3.8942e-03, -1.6313e-01,  1.0952e-01, -1.6662e-01, -7.2154e-02,\n",
      "          0.0000e+00,  1.1312e-01,  6.8710e-03, -1.1759e-01, -1.3583e-02,\n",
      "          0.0000e+00, -5.6366e-02, -3.2218e-02,  1.4146e-02,  4.9773e-02,\n",
      "          4.4380e-02,  1.3021e-01, -2.6991e-02,  4.4311e-02, -1.3366e-01,\n",
      "          9.0751e-02,  8.4480e-02,  0.0000e+00, -1.6171e-01, -4.7729e-02,\n",
      "         -5.0041e-02,  7.5358e-02,  4.7750e-02,  4.3683e-04,  1.6471e-03,\n",
      "         -1.2699e-01,  1.9275e-01, -6.7167e-03, -4.1082e-02, -6.0451e-02,\n",
      "         -1.5421e-01,  0.0000e+00,  0.0000e+00,  4.9569e-02, -2.7787e-02,\n",
      "          7.1555e-02,  1.2104e-01,  0.0000e+00,  1.3232e-01,  0.0000e+00,\n",
      "         -7.3022e-02,  4.8936e-02, -3.3570e-02,  3.7473e-02,  4.6180e-02,\n",
      "         -5.5547e-02,  5.3198e-02,  1.8686e-02, -3.7247e-02, -1.3032e-01,\n",
      "         -2.8337e-02, -3.9082e-02, -1.1924e-01,  4.7338e-02, -6.8633e-03,\n",
      "          1.7674e-01,  3.0197e-02, -1.1032e-04,  1.0454e-01,  1.2424e-01,\n",
      "         -1.4467e-01,  1.6818e-01, -1.7467e-02, -1.6499e-01, -4.9721e-02,\n",
      "          2.3562e-02, -3.0212e-02,  0.0000e+00,  9.2531e-03,  5.5130e-02,\n",
      "          2.1858e-02, -4.6262e-02, -1.4668e-01,  9.2646e-04, -1.9082e-02,\n",
      "          9.7087e-02,  6.1139e-02, -8.7825e-02,  1.7817e-01,  9.0819e-02]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches, iters = 1, lr = 0.001, min_background_length = 12, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 21s (- 56m 46s) (250 0%) loss : 4.910  accuracy : 21.1 %\n",
      "0m 42s (- 56m 16s) (500 1%) loss : 4.948  accuracy : 20.9 %\n",
      "1m 4s (- 56m 28s) (750 1%) loss : 4.947  accuracy : 21.5 %\n",
      "1m 26s (- 55m 59s) (1000 2%) loss : 5.021  accuracy : 20.9 %\n",
      "1m 47s (- 55m 33s) (1250 3%) loss : 4.884  accuracy : 21.8 %\n",
      "2m 8s (- 55m 10s) (1500 3%) loss : 4.918  accuracy : 21.4 %\n",
      "2m 30s (- 54m 44s) (1750 4%) loss : 4.994  accuracy : 21.5 %\n",
      "2m 51s (- 54m 19s) (2000 5%) loss : 4.999  accuracy : 21.1 %\n",
      "3m 13s (- 53m 58s) (2250 5%) loss : 5.029  accuracy : 21.8 %\n",
      "3m 34s (- 53m 34s) (2500 6%) loss : 5.013  accuracy : 21.3 %\n",
      "3m 55s (- 53m 11s) (2750 6%) loss : 5.011  accuracy : 21.7 %\n",
      "4m 16s (- 52m 48s) (3000 7%) loss : 4.989  accuracy : 21.8 %\n",
      "4m 38s (- 52m 25s) (3250 8%) loss : 5.005  accuracy : 21.5 %\n",
      "4m 59s (- 52m 5s) (3500 8%) loss : 4.973  accuracy : 22.0 %\n",
      "5m 20s (- 51m 42s) (3750 9%) loss : 4.899  accuracy : 21.7 %\n",
      "5m 42s (- 51m 21s) (4000 10%) loss : 4.966  accuracy : 21.7 %\n",
      "6m 3s (- 50m 59s) (4250 10%) loss : 4.973  accuracy : 22.2 %\n",
      "6m 25s (- 50m 37s) (4500 11%) loss : 4.981  accuracy : 21.9 %\n",
      "6m 46s (- 50m 15s) (4750 11%) loss : 5.043  accuracy : 21.4 %\n",
      "7m 7s (- 49m 53s) (5000 12%) loss : 4.971  accuracy : 22.0 %\n",
      "7m 29s (- 49m 31s) (5250 13%) loss : 5.060  accuracy : 21.4 %\n",
      "7m 50s (- 49m 10s) (5500 13%) loss : 4.984  accuracy : 21.6 %\n",
      "8m 11s (- 48m 48s) (5750 14%) loss : 4.967  accuracy : 21.7 %\n",
      "8m 32s (- 48m 26s) (6000 15%) loss : 5.007  accuracy : 21.1 %\n",
      "8m 54s (- 48m 4s) (6250 15%) loss : 5.027  accuracy : 21.2 %\n",
      "9m 15s (- 47m 44s) (6500 16%) loss : 4.965  accuracy : 21.7 %\n",
      "9m 37s (- 47m 22s) (6750 16%) loss : 5.000  accuracy : 21.5 %\n",
      "9m 58s (- 47m 1s) (7000 17%) loss : 4.896  accuracy : 22.2 %\n",
      "10m 19s (- 46m 40s) (7250 18%) loss : 4.981  accuracy : 21.9 %\n",
      "10m 41s (- 46m 18s) (7500 18%) loss : 5.013  accuracy : 21.2 %\n",
      "11m 2s (- 45m 56s) (7750 19%) loss : 4.906  accuracy : 22.3 %\n",
      "11m 23s (- 45m 35s) (8000 20%) loss : 4.901  accuracy : 22.5 %\n",
      "11m 45s (- 45m 14s) (8250 20%) loss : 4.921  accuracy : 21.9 %\n",
      "12m 7s (- 44m 54s) (8500 21%) loss : 4.916  accuracy : 22.1 %\n",
      "12m 28s (- 44m 32s) (8750 21%) loss : 4.953  accuracy : 21.9 %\n",
      "12m 49s (- 44m 11s) (9000 22%) loss : 5.005  accuracy : 21.8 %\n",
      "13m 11s (- 43m 50s) (9250 23%) loss : 4.930  accuracy : 21.6 %\n",
      "13m 32s (- 43m 28s) (9500 23%) loss : 4.938  accuracy : 22.3 %\n",
      "13m 53s (- 43m 7s) (9750 24%) loss : 4.950  accuracy : 22.1 %\n",
      "14m 15s (- 42m 46s) (10000 25%) loss : 4.964  accuracy : 21.6 %\n",
      "14m 36s (- 42m 24s) (10250 25%) loss : 4.911  accuracy : 22.4 %\n",
      "14m 58s (- 42m 3s) (10500 26%) loss : 4.888  accuracy : 21.9 %\n",
      "15m 19s (- 41m 41s) (10750 26%) loss : 4.993  accuracy : 21.7 %\n",
      "15m 40s (- 41m 19s) (11000 27%) loss : 4.858  accuracy : 21.9 %\n",
      "16m 2s (- 40m 58s) (11250 28%) loss : 4.961  accuracy : 22.5 %\n",
      "16m 23s (- 40m 36s) (11500 28%) loss : 4.896  accuracy : 21.9 %\n",
      "16m 44s (- 40m 15s) (11750 29%) loss : 4.981  accuracy : 22.1 %\n",
      "17m 6s (- 39m 54s) (12000 30%) loss : 4.852  accuracy : 22.6 %\n",
      "17m 27s (- 39m 32s) (12250 30%) loss : 5.029  accuracy : 22.1 %\n",
      "17m 48s (- 39m 11s) (12500 31%) loss : 4.967  accuracy : 22.0 %\n",
      "18m 10s (- 38m 50s) (12750 31%) loss : 4.901  accuracy : 22.4 %\n",
      "18m 31s (- 38m 28s) (13000 32%) loss : 4.829  accuracy : 22.5 %\n",
      "18m 52s (- 38m 6s) (13250 33%) loss : 4.963  accuracy : 20.9 %\n",
      "19m 14s (- 37m 45s) (13500 33%) loss : 4.878  accuracy : 21.8 %\n",
      "19m 35s (- 37m 24s) (13750 34%) loss : 4.889  accuracy : 22.2 %\n",
      "19m 56s (- 37m 2s) (14000 35%) loss : 4.912  accuracy : 22.0 %\n",
      "20m 18s (- 36m 41s) (14250 35%) loss : 4.869  accuracy : 22.6 %\n",
      "20m 39s (- 36m 19s) (14500 36%) loss : 4.894  accuracy : 22.6 %\n",
      "21m 0s (- 35m 58s) (14750 36%) loss : 4.954  accuracy : 21.7 %\n",
      "21m 22s (- 35m 36s) (15000 37%) loss : 4.988  accuracy : 21.4 %\n",
      "21m 43s (- 35m 15s) (15250 38%) loss : 4.991  accuracy : 22.0 %\n",
      "22m 4s (- 34m 54s) (15500 38%) loss : 4.820  accuracy : 23.1 %\n",
      "22m 26s (- 34m 32s) (15750 39%) loss : 4.888  accuracy : 22.6 %\n",
      "22m 47s (- 34m 11s) (16000 40%) loss : 4.821  accuracy : 23.0 %\n",
      "23m 9s (- 33m 50s) (16250 40%) loss : 4.784  accuracy : 23.0 %\n",
      "23m 30s (- 33m 28s) (16500 41%) loss : 4.924  accuracy : 22.5 %\n",
      "23m 51s (- 33m 7s) (16750 41%) loss : 4.907  accuracy : 21.7 %\n",
      "24m 13s (- 32m 46s) (17000 42%) loss : 4.870  accuracy : 22.3 %\n",
      "24m 34s (- 32m 24s) (17250 43%) loss : 4.876  accuracy : 22.5 %\n",
      "24m 55s (- 32m 3s) (17500 43%) loss : 4.878  accuracy : 22.5 %\n",
      "25m 17s (- 31m 41s) (17750 44%) loss : 4.964  accuracy : 21.6 %\n",
      "25m 38s (- 31m 20s) (18000 45%) loss : 4.876  accuracy : 22.4 %\n",
      "26m 0s (- 30m 59s) (18250 45%) loss : 4.907  accuracy : 22.1 %\n",
      "26m 21s (- 30m 38s) (18500 46%) loss : 4.946  accuracy : 21.6 %\n",
      "26m 42s (- 30m 16s) (18750 46%) loss : 4.894  accuracy : 21.9 %\n",
      "27m 4s (- 29m 55s) (19000 47%) loss : 4.842  accuracy : 22.3 %\n",
      "27m 25s (- 29m 33s) (19250 48%) loss : 4.870  accuracy : 22.5 %\n",
      "27m 46s (- 29m 12s) (19500 48%) loss : 4.956  accuracy : 21.9 %\n",
      "28m 8s (- 28m 50s) (19750 49%) loss : 4.919  accuracy : 21.6 %\n",
      "28m 29s (- 28m 29s) (20000 50%) loss : 4.854  accuracy : 22.4 %\n",
      "28m 51s (- 28m 9s) (20250 50%) loss : 4.849  accuracy : 22.5 %\n",
      "29m 13s (- 27m 47s) (20500 51%) loss : 4.883  accuracy : 21.9 %\n",
      "29m 35s (- 27m 26s) (20750 51%) loss : 4.887  accuracy : 21.9 %\n",
      "29m 56s (- 27m 5s) (21000 52%) loss : 4.868  accuracy : 21.9 %\n",
      "30m 17s (- 26m 43s) (21250 53%) loss : 4.933  accuracy : 21.7 %\n",
      "30m 38s (- 26m 22s) (21500 53%) loss : 4.863  accuracy : 22.6 %\n",
      "31m 0s (- 26m 0s) (21750 54%) loss : 4.877  accuracy : 22.2 %\n",
      "31m 21s (- 25m 39s) (22000 55%) loss : 4.817  accuracy : 22.6 %\n",
      "31m 43s (- 25m 18s) (22250 55%) loss : 4.870  accuracy : 22.2 %\n",
      "32m 4s (- 24m 56s) (22500 56%) loss : 4.884  accuracy : 22.1 %\n",
      "32m 25s (- 24m 35s) (22750 56%) loss : 4.860  accuracy : 22.2 %\n",
      "32m 47s (- 24m 13s) (23000 57%) loss : 4.838  accuracy : 22.2 %\n",
      "33m 8s (- 23m 52s) (23250 58%) loss : 4.910  accuracy : 22.3 %\n",
      "33m 29s (- 23m 31s) (23500 58%) loss : 4.870  accuracy : 22.1 %\n",
      "33m 51s (- 23m 9s) (23750 59%) loss : 4.898  accuracy : 22.0 %\n",
      "34m 12s (- 22m 48s) (24000 60%) loss : 4.851  accuracy : 22.5 %\n",
      "34m 33s (- 22m 26s) (24250 60%) loss : 4.812  accuracy : 22.2 %\n",
      "34m 55s (- 22m 5s) (24500 61%) loss : 4.829  accuracy : 22.0 %\n",
      "35m 16s (- 21m 44s) (24750 61%) loss : 4.858  accuracy : 22.5 %\n",
      "35m 37s (- 21m 22s) (25000 62%) loss : 4.965  accuracy : 21.7 %\n",
      "35m 58s (- 21m 1s) (25250 63%) loss : 4.818  accuracy : 23.0 %\n",
      "36m 20s (- 20m 39s) (25500 63%) loss : 4.809  accuracy : 22.3 %\n",
      "36m 41s (- 20m 18s) (25750 64%) loss : 4.770  accuracy : 22.6 %\n",
      "37m 2s (- 19m 56s) (26000 65%) loss : 4.859  accuracy : 22.1 %\n",
      "37m 24s (- 19m 35s) (26250 65%) loss : 4.886  accuracy : 21.9 %\n",
      "37m 45s (- 19m 14s) (26500 66%) loss : 4.709  accuracy : 22.8 %\n",
      "38m 6s (- 18m 52s) (26750 66%) loss : 4.817  accuracy : 22.8 %\n",
      "38m 28s (- 18m 31s) (27000 67%) loss : 4.757  accuracy : 22.9 %\n",
      "38m 49s (- 18m 9s) (27250 68%) loss : 4.825  accuracy : 22.6 %\n",
      "39m 10s (- 17m 48s) (27500 68%) loss : 4.827  accuracy : 22.3 %\n",
      "39m 32s (- 17m 27s) (27750 69%) loss : 4.771  accuracy : 23.1 %\n",
      "39m 53s (- 17m 5s) (28000 70%) loss : 4.824  accuracy : 22.2 %\n",
      "40m 14s (- 16m 44s) (28250 70%) loss : 4.955  accuracy : 21.6 %\n",
      "40m 36s (- 16m 22s) (28500 71%) loss : 4.863  accuracy : 22.2 %\n",
      "40m 57s (- 16m 1s) (28750 71%) loss : 4.847  accuracy : 22.1 %\n",
      "41m 18s (- 15m 40s) (29000 72%) loss : 4.918  accuracy : 21.6 %\n",
      "41m 39s (- 15m 18s) (29250 73%) loss : 4.820  accuracy : 22.1 %\n",
      "42m 1s (- 14m 57s) (29500 73%) loss : 4.875  accuracy : 21.3 %\n",
      "42m 22s (- 14m 36s) (29750 74%) loss : 4.938  accuracy : 21.7 %\n",
      "42m 44s (- 14m 14s) (30000 75%) loss : 4.956  accuracy : 21.2 %\n",
      "43m 5s (- 13m 53s) (30250 75%) loss : 4.895  accuracy : 21.6 %\n",
      "43m 26s (- 13m 31s) (30500 76%) loss : 4.833  accuracy : 22.4 %\n",
      "43m 47s (- 13m 10s) (30750 76%) loss : 4.838  accuracy : 22.6 %\n",
      "44m 9s (- 12m 49s) (31000 77%) loss : 4.820  accuracy : 22.2 %\n",
      "44m 30s (- 12m 27s) (31250 78%) loss : 4.708  accuracy : 23.4 %\n",
      "44m 52s (- 12m 6s) (31500 78%) loss : 4.729  accuracy : 23.1 %\n",
      "45m 13s (- 11m 45s) (31750 79%) loss : 4.823  accuracy : 22.3 %\n",
      "45m 34s (- 11m 23s) (32000 80%) loss : 4.790  accuracy : 22.5 %\n",
      "45m 56s (- 11m 2s) (32250 80%) loss : 4.868  accuracy : 21.8 %\n",
      "46m 17s (- 10m 40s) (32500 81%) loss : 4.797  accuracy : 22.5 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46m 38s (- 10m 19s) (32750 81%) loss : 4.799  accuracy : 22.2 %\n",
      "46m 59s (- 9m 58s) (33000 82%) loss : 4.891  accuracy : 21.8 %\n",
      "47m 21s (- 9m 36s) (33250 83%) loss : 4.816  accuracy : 22.3 %\n",
      "47m 42s (- 9m 15s) (33500 83%) loss : 4.864  accuracy : 21.9 %\n",
      "48m 4s (- 8m 54s) (33750 84%) loss : 4.826  accuracy : 22.3 %\n",
      "48m 25s (- 8m 32s) (34000 85%) loss : 4.760  accuracy : 22.8 %\n",
      "48m 46s (- 8m 11s) (34250 85%) loss : 4.853  accuracy : 22.2 %\n",
      "49m 7s (- 7m 49s) (34500 86%) loss : 4.830  accuracy : 22.4 %\n",
      "49m 29s (- 7m 28s) (34750 86%) loss : 4.761  accuracy : 23.2 %\n",
      "49m 50s (- 7m 7s) (35000 87%) loss : 4.774  accuracy : 23.1 %\n",
      "50m 11s (- 6m 45s) (35250 88%) loss : 4.764  accuracy : 22.9 %\n",
      "50m 33s (- 6m 24s) (35500 88%) loss : 4.681  accuracy : 22.6 %\n",
      "50m 54s (- 6m 3s) (35750 89%) loss : 4.923  accuracy : 22.0 %\n",
      "51m 16s (- 5m 41s) (36000 90%) loss : 4.830  accuracy : 22.0 %\n",
      "51m 37s (- 5m 20s) (36250 90%) loss : 4.737  accuracy : 22.7 %\n",
      "51m 58s (- 4m 59s) (36500 91%) loss : 4.828  accuracy : 22.3 %\n",
      "52m 20s (- 4m 37s) (36750 91%) loss : 4.731  accuracy : 23.3 %\n",
      "52m 41s (- 4m 16s) (37000 92%) loss : 4.907  accuracy : 21.6 %\n",
      "53m 2s (- 3m 54s) (37250 93%) loss : 4.802  accuracy : 22.6 %\n",
      "53m 23s (- 3m 33s) (37500 93%) loss : 4.899  accuracy : 21.8 %\n",
      "53m 45s (- 3m 12s) (37750 94%) loss : 4.823  accuracy : 22.0 %\n",
      "54m 6s (- 2m 50s) (38000 95%) loss : 4.814  accuracy : 22.8 %\n",
      "54m 27s (- 2m 29s) (38250 95%) loss : 4.728  accuracy : 22.6 %\n",
      "54m 49s (- 2m 8s) (38500 96%) loss : 4.883  accuracy : 22.0 %\n",
      "55m 10s (- 1m 46s) (38750 96%) loss : 4.856  accuracy : 22.4 %\n",
      "55m 32s (- 1m 25s) (39000 97%) loss : 4.838  accuracy : 22.2 %\n",
      "55m 53s (- 1m 4s) (39250 98%) loss : 4.829  accuracy : 22.4 %\n",
      "56m 14s (- 0m 42s) (39500 98%) loss : 4.831  accuracy : 21.9 %\n",
      "56m 35s (- 0m 21s) (39750 99%) loss : 4.816  accuracy : 22.6 %\n",
      "56m 57s (- 0m 0s) (40000 100%) loss : 4.798  accuracy : 22.5 %\n",
      "epoch 1\n",
      "0m 23s (- 40m 14s) (250 0%) loss : 4.641  accuracy : 22.9 %\n",
      "0m 46s (- 39m 51s) (500 1%) loss : 4.606  accuracy : 23.4 %\n",
      "1m 9s (- 39m 24s) (750 2%) loss : 4.682  accuracy : 23.4 %\n",
      "1m 33s (- 38m 59s) (1000 3%) loss : 4.708  accuracy : 23.7 %\n",
      "1m 56s (- 38m 37s) (1250 4%) loss : 4.819  accuracy : 22.2 %\n",
      "2m 19s (- 38m 14s) (1500 5%) loss : 4.703  accuracy : 23.3 %\n",
      "2m 43s (- 37m 52s) (1750 6%) loss : 4.678  accuracy : 23.3 %\n",
      "3m 6s (- 37m 32s) (2000 7%) loss : 4.672  accuracy : 23.4 %\n",
      "3m 29s (- 37m 9s) (2250 8%) loss : 4.691  accuracy : 22.9 %\n",
      "3m 53s (- 36m 45s) (2500 9%) loss : 4.753  accuracy : 22.9 %\n",
      "4m 16s (- 36m 20s) (2750 10%) loss : 4.789  accuracy : 22.4 %\n",
      "4m 39s (- 35m 57s) (3000 11%) loss : 4.716  accuracy : 23.0 %\n",
      "5m 3s (- 35m 35s) (3250 12%) loss : 4.718  accuracy : 23.1 %\n",
      "5m 26s (- 35m 12s) (3500 13%) loss : 4.642  accuracy : 23.9 %\n",
      "5m 49s (- 34m 49s) (3750 14%) loss : 4.706  accuracy : 22.9 %\n",
      "6m 13s (- 34m 25s) (4000 15%) loss : 4.747  accuracy : 23.5 %\n",
      "6m 36s (- 34m 1s) (4250 16%) loss : 4.771  accuracy : 23.0 %\n",
      "6m 59s (- 33m 38s) (4500 17%) loss : 4.830  accuracy : 22.6 %\n",
      "7m 22s (- 33m 14s) (4750 18%) loss : 4.754  accuracy : 23.3 %\n",
      "7m 46s (- 32m 51s) (5000 19%) loss : 4.789  accuracy : 23.0 %\n",
      "8m 9s (- 32m 28s) (5250 20%) loss : 4.621  accuracy : 23.8 %\n",
      "8m 32s (- 32m 5s) (5500 21%) loss : 4.721  accuracy : 23.6 %\n",
      "8m 56s (- 31m 41s) (5750 21%) loss : 4.648  accuracy : 23.2 %\n",
      "9m 19s (- 31m 18s) (6000 22%) loss : 4.682  accuracy : 24.0 %\n",
      "9m 42s (- 30m 55s) (6250 23%) loss : 4.751  accuracy : 23.0 %\n",
      "10m 6s (- 30m 32s) (6500 24%) loss : 4.674  accuracy : 23.8 %\n",
      "10m 29s (- 30m 8s) (6750 25%) loss : 4.768  accuracy : 23.9 %\n",
      "10m 52s (- 29m 45s) (7000 26%) loss : 4.784  accuracy : 22.7 %\n",
      "11m 15s (- 29m 21s) (7250 27%) loss : 4.670  accuracy : 24.0 %\n",
      "11m 39s (- 28m 58s) (7500 28%) loss : 4.716  accuracy : 23.8 %\n",
      "12m 2s (- 28m 35s) (7750 29%) loss : 4.776  accuracy : 23.0 %\n",
      "12m 25s (- 28m 11s) (8000 30%) loss : 4.633  accuracy : 24.4 %\n",
      "12m 49s (- 27m 48s) (8250 31%) loss : 4.665  accuracy : 23.9 %\n",
      "13m 12s (- 27m 25s) (8500 32%) loss : 4.666  accuracy : 23.6 %\n",
      "13m 35s (- 27m 1s) (8750 33%) loss : 4.765  accuracy : 23.4 %\n",
      "13m 58s (- 26m 38s) (9000 34%) loss : 4.714  accuracy : 23.5 %\n",
      "14m 22s (- 26m 15s) (9250 35%) loss : 4.691  accuracy : 23.7 %\n",
      "14m 45s (- 25m 52s) (9500 36%) loss : 4.606  accuracy : 24.2 %\n",
      "15m 8s (- 25m 28s) (9750 37%) loss : 4.640  accuracy : 24.1 %\n",
      "15m 32s (- 25m 5s) (10000 38%) loss : 4.735  accuracy : 23.8 %\n",
      "15m 55s (- 24m 41s) (10250 39%) loss : 4.700  accuracy : 23.5 %\n",
      "16m 18s (- 24m 18s) (10500 40%) loss : 4.711  accuracy : 23.8 %\n",
      "16m 42s (- 23m 55s) (10750 41%) loss : 4.661  accuracy : 23.7 %\n",
      "17m 5s (- 23m 31s) (11000 42%) loss : 4.711  accuracy : 24.0 %\n",
      "17m 28s (- 23m 8s) (11250 43%) loss : 4.609  accuracy : 24.1 %\n",
      "17m 51s (- 22m 45s) (11500 43%) loss : 4.660  accuracy : 23.8 %\n",
      "18m 15s (- 22m 21s) (11750 44%) loss : 4.671  accuracy : 23.7 %\n",
      "18m 38s (- 21m 58s) (12000 45%) loss : 4.677  accuracy : 23.7 %\n",
      "19m 1s (- 21m 35s) (12250 46%) loss : 4.752  accuracy : 23.5 %\n",
      "19m 24s (- 21m 11s) (12500 47%) loss : 4.655  accuracy : 24.0 %\n",
      "19m 48s (- 20m 48s) (12750 48%) loss : 4.755  accuracy : 23.6 %\n",
      "20m 11s (- 20m 25s) (13000 49%) loss : 4.824  accuracy : 23.1 %\n",
      "20m 34s (- 20m 2s) (13250 50%) loss : 4.690  accuracy : 24.2 %\n",
      "20m 58s (- 19m 38s) (13500 51%) loss : 4.710  accuracy : 23.2 %\n",
      "21m 21s (- 19m 15s) (13750 52%) loss : 4.727  accuracy : 23.3 %\n",
      "21m 44s (- 18m 52s) (14000 53%) loss : 4.743  accuracy : 23.1 %\n",
      "22m 7s (- 18m 28s) (14250 54%) loss : 4.711  accuracy : 24.0 %\n",
      "22m 31s (- 18m 5s) (14500 55%) loss : 4.717  accuracy : 23.9 %\n",
      "22m 54s (- 17m 42s) (14750 56%) loss : 4.696  accuracy : 23.3 %\n",
      "23m 17s (- 17m 18s) (15000 57%) loss : 4.690  accuracy : 23.8 %\n",
      "23m 41s (- 16m 55s) (15250 58%) loss : 4.634  accuracy : 23.4 %\n",
      "24m 4s (- 16m 32s) (15500 59%) loss : 4.636  accuracy : 23.9 %\n",
      "24m 27s (- 16m 8s) (15750 60%) loss : 4.779  accuracy : 23.1 %\n",
      "24m 50s (- 15m 45s) (16000 61%) loss : 4.768  accuracy : 23.0 %\n",
      "25m 14s (- 15m 22s) (16250 62%) loss : 4.689  accuracy : 23.5 %\n",
      "25m 37s (- 14m 59s) (16500 63%) loss : 4.634  accuracy : 24.2 %\n",
      "26m 0s (- 14m 35s) (16750 64%) loss : 4.673  accuracy : 23.4 %\n",
      "26m 24s (- 14m 12s) (17000 65%) loss : 4.646  accuracy : 24.0 %\n",
      "26m 47s (- 13m 49s) (17250 65%) loss : 4.693  accuracy : 23.5 %\n",
      "27m 10s (- 13m 25s) (17500 66%) loss : 4.725  accuracy : 23.3 %\n",
      "27m 34s (- 13m 2s) (17750 67%) loss : 4.627  accuracy : 23.9 %\n",
      "27m 57s (- 12m 39s) (18000 68%) loss : 4.633  accuracy : 24.2 %\n",
      "28m 20s (- 12m 16s) (18250 69%) loss : 4.676  accuracy : 24.2 %\n",
      "28m 44s (- 11m 52s) (18500 70%) loss : 4.670  accuracy : 24.1 %\n",
      "29m 7s (- 11m 29s) (18750 71%) loss : 4.615  accuracy : 24.3 %\n",
      "29m 30s (- 11m 6s) (19000 72%) loss : 4.681  accuracy : 23.7 %\n",
      "29m 53s (- 10m 42s) (19250 73%) loss : 4.681  accuracy : 23.6 %\n",
      "30m 17s (- 10m 19s) (19500 74%) loss : 4.705  accuracy : 23.0 %\n",
      "30m 40s (- 9m 56s) (19750 75%) loss : 4.682  accuracy : 23.7 %\n",
      "31m 3s (- 9m 32s) (20000 76%) loss : 4.688  accuracy : 23.4 %\n",
      "31m 26s (- 9m 9s) (20250 77%) loss : 4.683  accuracy : 23.6 %\n",
      "31m 50s (- 8m 46s) (20500 78%) loss : 4.754  accuracy : 23.2 %\n",
      "32m 13s (- 8m 22s) (20750 79%) loss : 4.570  accuracy : 24.4 %\n",
      "32m 36s (- 7m 59s) (21000 80%) loss : 4.663  accuracy : 24.5 %\n",
      "33m 0s (- 7m 36s) (21250 81%) loss : 4.731  accuracy : 23.4 %\n",
      "33m 23s (- 7m 13s) (21500 82%) loss : 4.655  accuracy : 23.7 %\n",
      "33m 46s (- 6m 49s) (21750 83%) loss : 4.666  accuracy : 23.7 %\n",
      "34m 10s (- 6m 26s) (22000 84%) loss : 4.644  accuracy : 24.1 %\n",
      "34m 33s (- 6m 3s) (22250 85%) loss : 4.601  accuracy : 24.0 %\n",
      "34m 56s (- 5m 39s) (22500 86%) loss : 4.701  accuracy : 23.4 %\n",
      "35m 20s (- 5m 16s) (22750 87%) loss : 4.692  accuracy : 23.2 %\n",
      "35m 43s (- 4m 53s) (23000 87%) loss : 4.739  accuracy : 23.9 %\n",
      "36m 6s (- 4m 30s) (23250 88%) loss : 4.717  accuracy : 23.1 %\n",
      "36m 30s (- 4m 6s) (23500 89%) loss : 4.639  accuracy : 24.2 %\n",
      "36m 53s (- 3m 43s) (23750 90%) loss : 4.688  accuracy : 23.9 %\n",
      "37m 16s (- 3m 20s) (24000 91%) loss : 4.698  accuracy : 23.5 %\n",
      "37m 40s (- 2m 56s) (24250 92%) loss : 4.642  accuracy : 24.2 %\n",
      "38m 3s (- 2m 33s) (24500 93%) loss : 4.659  accuracy : 23.9 %\n",
      "38m 26s (- 2m 10s) (24750 94%) loss : 4.653  accuracy : 24.1 %\n",
      "38m 49s (- 1m 46s) (25000 95%) loss : 4.661  accuracy : 23.6 %\n",
      "39m 13s (- 1m 23s) (25250 96%) loss : 4.770  accuracy : 23.2 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39m 36s (- 1m 0s) (25500 97%) loss : 4.698  accuracy : 23.5 %\n",
      "39m 59s (- 0m 37s) (25750 98%) loss : 4.643  accuracy : 23.6 %\n",
      "40m 23s (- 0m 13s) (26000 99%) loss : 4.601  accuracy : 24.2 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches[:40000], epochs = 1, lr = 0.00025, print_every = 250)\n",
    "language_model.fit(batches[40000:], epochs = 1, lr = 0.0001, print_every = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63064"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches  = language_model.generatePackedSentences(corpus, batch_size = 64, depth_range = (6, 7))\n",
    "batches += language_model.generatePackedSentences(corpus, batch_size = 64, depth_range = (11, 12))\n",
    "batches += language_model.generatePackedSentences(corpus, batch_size = 64, depth_range = (16, 17))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 22s (- 58m 36s) (250 0%) loss : 4.705  accuracy : 22.9 %\n",
      "0m 43s (- 57m 45s) (500 1%) loss : 4.697  accuracy : 23.2 %\n",
      "1m 5s (- 57m 14s) (750 1%) loss : 4.633  accuracy : 23.0 %\n",
      "1m 27s (- 56m 42s) (1000 2%) loss : 4.674  accuracy : 24.2 %\n",
      "1m 49s (- 56m 19s) (1250 3%) loss : 4.683  accuracy : 23.4 %\n",
      "2m 10s (- 55m 56s) (1500 3%) loss : 4.767  accuracy : 23.9 %\n",
      "2m 32s (- 55m 31s) (1750 4%) loss : 4.759  accuracy : 23.0 %\n",
      "2m 54s (- 55m 8s) (2000 5%) loss : 4.755  accuracy : 23.9 %\n",
      "3m 15s (- 54m 47s) (2250 5%) loss : 4.633  accuracy : 24.1 %\n",
      "3m 37s (- 54m 25s) (2500 6%) loss : 4.784  accuracy : 23.2 %\n",
      "3m 59s (- 54m 2s) (2750 6%) loss : 4.761  accuracy : 23.0 %\n",
      "4m 21s (- 53m 39s) (3000 7%) loss : 4.705  accuracy : 24.5 %\n",
      "4m 42s (- 53m 17s) (3250 8%) loss : 4.742  accuracy : 23.8 %\n",
      "5m 4s (- 52m 55s) (3500 8%) loss : 4.675  accuracy : 23.6 %\n",
      "5m 26s (- 52m 33s) (3750 9%) loss : 4.696  accuracy : 23.8 %\n",
      "5m 47s (- 52m 11s) (4000 10%) loss : 4.744  accuracy : 23.1 %\n",
      "6m 9s (- 51m 49s) (4250 10%) loss : 4.690  accuracy : 23.5 %\n",
      "6m 31s (- 51m 28s) (4500 11%) loss : 4.700  accuracy : 23.9 %\n",
      "6m 53s (- 51m 6s) (4750 11%) loss : 4.729  accuracy : 23.1 %\n",
      "7m 14s (- 50m 43s) (5000 12%) loss : 4.757  accuracy : 22.8 %\n",
      "7m 36s (- 50m 22s) (5250 13%) loss : 4.764  accuracy : 23.2 %\n",
      "7m 58s (- 50m 0s) (5500 13%) loss : 4.706  accuracy : 23.4 %\n",
      "8m 19s (- 49m 38s) (5750 14%) loss : 4.761  accuracy : 22.6 %\n",
      "8m 41s (- 49m 16s) (6000 15%) loss : 4.661  accuracy : 24.0 %\n",
      "9m 3s (- 48m 54s) (6250 15%) loss : 4.758  accuracy : 23.6 %\n",
      "9m 25s (- 48m 32s) (6500 16%) loss : 4.709  accuracy : 23.4 %\n",
      "9m 46s (- 48m 10s) (6750 16%) loss : 4.749  accuracy : 23.7 %\n",
      "10m 8s (- 47m 49s) (7000 17%) loss : 4.833  accuracy : 23.4 %\n",
      "10m 30s (- 47m 27s) (7250 18%) loss : 4.731  accuracy : 23.7 %\n",
      "10m 52s (- 47m 5s) (7500 18%) loss : 4.806  accuracy : 23.3 %\n",
      "11m 13s (- 46m 44s) (7750 19%) loss : 4.737  accuracy : 23.9 %\n",
      "11m 35s (- 46m 22s) (8000 20%) loss : 4.725  accuracy : 23.9 %\n",
      "11m 57s (- 46m 0s) (8250 20%) loss : 4.731  accuracy : 23.9 %\n",
      "12m 20s (- 45m 43s) (8500 21%) loss : 4.714  accuracy : 22.9 %\n",
      "12m 42s (- 45m 23s) (8750 21%) loss : 4.776  accuracy : 23.6 %\n",
      "13m 5s (- 45m 4s) (9000 22%) loss : 4.745  accuracy : 23.9 %\n",
      "13m 26s (- 44m 42s) (9250 23%) loss : 4.740  accuracy : 23.7 %\n",
      "13m 48s (- 44m 20s) (9500 23%) loss : 4.771  accuracy : 23.4 %\n",
      "14m 10s (- 43m 58s) (9750 24%) loss : 4.774  accuracy : 23.5 %\n",
      "14m 32s (- 43m 36s) (10000 25%) loss : 4.708  accuracy : 23.6 %\n",
      "14m 54s (- 43m 15s) (10250 25%) loss : 4.787  accuracy : 23.3 %\n",
      "15m 15s (- 42m 53s) (10500 26%) loss : 4.730  accuracy : 23.3 %\n",
      "15m 37s (- 42m 31s) (10750 26%) loss : 4.780  accuracy : 23.5 %\n",
      "15m 59s (- 42m 9s) (11000 27%) loss : 4.755  accuracy : 23.0 %\n",
      "16m 21s (- 41m 48s) (11250 28%) loss : 4.678  accuracy : 24.2 %\n",
      "16m 43s (- 41m 26s) (11500 28%) loss : 4.752  accuracy : 23.3 %\n",
      "17m 5s (- 41m 4s) (11750 29%) loss : 4.789  accuracy : 23.3 %\n",
      "17m 26s (- 40m 42s) (12000 30%) loss : 4.709  accuracy : 24.1 %\n",
      "17m 48s (- 40m 21s) (12250 30%) loss : 4.783  accuracy : 24.0 %\n",
      "18m 10s (- 39m 59s) (12500 31%) loss : 4.743  accuracy : 23.8 %\n",
      "18m 32s (- 39m 37s) (12750 31%) loss : 4.734  accuracy : 23.6 %\n",
      "18m 54s (- 39m 15s) (13000 32%) loss : 4.686  accuracy : 23.7 %\n",
      "19m 15s (- 38m 53s) (13250 33%) loss : 4.656  accuracy : 24.0 %\n",
      "19m 37s (- 38m 31s) (13500 33%) loss : 4.727  accuracy : 23.8 %\n",
      "19m 59s (- 38m 9s) (13750 34%) loss : 4.726  accuracy : 22.7 %\n",
      "20m 20s (- 37m 47s) (14000 35%) loss : 4.873  accuracy : 22.4 %\n",
      "20m 42s (- 37m 25s) (14250 35%) loss : 4.700  accuracy : 23.6 %\n",
      "21m 4s (- 37m 3s) (14500 36%) loss : 4.790  accuracy : 22.7 %\n",
      "21m 26s (- 36m 41s) (14750 36%) loss : 4.807  accuracy : 23.4 %\n",
      "21m 47s (- 36m 19s) (15000 37%) loss : 4.733  accuracy : 23.8 %\n",
      "22m 9s (- 35m 57s) (15250 38%) loss : 4.821  accuracy : 22.5 %\n",
      "22m 31s (- 35m 35s) (15500 38%) loss : 4.707  accuracy : 24.1 %\n",
      "22m 53s (- 35m 14s) (15750 39%) loss : 4.698  accuracy : 23.3 %\n",
      "23m 14s (- 34m 52s) (16000 40%) loss : 4.715  accuracy : 23.4 %\n",
      "23m 36s (- 34m 30s) (16250 40%) loss : 4.810  accuracy : 23.0 %\n",
      "23m 58s (- 34m 8s) (16500 41%) loss : 4.744  accuracy : 23.6 %\n",
      "24m 20s (- 33m 46s) (16750 41%) loss : 4.702  accuracy : 23.4 %\n",
      "24m 42s (- 33m 25s) (17000 42%) loss : 4.713  accuracy : 23.6 %\n",
      "25m 3s (- 33m 3s) (17250 43%) loss : 4.737  accuracy : 23.3 %\n",
      "25m 25s (- 32m 41s) (17500 43%) loss : 4.685  accuracy : 24.5 %\n",
      "25m 47s (- 32m 19s) (17750 44%) loss : 4.700  accuracy : 23.4 %\n",
      "26m 9s (- 31m 57s) (18000 45%) loss : 4.725  accuracy : 23.7 %\n",
      "26m 30s (- 31m 36s) (18250 45%) loss : 4.660  accuracy : 24.1 %\n",
      "26m 52s (- 31m 14s) (18500 46%) loss : 4.809  accuracy : 23.4 %\n",
      "27m 14s (- 30m 52s) (18750 46%) loss : 4.699  accuracy : 23.9 %\n",
      "27m 36s (- 30m 30s) (19000 47%) loss : 4.743  accuracy : 23.5 %\n",
      "27m 57s (- 30m 8s) (19250 48%) loss : 4.705  accuracy : 23.6 %\n",
      "28m 19s (- 29m 46s) (19500 48%) loss : 4.755  accuracy : 23.1 %\n",
      "28m 41s (- 29m 24s) (19750 49%) loss : 4.748  accuracy : 23.2 %\n",
      "29m 3s (- 29m 3s) (20000 50%) loss : 4.655  accuracy : 23.6 %\n",
      "29m 24s (- 28m 41s) (20250 50%) loss : 4.697  accuracy : 23.8 %\n",
      "29m 46s (- 28m 19s) (20500 51%) loss : 4.741  accuracy : 23.8 %\n",
      "30m 8s (- 27m 57s) (20750 51%) loss : 4.730  accuracy : 23.8 %\n",
      "30m 29s (- 27m 35s) (21000 52%) loss : 4.766  accuracy : 22.9 %\n",
      "30m 51s (- 27m 13s) (21250 53%) loss : 4.722  accuracy : 24.1 %\n",
      "31m 13s (- 26m 51s) (21500 53%) loss : 4.679  accuracy : 23.8 %\n",
      "31m 35s (- 26m 30s) (21750 54%) loss : 4.752  accuracy : 23.7 %\n",
      "31m 56s (- 26m 8s) (22000 55%) loss : 4.638  accuracy : 24.1 %\n",
      "32m 18s (- 25m 46s) (22250 55%) loss : 4.800  accuracy : 22.5 %\n",
      "32m 40s (- 25m 24s) (22500 56%) loss : 4.730  accuracy : 23.2 %\n",
      "33m 2s (- 25m 2s) (22750 56%) loss : 4.746  accuracy : 24.0 %\n",
      "33m 23s (- 24m 41s) (23000 57%) loss : 4.749  accuracy : 23.0 %\n",
      "33m 45s (- 24m 19s) (23250 58%) loss : 4.750  accuracy : 23.1 %\n",
      "34m 7s (- 23m 57s) (23500 58%) loss : 4.650  accuracy : 24.3 %\n",
      "34m 29s (- 23m 35s) (23750 59%) loss : 4.804  accuracy : 23.2 %\n",
      "34m 50s (- 23m 13s) (24000 60%) loss : 4.700  accuracy : 23.8 %\n",
      "35m 12s (- 22m 52s) (24250 60%) loss : 4.658  accuracy : 23.8 %\n",
      "35m 34s (- 22m 30s) (24500 61%) loss : 4.752  accuracy : 23.4 %\n",
      "35m 56s (- 22m 8s) (24750 61%) loss : 4.710  accuracy : 23.5 %\n",
      "36m 17s (- 21m 46s) (25000 62%) loss : 4.750  accuracy : 23.3 %\n",
      "36m 39s (- 21m 24s) (25250 63%) loss : 4.692  accuracy : 23.7 %\n",
      "37m 1s (- 21m 3s) (25500 63%) loss : 4.684  accuracy : 23.7 %\n",
      "37m 23s (- 20m 41s) (25750 64%) loss : 4.620  accuracy : 24.8 %\n",
      "37m 44s (- 20m 19s) (26000 65%) loss : 4.738  accuracy : 23.7 %\n",
      "38m 6s (- 19m 57s) (26250 65%) loss : 4.869  accuracy : 23.2 %\n",
      "38m 28s (- 19m 35s) (26500 66%) loss : 4.740  accuracy : 23.4 %\n",
      "38m 49s (- 19m 14s) (26750 66%) loss : 4.682  accuracy : 24.1 %\n",
      "39m 11s (- 18m 52s) (27000 67%) loss : 4.647  accuracy : 23.9 %\n",
      "39m 33s (- 18m 30s) (27250 68%) loss : 4.763  accuracy : 23.4 %\n",
      "39m 54s (- 18m 8s) (27500 68%) loss : 4.711  accuracy : 23.8 %\n",
      "40m 16s (- 17m 46s) (27750 69%) loss : 4.688  accuracy : 23.5 %\n",
      "40m 38s (- 17m 25s) (28000 70%) loss : 4.672  accuracy : 23.9 %\n",
      "41m 0s (- 17m 3s) (28250 70%) loss : 4.764  accuracy : 22.9 %\n",
      "41m 21s (- 16m 41s) (28500 71%) loss : 4.827  accuracy : 22.9 %\n",
      "41m 43s (- 16m 19s) (28750 71%) loss : 4.779  accuracy : 24.1 %\n",
      "42m 5s (- 15m 57s) (29000 72%) loss : 4.735  accuracy : 23.5 %\n",
      "42m 27s (- 15m 36s) (29250 73%) loss : 4.710  accuracy : 23.5 %\n",
      "42m 49s (- 15m 14s) (29500 73%) loss : 4.727  accuracy : 23.6 %\n",
      "43m 10s (- 14m 52s) (29750 74%) loss : 4.735  accuracy : 23.6 %\n",
      "43m 32s (- 14m 30s) (30000 75%) loss : 4.744  accuracy : 23.3 %\n",
      "43m 54s (- 14m 9s) (30250 75%) loss : 4.698  accuracy : 22.9 %\n",
      "44m 16s (- 13m 47s) (30500 76%) loss : 4.677  accuracy : 23.6 %\n",
      "44m 37s (- 13m 25s) (30750 76%) loss : 4.726  accuracy : 23.3 %\n",
      "44m 59s (- 13m 3s) (31000 77%) loss : 4.778  accuracy : 23.3 %\n",
      "45m 21s (- 12m 41s) (31250 78%) loss : 4.744  accuracy : 23.8 %\n",
      "45m 42s (- 12m 20s) (31500 78%) loss : 4.712  accuracy : 23.9 %\n",
      "46m 4s (- 11m 58s) (31750 79%) loss : 4.699  accuracy : 23.1 %\n",
      "46m 26s (- 11m 36s) (32000 80%) loss : 4.733  accuracy : 23.7 %\n",
      "46m 48s (- 11m 14s) (32250 80%) loss : 4.753  accuracy : 23.6 %\n",
      "47m 9s (- 10m 53s) (32500 81%) loss : 4.726  accuracy : 23.4 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47m 31s (- 10m 31s) (32750 81%) loss : 4.672  accuracy : 23.6 %\n",
      "47m 53s (- 10m 9s) (33000 82%) loss : 4.711  accuracy : 23.3 %\n",
      "48m 14s (- 9m 47s) (33250 83%) loss : 4.689  accuracy : 23.5 %\n",
      "48m 36s (- 9m 25s) (33500 83%) loss : 4.674  accuracy : 23.5 %\n",
      "48m 58s (- 9m 4s) (33750 84%) loss : 4.772  accuracy : 23.0 %\n",
      "49m 19s (- 8m 42s) (34000 85%) loss : 4.841  accuracy : 22.8 %\n",
      "49m 41s (- 8m 20s) (34250 85%) loss : 4.719  accuracy : 23.6 %\n",
      "50m 3s (- 7m 58s) (34500 86%) loss : 4.713  accuracy : 23.2 %\n",
      "50m 25s (- 7m 37s) (34750 86%) loss : 4.681  accuracy : 23.7 %\n",
      "50m 46s (- 7m 15s) (35000 87%) loss : 4.661  accuracy : 23.9 %\n",
      "51m 8s (- 6m 53s) (35250 88%) loss : 4.645  accuracy : 24.2 %\n",
      "51m 30s (- 6m 31s) (35500 88%) loss : 4.650  accuracy : 24.0 %\n",
      "51m 52s (- 6m 9s) (35750 89%) loss : 4.746  accuracy : 23.2 %\n",
      "52m 13s (- 5m 48s) (36000 90%) loss : 4.733  accuracy : 24.0 %\n",
      "52m 35s (- 5m 26s) (36250 90%) loss : 4.716  accuracy : 23.4 %\n",
      "52m 57s (- 5m 4s) (36500 91%) loss : 4.692  accuracy : 23.6 %\n",
      "53m 19s (- 4m 42s) (36750 91%) loss : 4.721  accuracy : 23.6 %\n",
      "53m 40s (- 4m 21s) (37000 92%) loss : 4.666  accuracy : 23.9 %\n",
      "54m 2s (- 3m 59s) (37250 93%) loss : 4.741  accuracy : 23.1 %\n",
      "54m 24s (- 3m 37s) (37500 93%) loss : 4.666  accuracy : 23.4 %\n",
      "54m 45s (- 3m 15s) (37750 94%) loss : 4.684  accuracy : 23.3 %\n",
      "55m 7s (- 2m 54s) (38000 95%) loss : 4.792  accuracy : 23.2 %\n",
      "55m 29s (- 2m 32s) (38250 95%) loss : 4.683  accuracy : 23.8 %\n",
      "55m 51s (- 2m 10s) (38500 96%) loss : 4.669  accuracy : 24.2 %\n",
      "56m 13s (- 1m 48s) (38750 96%) loss : 4.666  accuracy : 23.8 %\n",
      "56m 34s (- 1m 27s) (39000 97%) loss : 4.804  accuracy : 22.9 %\n",
      "56m 56s (- 1m 5s) (39250 98%) loss : 4.685  accuracy : 24.0 %\n",
      "57m 18s (- 0m 43s) (39500 98%) loss : 4.755  accuracy : 23.3 %\n",
      "57m 39s (- 0m 21s) (39750 99%) loss : 4.675  accuracy : 23.9 %\n",
      "58m 1s (- 0m 0s) (40000 100%) loss : 4.669  accuracy : 24.0 %\n",
      "epoch 1\n",
      "0m 23s (- 35m 58s) (250 1%) loss : 4.615  accuracy : 23.6 %\n",
      "0m 47s (- 35m 33s) (500 2%) loss : 4.597  accuracy : 24.3 %\n",
      "1m 10s (- 35m 12s) (750 3%) loss : 4.586  accuracy : 24.1 %\n",
      "1m 34s (- 34m 49s) (1000 4%) loss : 4.617  accuracy : 23.9 %\n",
      "1m 58s (- 34m 24s) (1250 5%) loss : 4.690  accuracy : 24.2 %\n",
      "2m 21s (- 34m 1s) (1500 6%) loss : 4.595  accuracy : 24.3 %\n",
      "2m 45s (- 33m 37s) (1750 7%) loss : 4.595  accuracy : 24.4 %\n",
      "3m 9s (- 33m 11s) (2000 8%) loss : 4.608  accuracy : 24.8 %\n",
      "3m 32s (- 32m 48s) (2250 9%) loss : 4.646  accuracy : 23.9 %\n",
      "3m 56s (- 32m 23s) (2500 10%) loss : 4.662  accuracy : 24.1 %\n",
      "4m 20s (- 32m 1s) (2750 11%) loss : 4.602  accuracy : 24.4 %\n",
      "4m 43s (- 31m 36s) (3000 13%) loss : 4.715  accuracy : 23.6 %\n",
      "5m 7s (- 31m 12s) (3250 14%) loss : 4.648  accuracy : 23.8 %\n",
      "5m 30s (- 30m 47s) (3500 15%) loss : 4.728  accuracy : 23.6 %\n",
      "5m 54s (- 30m 23s) (3750 16%) loss : 4.623  accuracy : 24.4 %\n",
      "6m 17s (- 30m 0s) (4000 17%) loss : 4.582  accuracy : 24.2 %\n",
      "6m 41s (- 29m 37s) (4250 18%) loss : 4.528  accuracy : 24.8 %\n",
      "7m 5s (- 29m 13s) (4500 19%) loss : 4.641  accuracy : 24.0 %\n",
      "7m 28s (- 28m 50s) (4750 20%) loss : 4.652  accuracy : 24.2 %\n",
      "7m 52s (- 28m 26s) (5000 21%) loss : 4.598  accuracy : 24.0 %\n",
      "8m 16s (- 28m 3s) (5250 22%) loss : 4.615  accuracy : 24.5 %\n",
      "8m 39s (- 27m 39s) (5500 23%) loss : 4.605  accuracy : 24.3 %\n",
      "9m 3s (- 27m 16s) (5750 24%) loss : 4.571  accuracy : 25.2 %\n",
      "9m 27s (- 26m 53s) (6000 26%) loss : 4.628  accuracy : 24.4 %\n",
      "9m 50s (- 26m 29s) (6250 27%) loss : 4.528  accuracy : 24.3 %\n",
      "10m 14s (- 26m 5s) (6500 28%) loss : 4.600  accuracy : 24.1 %\n",
      "10m 38s (- 25m 42s) (6750 29%) loss : 4.673  accuracy : 23.3 %\n",
      "11m 1s (- 25m 18s) (7000 30%) loss : 4.557  accuracy : 24.3 %\n",
      "11m 25s (- 24m 54s) (7250 31%) loss : 4.527  accuracy : 25.1 %\n",
      "11m 48s (- 24m 31s) (7500 32%) loss : 4.611  accuracy : 24.1 %\n",
      "12m 12s (- 24m 7s) (7750 33%) loss : 4.661  accuracy : 23.9 %\n",
      "12m 36s (- 23m 44s) (8000 34%) loss : 4.647  accuracy : 23.8 %\n",
      "13m 0s (- 23m 20s) (8250 35%) loss : 4.589  accuracy : 24.2 %\n",
      "13m 23s (- 22m 57s) (8500 36%) loss : 4.624  accuracy : 23.6 %\n",
      "13m 47s (- 22m 33s) (8750 37%) loss : 4.620  accuracy : 24.3 %\n",
      "14m 11s (- 22m 10s) (9000 39%) loss : 4.600  accuracy : 24.0 %\n",
      "14m 34s (- 21m 46s) (9250 40%) loss : 4.585  accuracy : 24.3 %\n",
      "14m 58s (- 21m 22s) (9500 41%) loss : 4.676  accuracy : 24.0 %\n",
      "15m 22s (- 20m 59s) (9750 42%) loss : 4.599  accuracy : 24.6 %\n",
      "15m 46s (- 20m 35s) (10000 43%) loss : 4.529  accuracy : 24.8 %\n",
      "16m 9s (- 20m 12s) (10250 44%) loss : 4.620  accuracy : 24.0 %\n",
      "16m 33s (- 19m 48s) (10500 45%) loss : 4.566  accuracy : 25.1 %\n",
      "16m 56s (- 19m 24s) (10750 46%) loss : 4.620  accuracy : 23.9 %\n",
      "17m 20s (- 19m 1s) (11000 47%) loss : 4.641  accuracy : 24.0 %\n",
      "17m 44s (- 18m 37s) (11250 48%) loss : 4.615  accuracy : 24.9 %\n",
      "18m 8s (- 18m 14s) (11500 49%) loss : 4.566  accuracy : 24.7 %\n",
      "18m 31s (- 17m 50s) (11750 50%) loss : 4.596  accuracy : 25.0 %\n",
      "18m 55s (- 17m 26s) (12000 52%) loss : 4.598  accuracy : 24.3 %\n",
      "19m 19s (- 17m 3s) (12250 53%) loss : 4.774  accuracy : 23.3 %\n",
      "19m 42s (- 16m 39s) (12500 54%) loss : 4.643  accuracy : 23.8 %\n",
      "20m 6s (- 16m 15s) (12750 55%) loss : 4.604  accuracy : 23.8 %\n",
      "20m 29s (- 15m 52s) (13000 56%) loss : 4.590  accuracy : 24.6 %\n",
      "20m 53s (- 15m 28s) (13250 57%) loss : 4.614  accuracy : 24.9 %\n",
      "21m 17s (- 15m 4s) (13500 58%) loss : 4.617  accuracy : 24.5 %\n",
      "21m 40s (- 14m 41s) (13750 59%) loss : 4.690  accuracy : 24.2 %\n",
      "22m 4s (- 14m 17s) (14000 60%) loss : 4.629  accuracy : 24.2 %\n",
      "22m 27s (- 13m 53s) (14250 61%) loss : 4.662  accuracy : 23.6 %\n",
      "22m 51s (- 13m 30s) (14500 62%) loss : 4.605  accuracy : 25.1 %\n",
      "23m 15s (- 13m 6s) (14750 63%) loss : 4.544  accuracy : 24.9 %\n",
      "23m 39s (- 12m 42s) (15000 65%) loss : 4.627  accuracy : 24.1 %\n",
      "24m 2s (- 12m 19s) (15250 66%) loss : 4.584  accuracy : 24.1 %\n",
      "24m 26s (- 11m 55s) (15500 67%) loss : 4.572  accuracy : 24.1 %\n",
      "24m 49s (- 11m 31s) (15750 68%) loss : 4.622  accuracy : 24.1 %\n",
      "25m 13s (- 11m 8s) (16000 69%) loss : 4.626  accuracy : 24.2 %\n",
      "25m 37s (- 10m 44s) (16250 70%) loss : 4.606  accuracy : 24.6 %\n",
      "26m 0s (- 10m 20s) (16500 71%) loss : 4.699  accuracy : 24.0 %\n",
      "26m 24s (- 9m 57s) (16750 72%) loss : 4.668  accuracy : 24.4 %\n",
      "26m 48s (- 9m 33s) (17000 73%) loss : 4.609  accuracy : 24.0 %\n",
      "27m 12s (- 9m 10s) (17250 74%) loss : 4.457  accuracy : 25.5 %\n",
      "27m 35s (- 8m 46s) (17500 75%) loss : 4.584  accuracy : 24.5 %\n",
      "27m 59s (- 8m 22s) (17750 76%) loss : 4.639  accuracy : 23.5 %\n",
      "28m 22s (- 7m 59s) (18000 78%) loss : 4.657  accuracy : 24.4 %\n",
      "28m 46s (- 7m 35s) (18250 79%) loss : 4.576  accuracy : 24.4 %\n",
      "29m 10s (- 7m 11s) (18500 80%) loss : 4.664  accuracy : 24.5 %\n",
      "29m 34s (- 6m 48s) (18750 81%) loss : 4.625  accuracy : 24.3 %\n",
      "29m 58s (- 6m 24s) (19000 82%) loss : 4.644  accuracy : 23.4 %\n",
      "30m 22s (- 6m 0s) (19250 83%) loss : 4.619  accuracy : 24.2 %\n",
      "30m 45s (- 5m 37s) (19500 84%) loss : 4.604  accuracy : 24.0 %\n",
      "31m 9s (- 5m 13s) (19750 85%) loss : 4.696  accuracy : 23.8 %\n",
      "31m 32s (- 4m 50s) (20000 86%) loss : 4.593  accuracy : 24.9 %\n",
      "31m 56s (- 4m 26s) (20250 87%) loss : 4.666  accuracy : 23.7 %\n",
      "32m 20s (- 4m 2s) (20500 88%) loss : 4.619  accuracy : 24.2 %\n",
      "32m 44s (- 3m 39s) (20750 89%) loss : 4.697  accuracy : 23.5 %\n",
      "33m 7s (- 3m 15s) (21000 91%) loss : 4.638  accuracy : 24.0 %\n",
      "33m 31s (- 2m 51s) (21250 92%) loss : 4.533  accuracy : 24.4 %\n",
      "33m 55s (- 2m 28s) (21500 93%) loss : 4.581  accuracy : 24.0 %\n",
      "34m 18s (- 2m 4s) (21750 94%) loss : 4.576  accuracy : 24.9 %\n",
      "34m 42s (- 1m 40s) (22000 95%) loss : 4.591  accuracy : 24.8 %\n",
      "35m 5s (- 1m 17s) (22250 96%) loss : 4.570  accuracy : 24.9 %\n",
      "35m 29s (- 0m 53s) (22500 97%) loss : 4.697  accuracy : 23.5 %\n",
      "35m 53s (- 0m 29s) (22750 98%) loss : 4.650  accuracy : 24.2 %\n",
      "36m 16s (- 0m 6s) (23000 99%) loss : 4.596  accuracy : 24.3 %\n"
     ]
    }
   ],
   "source": [
    "language_model.fit(batches[:40000], epochs = 1, lr = 0.00005, print_every = 250)\n",
    "language_model.fit(batches[40000:], epochs = 1, lr = 0.00001, print_every = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(language_model.state_dict(), path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I3_language_model.pth')\n",
    "\n",
    "# load\n",
    "#language_model.load_state_dict(torch.load(path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I3_language_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. museveni told the u.n. leader he \u001b[48;2;255;229;217m the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# fastText gensim, n_layers = 3, dh = 50\n",
    "language_model.eval()\n",
    "sentence = random.choice(corpus)\n",
    "i = random.choice(range(int(len(sentence)/2)))\n",
    "sentence = ' '.join(sentence[:i]) if i > 0 else '.'\n",
    "language_model(sentence, limit = '.', color_code = '\\x1b[48;2;255;229;217m') #  '\\x1b[48;2;255;229;217m' '\\x1b[31m'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
