{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=red>I - 4 </font>\n",
    "    Sentence Labelling\n",
    "    \n",
    "  </div> \n",
    "\n",
    "<div style=\"\n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 20px; \n",
    "      margin: 10px;\">\n",
    "  a. POS tagging\n",
    "  </div>\n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "\n",
    "1. Word Embedding\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. <font color=red>**Sequence Labelling**</font>\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "5. Auto-Encoding\n",
    "\n",
    "6. Machine Translation\n",
    "\n",
    "7. Text Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "8. Abstractive Summarization\n",
    "\n",
    "9. Question Answering\n",
    "\n",
    "10. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|------|------|------|------|\n",
    "| **Content** | [Corpus](#corpus) | [Modules](#modules) | [Model](#model) | \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "\n",
    "| **Corpus** | **Popular library using the corpus** | **Data availability** | \n",
    "|------|------|------|\n",
    "| [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42) | Stanford NLP | Extract in NLTK as detailed [here](https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537) | \n",
    "| OntoNotes 5 |  |  | \n",
    "| Groningen Meaning Bank | | Full data [here](https://gmb.let.rug.nl/data.php) - Extract found [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.4.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "#import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_NLP = 'C:\\\\Users\\\\Jb\\\\Desktop\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(path_to_NLP + '\\\\DL4NLP lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Groningen Meaning Bank extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
     ]
    }
   ],
   "source": [
    "df_GMB_extract = pd.read_csv(path_to_NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_GMB_extract_add = pd.read_csv(path_to_NLP + \"\\\\data\\\\Groningen Meaning Bank (extract)\\\\ner_dataset.csv\", encoding = \"ISO-8859-1\", error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_idx           word  pos\n",
       "0           1.0      Thousands  NNS\n",
       "1           1.0             of   IN\n",
       "2           1.0  demonstrators  NNS\n",
       "3           1.0           have  VBP\n",
       "4           1.0        marched  VBN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract.dropna(inplace = True)\n",
    "df_GMB_extract = df_GMB_extract[['sentence_idx', 'word', 'pos']]\n",
    "print(df_GMB_extract.shape)\n",
    "df_GMB_extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS\n",
       "0  Sentence: 1      Thousands  NNS\n",
       "1  Sentence: 1             of   IN\n",
       "2  Sentence: 1  demonstrators  NNS\n",
       "3  Sentence: 1           have  VBP\n",
       "4  Sentence: 1        marched  VBN"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GMB_extract_add.fillna(method = 'ffill', inplace = True)\n",
    "df_GMB_extract_add = df_GMB_extract_add[['Sentence #', 'Word', 'POS']]\n",
    "df_GMB_extract_add.replace('FW', 'NN', inplace = True)\n",
    "df_GMB_extract_add.head()\n",
    "df_GMB_extract_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "POSs = df_GMB_extract['pos'].unique().tolist()\n",
    "print(len(POSs))\n",
    "POSs2num = {tag : i for i, tag in enumerate(POSs)}\n",
    "num2POSs = {i : tag for i, tag in enumerate(POSs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = df_GMB_extract.groupby(\"sentence_idx\").apply(lambda s: [[w.lower() for w in s[\"word\"].values.tolist()], \n",
    "                                                                 [POSs2num[t] for t in s[\"pos\"].values.tolist()]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_add = df_GMB_extract_add.groupby(\"Sentence #\").apply(lambda s: [[w.lower() for w in s[\"Word\"].values.tolist()], \n",
    "                                                                       [POSs2num[t] for t in s[\"POS\"].values.tolist()]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_trn, corpus_tst = train_test_split(corpus, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_trn = [s_l[0] for s_l in corpus_trn if len(s_l[0]) <= 75]\n",
    "poslabels_trn = [s_l[1] for s_l in corpus_trn if len(s_l[1]) <= 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28141"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(top-seeded, 10, JJ)\n",
      "(lleyton, 4, NNP)\n",
      "(hewitt, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(australia, 4, NNP)\n",
      "(and, 9, CC)\n",
      "(number-two, 10, JJ)\n",
      "(dominik, 4, NNP)\n",
      "(hrbaty, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(slovakia, 4, NNP)\n",
      "(have, 2, VBP)\n",
      "(advanced, 3, VBN)\n",
      "(to, 5, TO)\n",
      "(the, 7, DT)\n",
      "(second, 10, JJ)\n",
      "(round, 8, NN)\n",
      "(of, 1, IN)\n",
      "(the, 7, DT)\n",
      "(international, 10, JJ)\n",
      "(men, 0, NNS)\n",
      "('s, 18, POS)\n",
      "(hardcourt, 8, NN)\n",
      "(tennis, 8, NN)\n",
      "(championships, 0, NNS)\n",
      "(in, 1, IN)\n",
      "(adelaide, 4, NNP)\n",
      "(,, 21, ,)\n",
      "(australia, 4, NNP)\n",
      "(., 11, .)\n",
      "(top-seeded, 10, JJ)\n",
      "(lleyton, 4, NNP)\n",
      "(hewitt, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(australia, 4, NNP)\n",
      "(and, 9, CC)\n",
      "(number-two, 10, JJ)\n",
      "(dominik, 4, NNP)\n",
      "(hrbaty, 4, NNP)\n",
      "(of, 1, IN)\n",
      "(slovakia, 4, NNP)\n",
      "(have, 2, VBP)\n",
      "(advanced, 3, VBN)\n",
      "(to, 5, TO)\n",
      "(the, 7, DT)\n",
      "(second, 10, JJ)\n",
      "(round, 8, NN)\n",
      "(of, 1, IN)\n",
      "(the, 7, DT)\n",
      "(international, 10, JJ)\n",
      "(men, 0, NNS)\n",
      "('s, 18, POS)\n",
      "(hardcourt, 8, NN)\n",
      "(tennis, 8, NN)\n",
      "(championships, 0, NNS)\n",
      "(in, 1, IN)\n",
      "(adelaide, 4, NNP)\n",
      "(,, 21, ,)\n",
      "(australia, 4, NNP)\n",
      "(., 11, .)\n"
     ]
    }
   ],
   "source": [
    "words = corpus_trn[0][0]\n",
    "tags  = corpus_trn[0][1]\n",
    "for word, tag in zip(words, tags) : print('(' + word + ', ' + str(tag)+ ', ' + str(num2POSs[tag]) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modules\n",
    "\n",
    "### 1.1 Word Embedding module\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "All details on Word Embedding modules and their pre-training are found in **Part I - 1**. We consider here a FastText model trained following the Skip-Gram training objective.\n",
    "\n",
    "#### $\\bullet$ FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec\n",
    "from libDL4NLP.models.Word_Embedding import Word2VecConnector\n",
    "from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec = FastText(size = 100, \n",
    "                    window = 5, \n",
    "                    min_count = 1, \n",
    "                    negative = 20,\n",
    "                    sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24652"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.build_vocab(sentences_trn)\n",
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec.train(sentences_trn,\n",
    "               epochs = 20,\n",
    "               total_examples = word2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ Word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def correctCorpus(word2vec, corpus, threshold = 0.9) :\n",
    "    new_sentences = []\n",
    "    corrections = []\n",
    "    for s in corpus :\n",
    "        new_s = []\n",
    "        for word in s[0] :\n",
    "            try : #fasttext raises an error if no character ngram seen during training appears in the word\n",
    "                if (word not in word2vec.wv.vocab and word2vec.most_similar(word)[0][1] >= threshold) :\n",
    "                    new_word = word2vec.most_similar(word)[0][0]\n",
    "                    new_s.append(new_word)\n",
    "                    corrections.append([word, new_word])\n",
    "                else :\n",
    "                    new_s.append(word)\n",
    "            except : \n",
    "                new_s.append(word)\n",
    "        new_sentences.append([new_s, s[1]])\n",
    "    return new_sentences, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrected_corpus_trn, corrections_trn = correctCorpus(word2vec, corpus_trn, threshold = 0.95)\n",
    "corrected_corpus_tst, corrections_tst = correctCorpus(word2vec, corpus_tst, threshold = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 274\n"
     ]
    }
   ],
   "source": [
    "print(len(corrections_trn), len(corrections_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gratification', 'ratification'],\n",
       " ['out-of-competition', 'competition'],\n",
       " ['out-of-competition', 'competition'],\n",
       " ['dennis', 'tennis'],\n",
       " ['444', '44'],\n",
       " ['olympian', 'olympia'],\n",
       " ['manufacturers', 'manufacturer'],\n",
       " ['eastward', 'westward'],\n",
       " ['constitutes', 'constitute'],\n",
       " ['mohamuud', 'mohamud'],\n",
       " ['mohamuud', 'mohamud'],\n",
       " ['catastrophes', 'catastrophe'],\n",
       " ['surface-to-air', 'surface-to-surface'],\n",
       " ['69,000', '9,000'],\n",
       " ['itv1', 'itv'],\n",
       " ['arafa', 'arafat'],\n",
       " ['c.i.a', 'h.i.v.'],\n",
       " ['rodrigues', 'rodriguez'],\n",
       " ['askari', 'askar'],\n",
       " ['askari', 'askar'],\n",
       " ['181', '1815'],\n",
       " ['ecosystem', 'system'],\n",
       " ['mehran', 'tehran'],\n",
       " ['27,000', '1,27,000'],\n",
       " ['27,000', '1,27,000'],\n",
       " ['stockpiles', 'stockpile'],\n",
       " ['h.i.v./aids', 'h.i.v.'],\n",
       " ['h.i.v./aids', 'h.i.v.'],\n",
       " ['identifications', 'identification'],\n",
       " ['1,90,000', '90,000'],\n",
       " ['hemorrhaging', 'hemorrhagic'],\n",
       " ['10-meter', '100-meter'],\n",
       " ['10-meter', '100-meter'],\n",
       " ['magazines', 'magazine'],\n",
       " ['magazines', 'magazine'],\n",
       " ['kohl', 'kohlu'],\n",
       " ['kohl', 'kohlu'],\n",
       " ['20-year-olds', '20-year-old'],\n",
       " ['3,30,000', '30,000'],\n",
       " ['guinean', 'guinea'],\n",
       " ['guinean', 'guinea'],\n",
       " ['shalikashvili', 'saakashvili'],\n",
       " ['225-member', '35-member'],\n",
       " ['intercepts', 'intercept'],\n",
       " ['intercepts', 'intercept'],\n",
       " ['blockades', 'blockade'],\n",
       " ['sharm-el-sheik', 'el-sheik'],\n",
       " ['sharm-el-sheik', 'el-sheik'],\n",
       " ['500-meter', '100-meter'],\n",
       " ['timoshenko', 'tymoshenko'],\n",
       " ['86th', '16th'],\n",
       " ['86th', '16th'],\n",
       " ['80-meter', '100-meter'],\n",
       " ['76th', '16th'],\n",
       " ['atttacks', 'attacks'],\n",
       " ['2.5-trillion', 'trillion'],\n",
       " ['zimbabweans', 'zimbabwean'],\n",
       " ['k.', 'a.k.'],\n",
       " ['jiang', 'zhejiang'],\n",
       " ['bioethics', 'ethics'],\n",
       " ['bioethics', 'ethics'],\n",
       " ['mr.yushchenko', 'yushchenko'],\n",
       " ['thoughtful', 'thought'],\n",
       " ['fortune', 'misfortune'],\n",
       " ['59-year-old', '79-year-old'],\n",
       " ['a.m.', 'p.m.'],\n",
       " ['a.m.', 'p.m.'],\n",
       " ['34,000', '14,000'],\n",
       " ['36,000', '6,000'],\n",
       " ['hyperinflation', 'inflation'],\n",
       " ['pinions', 'opinions'],\n",
       " ['9,60,000', '60,000'],\n",
       " ['1,12,000', '12,000'],\n",
       " ['1,12,000', '12,000'],\n",
       " ['1822', '1821'],\n",
       " ['46-member', '55-member'],\n",
       " ['46-member', '55-member'],\n",
       " ['administrations', 'administration'],\n",
       " ['partisan', 'bipartisan'],\n",
       " ['a.m.', 'p.m.'],\n",
       " ['yudhyono', 'yudhoyono'],\n",
       " ['yudhyono', 'yudhoyono'],\n",
       " ['yoadimadji', 'yoadimnadji'],\n",
       " ['kravchenko', 'yushchenko'],\n",
       " ['kravchenko', 'yushchenko'],\n",
       " ['05-apr', '06-apr'],\n",
       " ['vindicated', 'indicated'],\n",
       " ['azerbaijani', 'azerbaijan'],\n",
       " ['nano-technology', 'technology'],\n",
       " ['6,50,000', '7,50,000'],\n",
       " ['w.t.o', 'w.'],\n",
       " ['25-member', '55-member'],\n",
       " ['phosphates', 'phosphate'],\n",
       " ['1830', '1839'],\n",
       " ['29-year-old', '49-year-old'],\n",
       " ['98,000', '8,000'],\n",
       " ['ineligible', 'eligible'],\n",
       " ['daniele', 'daniel'],\n",
       " ['51.8', '1.8'],\n",
       " ['subterranean', 'mediterranean'],\n",
       " ['subterranean', 'mediterranean'],\n",
       " ['181', '1815'],\n",
       " ['hospitalizations', 'hospitalization'],\n",
       " ['22-kilometer', 'kilometer'],\n",
       " ['40-member', '120-member'],\n",
       " ['non-representative', 'representative'],\n",
       " ['leonidas', 'leonid'],\n",
       " ['co-defendant', 'co-defendants'],\n",
       " ['djibouti-yemen', 'djibouti'],\n",
       " ['environment-themed', 'environment'],\n",
       " ['4x5-kilometer', 'kilometer'],\n",
       " ['talib', 'taliban'],\n",
       " ['rude', 'crude'],\n",
       " ['60-year', '40-year'],\n",
       " ['60-year', '40-year'],\n",
       " ['1830', '1839'],\n",
       " ['promoter', 'promote'],\n",
       " ['1.65', '1.6'],\n",
       " [\"ha'aretz\", 'haaretz'],\n",
       " ['20-meter', '100-meter'],\n",
       " ['30-meter', '100-meter'],\n",
       " ['20-meter', '100-meter'],\n",
       " ['30-meter', '100-meter'],\n",
       " ['hyperinflation', 'inflation'],\n",
       " ['thirty-one-year-old', 'one-year-old'],\n",
       " ['thirty-one-year-old', 'one-year-old'],\n",
       " ['400-kilometer', 'kilometer'],\n",
       " ['dmitrivev', 'dmitri'],\n",
       " ['10-member', '120-member'],\n",
       " ['75-member', '275-member'],\n",
       " ['75-member', '275-member'],\n",
       " ['unconstitutional', 'constitutional'],\n",
       " ['225-member', '35-member'],\n",
       " ['pro-environment', 'environment'],\n",
       " ['constitutes', 'constitute'],\n",
       " ['gabashvili', 'saakashvili'],\n",
       " ['gabashvili', 'saakashvili'],\n",
       " ['baqubah', 'baquba'],\n",
       " ['baqubah', 'baquba'],\n",
       " ['4,10,000', '2,10,000'],\n",
       " ['uranium-enriching', 'enriching'],\n",
       " ['hemispheres', 'hemisphere'],\n",
       " ['suicide-bomber', 'suicide'],\n",
       " ['suicide-bomber', 'suicide'],\n",
       " ['barzan', 'barzani'],\n",
       " ['overborrowing', 'borrowing'],\n",
       " ['unsustainable', 'sustainable'],\n",
       " ['unsustainable', 'sustainable'],\n",
       " ['state-of-the-union', 'state-of-the-art'],\n",
       " ['365-5', '36'],\n",
       " ['365-5', '36'],\n",
       " ['450-member', '120-member'],\n",
       " ['silvan', 'silva'],\n",
       " ['silvan', 'silva'],\n",
       " ['de-facto', 'de~facto'],\n",
       " ['fitness', 'witness'],\n",
       " ['fitness', 'witness'],\n",
       " ['135.6', '135'],\n",
       " ['135.6', '135'],\n",
       " ['a-h1n1', 'h1n1'],\n",
       " ['1.66', '1.6'],\n",
       " ['cargolux', 'cargo'],\n",
       " ['1,74,000', '4,000'],\n",
       " ['1,74,000', '4,000'],\n",
       " ['14-year-old', '64-year-old'],\n",
       " ['19,000', '9,000'],\n",
       " ['19,000', '9,000'],\n",
       " ['singer-songwriter', 'songwriter'],\n",
       " ['yisrael', 'israel'],\n",
       " ['01-jun', '07-jun'],\n",
       " ['01-jun', '07-jun'],\n",
       " ['us-cambodia', 'cambodia'],\n",
       " ['56-year-old', '36-year-old'],\n",
       " ['60-kilometer', 'kilometer'],\n",
       " ['83-year-old', '23-year-old'],\n",
       " ['massacres', 'massacre'],\n",
       " ['1.48', '1.4'],\n",
       " ['krasnoyarsk', 'krasniqi'],\n",
       " ['hemorrhages', 'hemorrhage'],\n",
       " ['hemorrhages', 'hemorrhage'],\n",
       " ['zheijiang', 'zhejiang'],\n",
       " ['zheijiang', 'zhejiang'],\n",
       " ['tulkarm', 'tulkarem'],\n",
       " ['tulkarm', 'tulkarem'],\n",
       " ['2,80,000', '1,80,000'],\n",
       " ['2,30,000', '30,000'],\n",
       " ['ac-130', 'c-130'],\n",
       " ['post-assassination', 'assassination'],\n",
       " ['misfortunes', 'misfortune'],\n",
       " ['wenesday', 'wednesday'],\n",
       " ['22,000', '2,000'],\n",
       " ['70-year', '40-year'],\n",
       " ['daniela', 'daniel'],\n",
       " ['36th', '16th'],\n",
       " ['102-member', '55-member'],\n",
       " ['1055', '105'],\n",
       " ['1055', '105'],\n",
       " ['ldp', 'gdp'],\n",
       " ['ldp', 'gdp'],\n",
       " ['re-deployment', 'deployment'],\n",
       " ['vexation', 'taxation'],\n",
       " ['b.', 'b.c.'],\n",
       " ['lynn', 'lynndie'],\n",
       " ['subur', 'suburb'],\n",
       " ['subur', 'suburb'],\n",
       " ['8-year-old', '48-year-old'],\n",
       " ['95,000', '55,000'],\n",
       " ['netanya', 'netanyahu'],\n",
       " ['netanya', 'netanyahu'],\n",
       " ['nicaraguans', 'nicaraguan'],\n",
       " ['nicaraguans', 'nicaraguan'],\n",
       " ['tightens', 'tighten'],\n",
       " ['unsustainable', 'sustainable'],\n",
       " ['unsustainable', 'sustainable'],\n",
       " ['fnj', 'fnl'],\n",
       " ['22.5', '2.5'],\n",
       " ['indifferent', 'different'],\n",
       " ['fallujans', 'fallujah'],\n",
       " ['fallujans', 'fallujah'],\n",
       " ['sino-tibetan', 'tibetan'],\n",
       " ['sino-tibetan', 'tibetan'],\n",
       " ['lankans', 'lankan'],\n",
       " ['authenticate', 'authenticated'],\n",
       " ['co-called', 'so-called'],\n",
       " ['decommission', 'commission'],\n",
       " ['eco-tourism', 'tourism'],\n",
       " ['ecosystem', 'system'],\n",
       " ['ransoms', 'ransom'],\n",
       " ['1657', '165'],\n",
       " ['1657', '165'],\n",
       " ['michaela', 'michael'],\n",
       " ['eco-tourism', 'tourism'],\n",
       " ['sub-continent', 'continent'],\n",
       " ['saudis', 'saudi'],\n",
       " ['71-year-old', '31-year-old'],\n",
       " ['17th-century', 'ninth-century'],\n",
       " ['17th-century', 'ninth-century'],\n",
       " ['30-kilometer', 'kilometer'],\n",
       " ['drug-trafficking', 'trafficking'],\n",
       " ['re-liberation', 'liberation'],\n",
       " ['48,000', '8,000'],\n",
       " ['lynn', 'lynndie'],\n",
       " ['altogether', 'together'],\n",
       " ['altogether', 'together'],\n",
       " ['suyono', 'yudhoyono'],\n",
       " ['suyono', 'yudhoyono'],\n",
       " ['yugansk', 'yuganskneftegaz'],\n",
       " ['37.5', '7.5'],\n",
       " ['37.5', '7.5'],\n",
       " ['sumatran', 'sumatra'],\n",
       " ['coffins', 'coffin'],\n",
       " ['1838', '1839'],\n",
       " [\"ma'ariv\", 'maariv'],\n",
       " ['co-chairman', 'chairman'],\n",
       " ['uncertainties', 'uncertainty'],\n",
       " ['cheikh', 'sheikh'],\n",
       " ['husbandry', 'husband'],\n",
       " ['200-kilometer', 'kilometer'],\n",
       " ['200-kilometer', 'kilometer'],\n",
       " ['yugoslavian', 'yugoslavia'],\n",
       " ['manufactures', 'manufacture'],\n",
       " ['cctv', 'rctv'],\n",
       " ['overwhelmed', 'overwhelming'],\n",
       " ['2,68,000', '8,000'],\n",
       " ['unaffected', 'affected'],\n",
       " ['umbrella-like', 'ebola-like'],\n",
       " ['umbrella-like', 'ebola-like'],\n",
       " ['aid-distribution', 'distribution'],\n",
       " ['2,60,000', '60,000'],\n",
       " ['2,60,000', '60,000'],\n",
       " ['co-defendant', 'co-defendants'],\n",
       " ['45-year-old', '55-year-old'],\n",
       " ['45-year-old', '55-year-old'],\n",
       " ['56-year-old', '36-year-old'],\n",
       " ['ex-yukos', 'yukos'],\n",
       " ['hurriya', 'hurriyat'],\n",
       " ['1386', '386'],\n",
       " ['bangladeshis', 'bangladeshi'],\n",
       " ['unconstitutional', 'constitutional'],\n",
       " ['e-commerce', 'commerce'],\n",
       " ['e-commerce', 'commerce'],\n",
       " ['phoenixes', 'phoenix'],\n",
       " ['phoenixes', 'phoenix'],\n",
       " ['325-member', '55-member'],\n",
       " ['325-member', '55-member'],\n",
       " ['fixation', 'taxation'],\n",
       " ['fixation', 'taxation'],\n",
       " ['eu-latin', 'latin'],\n",
       " ['eu-latin', 'latin'],\n",
       " ['61,000', '31,000'],\n",
       " ['61,000', '31,000'],\n",
       " ['92,000', '2,000'],\n",
       " ['congratulation', 'congratulate'],\n",
       " ['87,000', '7,000'],\n",
       " ['1.36', '1.3'],\n",
       " ['1.36', '1.3'],\n",
       " ['rafiqi', 'rafiq'],\n",
       " ['rafiqi', 'rafiq'],\n",
       " ['clunkers', 'bunkers'],\n",
       " ['j.', 'p.j.'],\n",
       " ['j.', 'p.j.'],\n",
       " ['69,000', '9,000'],\n",
       " ['43-year-old', '23-year-old'],\n",
       " ['5,50,000', '7,50,000'],\n",
       " ['f-16s', 'f-16'],\n",
       " ['yuschenko', 'yushchenko'],\n",
       " ['yuschenko', 'yushchenko'],\n",
       " ['massacring', 'massacre'],\n",
       " ['300-kilometer', 'kilometer'],\n",
       " ['gholam', 'gholamreza'],\n",
       " ['65-year-old', '55-year-old'],\n",
       " ['65-year-old', '55-year-old'],\n",
       " ['abdirahman', 'rahman'],\n",
       " ['lashkar-e-taiba', 'lashkar-e-toiba'],\n",
       " ['vindicates', 'indicates'],\n",
       " ['vindicates', 'indicates'],\n",
       " ['bangladeshis', 'bangladeshi'],\n",
       " ['dp', 'gdp'],\n",
       " ['abdelrahman', 'abdel-rahman'],\n",
       " ['altar', 'gibraltar'],\n",
       " ['altar', 'gibraltar'],\n",
       " ['f-4', 'f-16'],\n",
       " ['iskandariya', 'iskandariyah'],\n",
       " ['gholam', 'gholamreza'],\n",
       " ['57-member', '55-member'],\n",
       " ['5.1-magnitude', 'magnitude'],\n",
       " [\"d'\", \"d'ivoire\"],\n",
       " [\"d'\", \"d'ivoire\"],\n",
       " ['60-day', '40-day'],\n",
       " ['43,000', '3,000'],\n",
       " ['43,000', '3,000'],\n",
       " ['xdr', 'xdr-tb'],\n",
       " ['participates', 'participate'],\n",
       " ['82,000', '2,000'],\n",
       " ['1,76,000', '6,000'],\n",
       " ['1,28,000', '28,000'],\n",
       " ['1,76,000', '6,000'],\n",
       " ['1,28,000', '28,000'],\n",
       " ['37-degree', 'degree'],\n",
       " ['37-degree', 'degree'],\n",
       " ['computer-chip', 'computer'],\n",
       " ['02-feb', '07-feb'],\n",
       " ['abnormally', 'normally'],\n",
       " ['abnormally', 'normally'],\n",
       " ['nicaraguans', 'nicaraguan'],\n",
       " ['nicaraguans', 'nicaraguan'],\n",
       " ['2.43', '2.4'],\n",
       " ['2.43', '2.4'],\n",
       " ['lieutenants', 'lieutenant'],\n",
       " ['90-year-old', '80-year-old'],\n",
       " ['90-year-old', '80-year-old'],\n",
       " ['29-year-old', '49-year-old'],\n",
       " ['dmitrij', 'dmitri'],\n",
       " ['co-educational', 'educational'],\n",
       " ['co-educational', 'educational'],\n",
       " ['a.m.', 'p.m.'],\n",
       " ['noneducational', 'educational'],\n",
       " ['overwhelmed', 'overwhelming'],\n",
       " ['1644', '1648'],\n",
       " ['silvan', 'silva'],\n",
       " ['nonproliferation', 'proliferation'],\n",
       " ['nonproliferation', 'proliferation'],\n",
       " ['valleys', 'valley'],\n",
       " ['undemocratic', 'democratic'],\n",
       " ['5-member', '55-member'],\n",
       " ['5-member', '55-member'],\n",
       " ['03-apr', '06-apr'],\n",
       " ['444', '44'],\n",
       " ['saudis', 'saudi'],\n",
       " ['83rd', '23rd'],\n",
       " ['5,49,000', '9,000'],\n",
       " ['5,49,000', '9,000'],\n",
       " ['33-member', '55-member'],\n",
       " ['33-member', '55-member'],\n",
       " ['uncertainties', 'uncertainty'],\n",
       " ['peugeot-citroen', 'peugeot'],\n",
       " ['39,000', '9,000'],\n",
       " ['massacres', 'massacre'],\n",
       " ['dp', 'gdp'],\n",
       " ['euro-mediterranean', 'mediterranean'],\n",
       " ['kezerashvili', 'saakashvili'],\n",
       " ['soo-hyuck', 'moo-hyun'],\n",
       " ['news-washington', 'washington'],\n",
       " ['news-washington', 'washington'],\n",
       " ['07-apr', '06-apr'],\n",
       " ['50-year-old', '80-year-old'],\n",
       " ['pre-olympic', 'olympic'],\n",
       " ['2,30,000', '30,000'],\n",
       " ['reorganization', 'organization'],\n",
       " ['1,07,000', '1,27,000'],\n",
       " ['1,07,000', '1,27,000'],\n",
       " ['25-member', '55-member'],\n",
       " ['ziyad', 'iyad'],\n",
       " ['10-member', '120-member'],\n",
       " ['uruguayan', 'uruguay'],\n",
       " ['bezhuashvili', 'saakashvili'],\n",
       " ['bezhuashvili', 'saakashvili'],\n",
       " ['al-zarqwai', 'al-zarqawi'],\n",
       " ['raufi', 'rauf'],\n",
       " ['biographical', 'geographical'],\n",
       " ['muga', 'mugabe'],\n",
       " ['al-dulaymi', 'al-dulaimi'],\n",
       " ['al-dulaymi', 'al-dulaimi'],\n",
       " ['15-kilometer', 'kilometer'],\n",
       " ['npp', 'ppp'],\n",
       " ['npp', 'ppp'],\n",
       " ['wednesdays', 'wednesday'],\n",
       " ['bi-partisan', 'bipartisan'],\n",
       " ['agriculture-led', 'agriculture'],\n",
       " ['agriculture-led', 'agriculture'],\n",
       " ['123-kilometer', 'kilometer'],\n",
       " ['123-kilometer', 'kilometer'],\n",
       " ['dahoud', 'lahoud'],\n",
       " ['flu-like', 'ebola-like'],\n",
       " ['yuganskneftgaz', 'yuganskneftegaz'],\n",
       " ['86-year-old', '26-year-old'],\n",
       " ['bosnian-serb', 'bosnian'],\n",
       " ['dc', 'mdc'],\n",
       " ['09-feb', '07-feb'],\n",
       " ['xijing', 'beijing'],\n",
       " ['200-meter', '100-meter'],\n",
       " ['silvan', 'silva'],\n",
       " ['500-meter', '100-meter'],\n",
       " ['gustavo', 'gustav'],\n",
       " ['mr.yushchenko', 'yushchenko'],\n",
       " ['14.5', '4.5'],\n",
       " ['ansari', 'ansar'],\n",
       " ['ansari', 'ansar'],\n",
       " ['148.9', '148'],\n",
       " ['timoshenko', 'tymoshenko'],\n",
       " ['mohammadi', 'mohammad'],\n",
       " ['bildnewspaper', 'newspaper'],\n",
       " ['3,72,000', '2,000'],\n",
       " ['mfdc', 'mdc'],\n",
       " ['tayyeb', 'tayyip'],\n",
       " ['1,17,000', '17,000'],\n",
       " ['10-kilometer', 'kilometer'],\n",
       " ['8,88,000', '8,000'],\n",
       " ['8,88,000', '8,000'],\n",
       " ['mid-february', 'february'],\n",
       " ['neo-liberal', 'liberal'],\n",
       " ['maarib', 'maariv'],\n",
       " ['maarib', 'maariv'],\n",
       " ['20-kilometer', 'kilometer'],\n",
       " ['kirk', 'kirkuk'],\n",
       " ['04-feb', '07-feb'],\n",
       " ['04-feb', '07-feb'],\n",
       " ['1881', '1885'],\n",
       " ['ever-increasing', 'increasing'],\n",
       " ['ever-increasing', 'increasing'],\n",
       " ['ecotourism', 'tourism'],\n",
       " ['49.5', '9.5'],\n",
       " ['patriarchal', 'patriarch'],\n",
       " ['96,000', '6,000'],\n",
       " ['96,000', '6,000'],\n",
       " ['anglo-egyptian', 'egyptian'],\n",
       " ['6th', '16th'],\n",
       " ['6th', '16th'],\n",
       " ['57-member', '55-member'],\n",
       " ['al-mashhadani', 'al-masri'],\n",
       " ['dennis', 'tennis'],\n",
       " ['heptathlon', 'biathlon'],\n",
       " ['structur', 'structure'],\n",
       " ['cheikh', 'sheikh'],\n",
       " ['ex-communist', 'communist'],\n",
       " ['kilometers-per-hour', 'kilometer-per-hour'],\n",
       " ['03-apr', '06-apr'],\n",
       " ['1860', '1863'],\n",
       " ['300-member', '120-member'],\n",
       " ['kashmirs', 'kashmir'],\n",
       " ['ill-treatment', 'treatment'],\n",
       " ['5.7-kilometer', 'kilometer'],\n",
       " ['5.7-kilometer', 'kilometer'],\n",
       " ['58.5', '8.5'],\n",
       " ['ex-communist', 'communist'],\n",
       " ['ex-communist', 'communist'],\n",
       " ['41-year-old', '31-year-old'],\n",
       " ['prodemocracy', 'democracy'],\n",
       " ['prodemocracy', 'democracy'],\n",
       " ['falluja', 'fallujah'],\n",
       " ['sub-antarctic', 'antarctic'],\n",
       " ['gmt', 'gm'],\n",
       " ['volcanos', 'volcano'],\n",
       " ['risk-management', 'management'],\n",
       " ['33-year-old', '23-year-old'],\n",
       " ['33-year-old', '23-year-old'],\n",
       " ['refusals', 'refusal'],\n",
       " ['clunkers', 'bunkers'],\n",
       " ['procedural', 'procedure'],\n",
       " ['coordinates', 'coordinate'],\n",
       " ['magnitude-7.0', 'magnitude'],\n",
       " ['go-betweens', 'between'],\n",
       " ['60-year-old', '80-year-old'],\n",
       " ['64th', '14th'],\n",
       " ['72-hour', '12-hour'],\n",
       " ['quarter-on-quarter', 'quarter'],\n",
       " ['quarter-on-quarter', 'quarter'],\n",
       " ['lankans', 'lankan'],\n",
       " ['12.5-kilometer', 'kilometer'],\n",
       " ['massacring', 'massacre'],\n",
       " ['miscommunication', 'communication'],\n",
       " ['miscommunication', 'communication'],\n",
       " ['rajapakshe', 'rajapakse'],\n",
       " ['rajapakshe', 'rajapakse'],\n",
       " ['pro-hezbollah', 'hezbollah'],\n",
       " ['pro-hezbollah', 'hezbollah'],\n",
       " ['publishers', 'publisher'],\n",
       " ['74,000', '4,000'],\n",
       " ['74,000', '4,000'],\n",
       " ['50-meter', '100-meter'],\n",
       " ['bulky', 'bulk'],\n",
       " ['bulky', 'bulk'],\n",
       " ['benedicto', 'benedict'],\n",
       " ['4th', '14th'],\n",
       " ['41-year-old', '31-year-old'],\n",
       " ['interceptors', 'interceptor'],\n",
       " ['unprofessional', 'professional'],\n",
       " ['unprofessional', 'professional'],\n",
       " ['9-percent', 'percent'],\n",
       " ['interceptors', 'interceptor'],\n",
       " ['sassou', 'sassou-nguesso'],\n",
       " ['nguesso', 'sassou-nguesso'],\n",
       " ['sassou', 'sassou-nguesso'],\n",
       " ['nguesso', 'sassou-nguesso'],\n",
       " ['315-member', '15-member'],\n",
       " ['antonin', 'antonio'],\n",
       " ['29-member', '55-member'],\n",
       " ['re-liberation', 'liberation'],\n",
       " ['barzan', 'barzani'],\n",
       " ['1621', '162'],\n",
       " ['1621', '162'],\n",
       " ['un-negotiated', 'negotiated'],\n",
       " ['24,000', '4,000'],\n",
       " ['crucifixes', 'crucifixion'],\n",
       " ['crucifixes', 'crucifixion'],\n",
       " ['berkovsky', 'khodorkovsky'],\n",
       " ['106.5', '106'],\n",
       " ['14-hour', '24-hour'],\n",
       " ['1.65', '1.6'],\n",
       " ['shenzhen', 'shenzhou'],\n",
       " ['drug-trafficking', 'trafficking'],\n",
       " ['hydrocarbon', 'hydrocarbons'],\n",
       " ['hydrocarbon', 'hydrocarbons'],\n",
       " ['1828', '1821'],\n",
       " ['azerbaijani', 'azerbaijan'],\n",
       " ['3rd', '23rd'],\n",
       " ['60-day', '40-day'],\n",
       " ['5,500-member', '120-member'],\n",
       " ['daniele', 'daniel'],\n",
       " ['moldovan', 'moldova'],\n",
       " ['47,000', '7,000'],\n",
       " ['47,000', '7,000'],\n",
       " ['reorganize', 'organize'],\n",
       " ['reorganize', 'organize'],\n",
       " ['5\\xa0storm', 'storm'],\n",
       " ['hajj', 'hajdib'],\n",
       " ['mid-february', 'february'],\n",
       " ['14-year-old', '64-year-old'],\n",
       " ['m.', 'p.m.'],\n",
       " ['m.', 'p.m.'],\n",
       " ['taras', 'tarasyuk'],\n",
       " ['husseinov', 'hussein'],\n",
       " ['husseinov', 'hussein'],\n",
       " ['yousseff', 'youssef'],\n",
       " ['yousseff', 'youssef'],\n",
       " ['ex-husband', 'husband'],\n",
       " ['ex-husband', 'husband'],\n",
       " ['2,46,000', '46,000'],\n",
       " ['jammu-kashmir', 'kashmir'],\n",
       " ['boycotts', 'boycott'],\n",
       " ['boycotts', 'boycott'],\n",
       " ['f-5', 'f-16'],\n",
       " ['f-5', 'f-16'],\n",
       " ['82,000', '2,000'],\n",
       " ['krzyzewski', 'skubiszewski'],\n",
       " ['krzyzewski', 'skubiszewski'],\n",
       " ['constitutions', 'constitution'],\n",
       " ['constitutions', 'constitution'],\n",
       " ['utiashvili', 'saakashvili'],\n",
       " ['120-seat', '450-seat'],\n",
       " ['27.5', '7.5'],\n",
       " ['ransoms', 'ransom'],\n",
       " ['ex-soviet', 'soviet'],\n",
       " ['ex-soviet', 'soviet'],\n",
       " ['53,000', '3,000'],\n",
       " ['6,10,000', '2,10,000'],\n",
       " ['57.6', '7.6'],\n",
       " ['reintegration', 'integration'],\n",
       " ['1.32', '1.3'],\n",
       " ['188', '1885'],\n",
       " ['non-manufacturing', 'manufacturing'],\n",
       " ['biotechnology', 'technology'],\n",
       " ['nano-technology', 'technology'],\n",
       " ['occidental', 'accidental'],\n",
       " ['10-kilometer', 'kilometer'],\n",
       " ['737s', '737'],\n",
       " ['150-year-old', '80-year-old'],\n",
       " ['150-year-old', '80-year-old'],\n",
       " ['72-year-old', '42-year-old'],\n",
       " ['72-year-old', '42-year-old'],\n",
       " ['six-to-11-year-old', '11-year-old'],\n",
       " ['sawt', 'saw'],\n",
       " ['al-jihad', 'jihad'],\n",
       " ['otalvaro', 'alvaro'],\n",
       " ['undeveloped', 'developed'],\n",
       " ['undeveloped', 'developed'],\n",
       " [\"murphy-o'connor\", \"o'connor\"],\n",
       " [\"murphy-o'connor\", \"o'connor\"],\n",
       " ['record-highs', 'record-high'],\n",
       " ['record-highs', 'record-high'],\n",
       " ['bavarian', 'ovarian'],\n",
       " ['bavarian', 'ovarian'],\n",
       " ['maxim', 'maximum'],\n",
       " ['misfortunes', 'misfortune'],\n",
       " ['californian', 'california'],\n",
       " ['100-year-old', '80-year-old'],\n",
       " ['prehistoric', 'historic'],\n",
       " ['adumin', 'adumim'],\n",
       " ['adumin', 'adumim'],\n",
       " ['yugansk', 'yuganskneftegaz'],\n",
       " ['phosphates', 'phosphate'],\n",
       " ['phosphates', 'phosphate'],\n",
       " ['hanjiang', 'xinjiang'],\n",
       " ['hanjiang', 'xinjiang'],\n",
       " ['73rd', '23rd'],\n",
       " ['73rd', '23rd'],\n",
       " ['23.7', '3.7'],\n",
       " ['71,000', '1,000'],\n",
       " ['71,000', '1,000'],\n",
       " ['h.w', 'h.w.'],\n",
       " ['10.8', '10.5'],\n",
       " ['2.38.46', '2.3'],\n",
       " ['2.38.46', '2.3']]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections_trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Contextualization module\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from libDL4NLP.modules import RecurrentEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Part Of Speech Tagging Model\n",
    "\n",
    "[Back to top](#plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class POSTagger(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 n_class = 2,\n",
    "                 dropout = 0,\n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(POSTagger, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, n_class)\n",
    "        self.act       = F.softmax\n",
    "        self.n_class   = n_class\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index_in  = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.ignore_index_out = n_class\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index_out, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, n_class)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length,  n_class)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        return [(w, i) for w, i in zip(words, inds)]\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "            \n",
    "        def sentence2indices(words) :\n",
    "            # convert to indices\n",
    "            inds = [self.word2vec.lang.getIndex(w) for w in words]\n",
    "            inds = [i for i in inds if i is not None]\n",
    "            # apply mask\n",
    "            mask = [m for m, i in enumerate(inds) if i != self.word2vec.lang.getIndex('UNK')]\n",
    "            mask = random.sample(mask, k = int(mask_ratio*(len(mask) +1)))\n",
    "            inds = [maskInput(i, m in mask) for m, i in enumerate(inds)]\n",
    "            return inds\n",
    "        \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s[0]), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = [[sentence2indices(s[0]), s[1]] for s in sentences[i:i + batch_size]]\n",
    "            pack.sort(key = lambda p : len(p[0]), reverse = True)\n",
    "            pack0 = [p[0] for p in pack] \n",
    "            pack0 = list(itertools.zip_longest(*pack0, fillvalue = self.ignore_index_in))\n",
    "            pack0 = Variable(torch.LongTensor(pack0).transpose(0, 1)) # size (batch_size, max_length)\n",
    "            lengths = torch.tensor([len(p[0]) for p in pack])         # size (batch_size)\n",
    "            pack1 = [p[1] for p in pack]                              # size (batch_size, max_length)\n",
    "            pack1 = list(itertools.zip_longest(*pack1, fillvalue = self.ignore_index_out))\n",
    "            pack1 = Variable(torch.LongTensor(pack1).transpose(0, 1)) # size (batch_size, max_length) \n",
    "            packed_data.append([[pack0, lengths], pack1])\n",
    "        return packed_data\n",
    "\n",
    "    # compute model perf\n",
    "    def compute_accuracy(self, sentences, batch_size = 32) :\n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeSuccess(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return success, total\n",
    "        \n",
    "        # --- main ----\n",
    "        self.eval()\n",
    "        batches = self.generatePackedSentences(sentences, batch_size)\n",
    "        score, total = 0, 0\n",
    "        for batch, targets in batches :\n",
    "            log_probs = computeLogProbs(batch).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets = targets.to(self.device)                  # dim = (batch_size, input_length)\n",
    "            s, t = computeSuccess(log_probs, targets)\n",
    "            score += s\n",
    "            total += t\n",
    "        return score * 100 / total\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, \n",
    "            iters = None, epochs = None, lr = 0.025, random_state = 42, \n",
    "            print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs  = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets    = batch[1].to(self.device)                  # dim = (batch_size, input_length)\n",
    "            loss       = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if compute_accuracy :\n",
    "                accuracy = computeAccuracy(log_probs, targets)\n",
    "            else : \n",
    "                accuracy = 0\n",
    "            error = float(loss.item() / np.sum(targets.data.cpu().numpy() != self.ignore_index_out))\n",
    "            return error, accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SequenceLabeller(nn.Module) :\n",
    "    def __init__(self, device, tokenizer, word2vec, \n",
    "                 hidden_dim = 100, \n",
    "                 n_layers = 1, \n",
    "                 n_class = 2,\n",
    "                 dropout = 0,\n",
    "                 class_weights = None, \n",
    "                 optimizer = optim.SGD\n",
    "                 ):\n",
    "        super(SequenceLabeller, self).__init__()\n",
    "        \n",
    "        # embedding\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2vec  = word2vec\n",
    "        self.context   = RecurrentEncoder(self.word2vec.output_dim, hidden_dim, n_layers, dropout, bidirectional = True)\n",
    "        self.out       = nn.Linear(self.context.output_dim, n_class)\n",
    "        self.act       = F.softmax\n",
    "        self.n_class   = n_class\n",
    "        \n",
    "        # optimizer\n",
    "        self.ignore_index_in  = self.word2vec.lang.getIndex('PADDING_WORD')\n",
    "        self.ignore_index_out = n_class\n",
    "        self.criterion = nn.NLLLoss(size_average = False, \n",
    "                                    ignore_index = self.ignore_index_out, \n",
    "                                    weight = class_weights)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def nbParametres(self) :\n",
    "        return sum([p.data.nelement() for p in self.parameters() if p.requires_grad == True])\n",
    "    \n",
    "    def predict_proba(self, words):\n",
    "        embeddings = self.word2vec.twin(words, self.device) # dim = (1, input_length, hidden_dim)\n",
    "        hiddens, _ = self.context(embeddings)               # dim = (1, input_length, hidden_dim)\n",
    "        probs      = self.act(self.out(hiddens), dim = 2)   # dim = (1, input_length, n_class)\n",
    "        return probs\n",
    "\n",
    "    # main method\n",
    "    def forward(self, sentence = '.', color = '\\033[94m'):\n",
    "        def addColor(w1, w2, color) : return color + w2 + '\\033[0m' if w1 != w2 else w2\n",
    "        words  = self.tokenizer(sentence)\n",
    "        probs  = self.predict_proba(words).squeeze(0) # dim = (input_length,  n_class)\n",
    "        inds   = [probs[i].data.topk(1)[1].item() for i in range(probs.size(0))]\n",
    "        return [(w, i) for w, i in zip(words, inds)]\n",
    "\n",
    "    # load data\n",
    "    def generatePackedSentences(self, \n",
    "                                sentences, \n",
    "                                batch_size = 32, \n",
    "                                mask_ratio = 0,\n",
    "                                seed = 42) :\n",
    "        def maskInput(index, b) :\n",
    "            if   b and random.random() > 0.25 : return self.word2vec.lang.getIndex('UNK')\n",
    "            elif b and random.random() > 0.10 : return random.choice(list(self.word2vec.twin.lang.word2index.values()))\n",
    "            else                              : return index\n",
    "\n",
    "        def maskOutput(index, b) :\n",
    "            return index if b else self.ignore_index_out\n",
    "            \n",
    "        def sentence2indices(words) :\n",
    "            # convert to indices\n",
    "            inds = [self.word2vec.lang.getIndex(w) for w in words]\n",
    "            inds = [i for i in inds if i is not None]\n",
    "            # apply mask\n",
    "            mask = [m for m, i in enumerate(inds) if i != self.word2vec.lang.getIndex('UNK')]\n",
    "            mask = random.sample(mask, k = int(mask_ratio*(len(mask) +1)))\n",
    "            inds = [maskInput(i, m in mask) for m, i in enumerate(inds)]\n",
    "            return inds\n",
    "        \n",
    "        random.seed(seed)\n",
    "        sentences.sort(key = lambda s: len(s[0]), reverse = True)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(sentences), batch_size) :\n",
    "            pack = sentences[i:i + batch_size]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w) for w in p[0]] for p in pack]\n",
    "            pack1 = [p[1] for p in pack]\n",
    "            # prepare mask\n",
    "            mask_xl = [[i for i, w in enumerate(p) if w != self.word2vec.lang.getIndex('UNK')] for p in pack0]\n",
    "            mask_xs = [random.sample(m, k = int(mask_ratio*(len(m) +1))) for m in mask_xl]\n",
    "            # prepare input and target packs\n",
    "            pack0    = [[ maskInput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack0, mask_xs)]\n",
    "            pack1_xs = [[maskOutput(s[i], i in m) for i in range(len(s))] for s, m in zip(pack1, mask_xs)]\n",
    "            pack1_xl = pack1\n",
    "            lengths  = torch.tensor([len(p) for p in pack0]) # size = (batch_size) \n",
    "            # padd\n",
    "            pack0    = list(itertools.zip_longest(*pack0,    fillvalue = self.ignore_index_in )) \n",
    "            pack1_xs = list(itertools.zip_longest(*pack1_xs, fillvalue = self.ignore_index_out))\n",
    "            pack1_xl = list(itertools.zip_longest(*pack1_xl, fillvalue = self.ignore_index_out))\n",
    "            # turn into torch variables\n",
    "            pack0    = Variable(torch.LongTensor(pack0   ).transpose(0, 1))   # size = (batch_size, max_length)\n",
    "            pack1_xs = Variable(torch.LongTensor(pack1_xs).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            pack1_xl = Variable(torch.LongTensor(pack1_xl).transpose(0, 1))   # size = (batch_size, max_length) \n",
    "            # store pack\n",
    "            packed_data.append([[pack0, lengths], [pack1_xs, pack1_xl]])\n",
    "        return packed_data\n",
    "\n",
    "    # compute model perf\n",
    "    def compute_accuracy(self, sentences, batch_size = 32) :\n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeSuccess(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return success, total\n",
    "        \n",
    "        # --- main ----\n",
    "        self.eval()\n",
    "        batches = self.generatePackedSentences(sentences, batch_size)\n",
    "        score, total = 0, 0\n",
    "        for batch, targets in batches :\n",
    "            log_probs = computeLogProbs(batch).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets = targets[1].to(self.device)               # dim = (batch_size, input_length)\n",
    "            s, t = computeSuccess(log_probs, targets)\n",
    "            score += s\n",
    "            total += t\n",
    "        return score * 100 / total\n",
    "    \n",
    "    # fit model\n",
    "    def fit(self, batches, \n",
    "            iters = None, epochs = None, lr = 0.025, masked_ratio = 0,\n",
    "            random_state = 42, print_every = 10, compute_accuracy = True):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loops\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "        \n",
    "        def computeLogProbs(batch) :\n",
    "            embeddings = self.word2vec.embedding(batch[0].to(self.device))\n",
    "            hiddens,_  = self.context(embeddings, lengths = batch[1].to(self.device)) # dim = (batch_size, input_length, hidden_dim)\n",
    "            log_probs  = F.log_softmax(self.out(hiddens), dim = 2)                    # dim = (batch_size, input_length, lang_size)\n",
    "            return log_probs\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            total   = np.sum(targets.data.cpu().numpy() != self.ignore_index_out)\n",
    "            success = sum([self.ignore_index_out != targets[i, j].item() == log_probs[i, :, j].data.topk(1)[1].item() \\\n",
    "                           for i in range(targets.size(0)) \\\n",
    "                           for j in range(targets.size(1)) ])\n",
    "            return  success * 100 / total\n",
    "\n",
    "        def trainLoop(batch, optimizer, compute_accuracy = True):\n",
    "            \"\"\"Performs a training loop, with forward pass, backward pass and weight update.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs  = computeLogProbs(batch[0]).transpose(1, 2) # dim = (batch_size, lang_size, input_length)\n",
    "            targets_xs = batch[1][0].to(self.device)               # dim = (batch_size, input_length)\n",
    "            targets_xl = batch[1][1].to(self.device)               # dim = (batch_size, input_length)\n",
    "            #loss       = (1-unmasked_ratio)*self.criterion(log_probs, targets_xs) \\\n",
    "            #           + unmasked_ratio    *self.criterion(log_probs, targets_xl)\n",
    "            loss = self.criterion(log_probs, targets_xl) * (1-masked_ratio) \\\n",
    "                 + self.criterion(log_probs, targets_xs) * masked_ratio\n",
    " \n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            if compute_accuracy :\n",
    "                accuracy = computeAccuracy(log_probs, targets_xl)\n",
    "            else : \n",
    "                accuracy = 0\n",
    "            error = float(loss.item() / np.sum(targets_xl.data.cpu().numpy() != self.ignore_index_out))\n",
    "            return error, accuracy\n",
    "        \n",
    "        # --- main ---\n",
    "        self.train()\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_acc  = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                batch = random.choice(batches)\n",
    "                loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_acc += acc      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(batches) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(batches)\n",
    "                for batch in batches :\n",
    "                    loss, acc = trainLoop(batch, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_acc += acc \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_acc = printScores(start, iter, iters, tot_loss, tot_acc, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ POSTagger Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306541"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger = POSTagger(device = torch.device('cpu'), # device\n",
    "                            tokenizer = lambda s : s.split(' '),\n",
    "                            word2vec = Word2VecConnector(word2vec),\n",
    "                            hidden_dim = 100, \n",
    "                            n_layers = 2, \n",
    "                            n_class = 41,\n",
    "                            dropout = 0.1,\n",
    "                            optimizer = optim.AdamW)\n",
    "\n",
    "pos_tagger.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batches = pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 42)\n",
    "batches+= pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 4242)\n",
    "batches+= pos_tagger.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 1331)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 10s (- 18m 2s) (50 0%) loss : 2.869  accuracy : 22.3 %\n",
      "0m 18s (- 15m 57s) (100 1%) loss : 1.771  accuracy : 51.3 %\n",
      "0m 26s (- 15m 13s) (150 2%) loss : 1.097  accuracy : 70.6 %\n",
      "0m 34s (- 14m 30s) (200 3%) loss : 0.896  accuracy : 75.5 %\n",
      "0m 42s (- 14m 5s) (250 4%) loss : 0.799  accuracy : 77.9 %\n",
      "0m 49s (- 13m 39s) (300 5%) loss : 0.744  accuracy : 79.1 %\n",
      "0m 58s (- 13m 39s) (350 6%) loss : 0.689  accuracy : 80.3 %\n",
      "1m 5s (- 13m 17s) (400 7%) loss : 0.671  accuracy : 80.6 %\n",
      "1m 12s (- 12m 57s) (450 8%) loss : 0.656  accuracy : 81.0 %\n",
      "1m 18s (- 12m 30s) (500 9%) loss : 0.607  accuracy : 82.0 %\n",
      "1m 26s (- 12m 24s) (550 10%) loss : 0.633  accuracy : 81.3 %\n",
      "1m 34s (- 12m 19s) (600 11%) loss : 0.604  accuracy : 82.2 %\n",
      "1m 42s (- 12m 10s) (650 12%) loss : 0.608  accuracy : 81.8 %\n",
      "1m 49s (- 11m 54s) (700 13%) loss : 0.585  accuracy : 82.8 %\n",
      "1m 57s (- 11m 51s) (750 14%) loss : 0.585  accuracy : 82.8 %\n",
      "2m 6s (- 11m 48s) (800 15%) loss : 0.553  accuracy : 83.5 %\n",
      "2m 13s (- 11m 34s) (850 16%) loss : 0.552  accuracy : 83.7 %\n",
      "2m 20s (- 11m 25s) (900 17%) loss : 0.543  accuracy : 83.5 %\n",
      "2m 27s (- 11m 13s) (950 18%) loss : 0.530  accuracy : 84.2 %\n",
      "2m 35s (- 11m 4s) (1000 18%) loss : 0.543  accuracy : 83.7 %\n",
      "2m 43s (- 10m 58s) (1050 19%) loss : 0.544  accuracy : 83.5 %\n",
      "2m 52s (- 10m 55s) (1100 20%) loss : 0.527  accuracy : 84.0 %\n",
      "3m 1s (- 10m 52s) (1150 21%) loss : 0.524  accuracy : 84.2 %\n",
      "3m 9s (- 10m 43s) (1200 22%) loss : 0.494  accuracy : 85.3 %\n",
      "3m 15s (- 10m 31s) (1250 23%) loss : 0.494  accuracy : 85.2 %\n",
      "3m 23s (- 10m 23s) (1300 24%) loss : 0.489  accuracy : 85.2 %\n",
      "3m 31s (- 10m 14s) (1350 25%) loss : 0.492  accuracy : 85.3 %\n",
      "3m 39s (- 10m 6s) (1400 26%) loss : 0.506  accuracy : 84.7 %\n",
      "3m 45s (- 9m 55s) (1450 27%) loss : 0.484  accuracy : 85.4 %\n",
      "3m 54s (- 9m 49s) (1500 28%) loss : 0.501  accuracy : 85.1 %\n",
      "4m 3s (- 9m 45s) (1550 29%) loss : 0.464  accuracy : 85.8 %\n",
      "4m 12s (- 9m 40s) (1600 30%) loss : 0.483  accuracy : 85.5 %\n",
      "4m 20s (- 9m 32s) (1650 31%) loss : 0.488  accuracy : 85.2 %\n",
      "4m 28s (- 9m 24s) (1700 32%) loss : 0.454  accuracy : 86.4 %\n",
      "4m 36s (- 9m 17s) (1750 33%) loss : 0.474  accuracy : 85.7 %\n",
      "4m 43s (- 9m 6s) (1800 34%) loss : 0.456  accuracy : 86.3 %\n",
      "4m 51s (- 9m 0s) (1850 35%) loss : 0.484  accuracy : 85.4 %\n",
      "5m 0s (- 8m 54s) (1900 36%) loss : 0.446  accuracy : 86.7 %\n",
      "5m 8s (- 8m 45s) (1950 36%) loss : 0.452  accuracy : 86.5 %\n",
      "5m 14s (- 8m 35s) (2000 37%) loss : 0.453  accuracy : 86.2 %\n",
      "5m 22s (- 8m 28s) (2050 38%) loss : 0.445  accuracy : 86.2 %\n",
      "5m 31s (- 8m 21s) (2100 39%) loss : 0.467  accuracy : 85.7 %\n",
      "5m 40s (- 8m 15s) (2150 40%) loss : 0.464  accuracy : 85.9 %\n",
      "5m 49s (- 8m 9s) (2200 41%) loss : 0.456  accuracy : 86.2 %\n",
      "5m 58s (- 8m 2s) (2250 42%) loss : 0.435  accuracy : 86.8 %\n",
      "6m 6s (- 7m 53s) (2300 43%) loss : 0.441  accuracy : 86.6 %\n",
      "6m 13s (- 7m 45s) (2350 44%) loss : 0.445  accuracy : 86.6 %\n",
      "6m 20s (- 7m 35s) (2400 45%) loss : 0.438  accuracy : 87.3 %\n",
      "6m 29s (- 7m 29s) (2450 46%) loss : 0.444  accuracy : 86.5 %\n",
      "6m 39s (- 7m 23s) (2500 47%) loss : 0.443  accuracy : 86.5 %\n",
      "6m 45s (- 7m 13s) (2550 48%) loss : 0.421  accuracy : 87.4 %\n",
      "6m 54s (- 7m 6s) (2600 49%) loss : 0.422  accuracy : 86.9 %\n",
      "7m 1s (- 6m 57s) (2650 50%) loss : 0.449  accuracy : 86.4 %\n",
      "7m 8s (- 6m 48s) (2700 51%) loss : 0.421  accuracy : 87.2 %\n",
      "7m 16s (- 6m 40s) (2750 52%) loss : 0.437  accuracy : 87.0 %\n",
      "7m 25s (- 6m 34s) (2800 53%) loss : 0.427  accuracy : 87.2 %\n",
      "7m 33s (- 6m 26s) (2850 54%) loss : 0.410  accuracy : 87.5 %\n",
      "7m 41s (- 6m 18s) (2900 54%) loss : 0.418  accuracy : 87.3 %\n",
      "7m 48s (- 6m 9s) (2950 55%) loss : 0.409  accuracy : 87.5 %\n",
      "7m 56s (- 6m 1s) (3000 56%) loss : 0.400  accuracy : 88.0 %\n",
      "8m 4s (- 5m 53s) (3050 57%) loss : 0.398  accuracy : 87.8 %\n",
      "8m 12s (- 5m 45s) (3100 58%) loss : 0.396  accuracy : 87.9 %\n",
      "8m 20s (- 5m 37s) (3150 59%) loss : 0.407  accuracy : 88.0 %\n",
      "8m 27s (- 5m 29s) (3200 60%) loss : 0.405  accuracy : 87.6 %\n",
      "8m 36s (- 5m 21s) (3250 61%) loss : 0.409  accuracy : 87.5 %\n",
      "8m 43s (- 5m 13s) (3300 62%) loss : 0.402  accuracy : 87.8 %\n",
      "8m 50s (- 5m 4s) (3350 63%) loss : 0.391  accuracy : 88.0 %\n",
      "8m 56s (- 4m 56s) (3400 64%) loss : 0.386  accuracy : 88.2 %\n",
      "9m 4s (- 4m 48s) (3450 65%) loss : 0.394  accuracy : 87.9 %\n",
      "9m 13s (- 4m 41s) (3500 66%) loss : 0.413  accuracy : 87.2 %\n",
      "9m 21s (- 4m 33s) (3550 67%) loss : 0.395  accuracy : 87.8 %\n",
      "9m 28s (- 4m 24s) (3600 68%) loss : 0.398  accuracy : 87.7 %\n",
      "9m 39s (- 4m 18s) (3650 69%) loss : 0.411  accuracy : 87.7 %\n",
      "9m 46s (- 4m 10s) (3700 70%) loss : 0.392  accuracy : 88.0 %\n",
      "9m 56s (- 4m 2s) (3750 71%) loss : 0.400  accuracy : 88.1 %\n",
      "10m 4s (- 3m 55s) (3800 72%) loss : 0.387  accuracy : 88.3 %\n",
      "10m 12s (- 3m 46s) (3850 72%) loss : 0.396  accuracy : 87.8 %\n",
      "10m 18s (- 3m 38s) (3900 73%) loss : 0.400  accuracy : 87.6 %\n",
      "10m 26s (- 3m 30s) (3950 74%) loss : 0.388  accuracy : 88.0 %\n",
      "10m 36s (- 3m 23s) (4000 75%) loss : 0.410  accuracy : 87.4 %\n",
      "10m 44s (- 3m 15s) (4050 76%) loss : 0.391  accuracy : 87.9 %\n",
      "10m 54s (- 3m 7s) (4100 77%) loss : 0.395  accuracy : 87.9 %\n",
      "11m 2s (- 2m 59s) (4150 78%) loss : 0.377  accuracy : 88.5 %\n",
      "11m 11s (- 2m 52s) (4200 79%) loss : 0.394  accuracy : 87.9 %\n",
      "11m 18s (- 2m 43s) (4250 80%) loss : 0.387  accuracy : 88.1 %\n",
      "11m 26s (- 2m 35s) (4300 81%) loss : 0.386  accuracy : 88.2 %\n",
      "11m 34s (- 2m 28s) (4350 82%) loss : 0.379  accuracy : 88.6 %\n",
      "11m 42s (- 2m 20s) (4400 83%) loss : 0.382  accuracy : 88.2 %\n",
      "11m 50s (- 2m 12s) (4450 84%) loss : 0.388  accuracy : 88.2 %\n",
      "12m 1s (- 2m 4s) (4500 85%) loss : 0.390  accuracy : 88.2 %\n",
      "12m 9s (- 1m 56s) (4550 86%) loss : 0.387  accuracy : 88.0 %\n",
      "12m 18s (- 1m 48s) (4600 87%) loss : 0.384  accuracy : 88.4 %\n",
      "12m 26s (- 1m 40s) (4650 88%) loss : 0.366  accuracy : 88.9 %\n",
      "12m 34s (- 1m 32s) (4700 89%) loss : 0.386  accuracy : 88.1 %\n",
      "12m 43s (- 1m 24s) (4750 90%) loss : 0.375  accuracy : 88.6 %\n",
      "12m 50s (- 1m 16s) (4800 90%) loss : 0.374  accuracy : 88.3 %\n",
      "12m 56s (- 1m 8s) (4850 91%) loss : 0.371  accuracy : 88.6 %\n",
      "13m 2s (- 1m 0s) (4900 92%) loss : 0.349  accuracy : 89.4 %\n",
      "13m 10s (- 0m 52s) (4950 93%) loss : 0.377  accuracy : 88.5 %\n",
      "13m 17s (- 0m 44s) (5000 94%) loss : 0.368  accuracy : 88.9 %\n",
      "13m 23s (- 0m 36s) (5050 95%) loss : 0.365  accuracy : 88.8 %\n",
      "13m 32s (- 0m 28s) (5100 96%) loss : 0.383  accuracy : 88.1 %\n",
      "13m 38s (- 0m 20s) (5150 97%) loss : 0.373  accuracy : 88.6 %\n",
      "13m 45s (- 0m 12s) (5200 98%) loss : 0.384  accuracy : 88.5 %\n",
      "13m 54s (- 0m 4s) (5250 99%) loss : 0.374  accuracy : 88.7 %\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.fit(batches, epochs = 1, lr = 0.001, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 8s (- 14m 36s) (50 0%) loss : 0.362  accuracy : 89.0 %\n",
      "0m 15s (- 13m 39s) (100 1%) loss : 0.351  accuracy : 89.4 %\n",
      "0m 24s (- 13m 51s) (150 2%) loss : 0.347  accuracy : 89.2 %\n",
      "0m 32s (- 13m 56s) (200 3%) loss : 0.363  accuracy : 88.9 %\n",
      "0m 41s (- 13m 44s) (250 4%) loss : 0.348  accuracy : 89.0 %\n",
      "0m 48s (- 13m 18s) (300 5%) loss : 0.354  accuracy : 89.3 %\n",
      "0m 56s (- 13m 9s) (350 6%) loss : 0.372  accuracy : 88.6 %\n",
      "1m 3s (- 12m 59s) (400 7%) loss : 0.354  accuracy : 88.9 %\n",
      "1m 10s (- 12m 31s) (450 8%) loss : 0.360  accuracy : 88.9 %\n",
      "1m 16s (- 12m 12s) (500 9%) loss : 0.348  accuracy : 89.3 %\n",
      "1m 24s (- 12m 7s) (550 10%) loss : 0.354  accuracy : 89.1 %\n",
      "1m 32s (- 11m 58s) (600 11%) loss : 0.342  accuracy : 89.4 %\n",
      "1m 41s (- 12m 2s) (650 12%) loss : 0.342  accuracy : 89.4 %\n",
      "1m 49s (- 11m 54s) (700 13%) loss : 0.368  accuracy : 88.8 %\n",
      "1m 56s (- 11m 41s) (750 14%) loss : 0.346  accuracy : 89.4 %\n",
      "2m 2s (- 11m 28s) (800 15%) loss : 0.354  accuracy : 89.5 %\n",
      "2m 11s (- 11m 26s) (850 16%) loss : 0.343  accuracy : 89.6 %\n",
      "2m 22s (- 11m 30s) (900 17%) loss : 0.353  accuracy : 89.3 %\n",
      "2m 29s (- 11m 21s) (950 18%) loss : 0.337  accuracy : 89.7 %\n",
      "2m 38s (- 11m 18s) (1000 18%) loss : 0.351  accuracy : 89.2 %\n",
      "2m 47s (- 11m 13s) (1050 19%) loss : 0.337  accuracy : 89.6 %\n",
      "2m 54s (- 11m 4s) (1100 20%) loss : 0.343  accuracy : 89.6 %\n",
      "3m 3s (- 10m 59s) (1150 21%) loss : 0.341  accuracy : 89.4 %\n",
      "3m 10s (- 10m 47s) (1200 22%) loss : 0.333  accuracy : 89.9 %\n",
      "3m 19s (- 10m 41s) (1250 23%) loss : 0.360  accuracy : 88.9 %\n",
      "3m 27s (- 10m 34s) (1300 24%) loss : 0.360  accuracy : 89.0 %\n",
      "3m 33s (- 10m 22s) (1350 25%) loss : 0.352  accuracy : 89.2 %\n",
      "3m 42s (- 10m 15s) (1400 26%) loss : 0.361  accuracy : 88.7 %\n",
      "3m 50s (- 10m 8s) (1450 27%) loss : 0.363  accuracy : 88.9 %\n",
      "3m 58s (- 9m 59s) (1500 28%) loss : 0.344  accuracy : 89.6 %\n",
      "4m 7s (- 9m 53s) (1550 29%) loss : 0.364  accuracy : 88.8 %\n",
      "4m 18s (- 9m 53s) (1600 30%) loss : 0.347  accuracy : 89.2 %\n",
      "4m 25s (- 9m 42s) (1650 31%) loss : 0.341  accuracy : 89.4 %\n",
      "4m 32s (- 9m 33s) (1700 32%) loss : 0.356  accuracy : 89.0 %\n",
      "4m 39s (- 9m 22s) (1750 33%) loss : 0.332  accuracy : 89.9 %\n",
      "4m 47s (- 9m 14s) (1800 34%) loss : 0.364  accuracy : 89.0 %\n",
      "4m 55s (- 9m 6s) (1850 35%) loss : 0.343  accuracy : 89.7 %\n",
      "5m 5s (- 9m 2s) (1900 36%) loss : 0.347  accuracy : 89.3 %\n",
      "5m 11s (- 8m 52s) (1950 36%) loss : 0.331  accuracy : 89.9 %\n",
      "5m 19s (- 8m 42s) (2000 37%) loss : 0.350  accuracy : 89.3 %\n",
      "5m 27s (- 8m 35s) (2050 38%) loss : 0.335  accuracy : 89.7 %\n",
      "5m 34s (- 8m 25s) (2100 39%) loss : 0.346  accuracy : 89.2 %\n",
      "5m 43s (- 8m 19s) (2150 40%) loss : 0.339  accuracy : 89.8 %\n",
      "5m 52s (- 8m 12s) (2200 41%) loss : 0.338  accuracy : 89.4 %\n",
      "6m 0s (- 8m 5s) (2250 42%) loss : 0.340  accuracy : 89.5 %\n",
      "6m 6s (- 7m 54s) (2300 43%) loss : 0.332  accuracy : 89.7 %\n",
      "6m 13s (- 7m 45s) (2350 44%) loss : 0.335  accuracy : 89.8 %\n",
      "6m 20s (- 7m 36s) (2400 45%) loss : 0.346  accuracy : 89.7 %\n",
      "6m 29s (- 7m 29s) (2450 46%) loss : 0.350  accuracy : 89.3 %\n",
      "6m 37s (- 7m 21s) (2500 47%) loss : 0.345  accuracy : 89.3 %\n",
      "6m 43s (- 7m 11s) (2550 48%) loss : 0.339  accuracy : 89.7 %\n",
      "6m 52s (- 7m 4s) (2600 49%) loss : 0.347  accuracy : 89.2 %\n",
      "7m 0s (- 6m 56s) (2650 50%) loss : 0.333  accuracy : 89.8 %\n",
      "7m 6s (- 6m 46s) (2700 51%) loss : 0.343  accuracy : 89.5 %\n",
      "7m 13s (- 6m 38s) (2750 52%) loss : 0.346  accuracy : 89.4 %\n",
      "7m 22s (- 6m 31s) (2800 53%) loss : 0.340  accuracy : 89.7 %\n",
      "7m 30s (- 6m 23s) (2850 54%) loss : 0.350  accuracy : 89.5 %\n",
      "7m 39s (- 6m 16s) (2900 54%) loss : 0.329  accuracy : 90.0 %\n",
      "7m 48s (- 6m 9s) (2950 55%) loss : 0.350  accuracy : 89.0 %\n",
      "7m 56s (- 6m 1s) (3000 56%) loss : 0.341  accuracy : 89.4 %\n",
      "8m 4s (- 5m 53s) (3050 57%) loss : 0.344  accuracy : 89.4 %\n",
      "8m 12s (- 5m 45s) (3100 58%) loss : 0.332  accuracy : 89.8 %\n",
      "8m 21s (- 5m 38s) (3150 59%) loss : 0.347  accuracy : 89.4 %\n",
      "8m 28s (- 5m 30s) (3200 60%) loss : 0.331  accuracy : 90.0 %\n",
      "8m 36s (- 5m 22s) (3250 61%) loss : 0.348  accuracy : 89.3 %\n",
      "8m 46s (- 5m 15s) (3300 62%) loss : 0.338  accuracy : 89.4 %\n",
      "8m 53s (- 5m 6s) (3350 63%) loss : 0.333  accuracy : 89.7 %\n",
      "9m 2s (- 4m 59s) (3400 64%) loss : 0.336  accuracy : 89.9 %\n",
      "9m 9s (- 4m 50s) (3450 65%) loss : 0.336  accuracy : 89.8 %\n",
      "9m 16s (- 4m 42s) (3500 66%) loss : 0.339  accuracy : 89.6 %\n",
      "9m 25s (- 4m 34s) (3550 67%) loss : 0.327  accuracy : 89.8 %\n",
      "9m 34s (- 4m 27s) (3600 68%) loss : 0.341  accuracy : 89.7 %\n",
      "9m 42s (- 4m 19s) (3650 69%) loss : 0.333  accuracy : 90.1 %\n",
      "9m 51s (- 4m 11s) (3700 70%) loss : 0.332  accuracy : 89.6 %\n",
      "9m 59s (- 4m 4s) (3750 71%) loss : 0.345  accuracy : 89.4 %\n",
      "10m 7s (- 3m 56s) (3800 72%) loss : 0.334  accuracy : 89.8 %\n",
      "10m 16s (- 3m 48s) (3850 72%) loss : 0.354  accuracy : 89.0 %\n",
      "10m 23s (- 3m 40s) (3900 73%) loss : 0.332  accuracy : 89.6 %\n",
      "10m 31s (- 3m 32s) (3950 74%) loss : 0.338  accuracy : 89.4 %\n",
      "10m 41s (- 3m 24s) (4000 75%) loss : 0.331  accuracy : 89.8 %\n",
      "10m 53s (- 3m 18s) (4050 76%) loss : 0.335  accuracy : 89.5 %\n",
      "11m 1s (- 3m 9s) (4100 77%) loss : 0.329  accuracy : 89.9 %\n",
      "11m 10s (- 3m 2s) (4150 78%) loss : 0.337  accuracy : 89.6 %\n",
      "11m 19s (- 2m 54s) (4200 79%) loss : 0.328  accuracy : 89.8 %\n",
      "11m 27s (- 2m 46s) (4250 80%) loss : 0.335  accuracy : 89.6 %\n",
      "11m 34s (- 2m 37s) (4300 81%) loss : 0.338  accuracy : 89.6 %\n",
      "11m 40s (- 2m 29s) (4350 82%) loss : 0.321  accuracy : 90.2 %\n",
      "11m 47s (- 2m 21s) (4400 83%) loss : 0.342  accuracy : 89.6 %\n",
      "11m 56s (- 2m 13s) (4450 84%) loss : 0.329  accuracy : 89.9 %\n",
      "12m 5s (- 2m 5s) (4500 85%) loss : 0.329  accuracy : 90.1 %\n",
      "12m 13s (- 1m 57s) (4550 86%) loss : 0.333  accuracy : 89.7 %\n",
      "12m 21s (- 1m 49s) (4600 87%) loss : 0.338  accuracy : 89.8 %\n",
      "12m 28s (- 1m 40s) (4650 88%) loss : 0.334  accuracy : 89.7 %\n",
      "12m 35s (- 1m 32s) (4700 89%) loss : 0.327  accuracy : 89.8 %\n",
      "12m 42s (- 1m 24s) (4750 90%) loss : 0.337  accuracy : 89.6 %\n",
      "12m 49s (- 1m 16s) (4800 90%) loss : 0.327  accuracy : 90.0 %\n",
      "12m 57s (- 1m 8s) (4850 91%) loss : 0.343  accuracy : 89.5 %\n",
      "13m 5s (- 1m 0s) (4900 92%) loss : 0.320  accuracy : 90.2 %\n",
      "13m 11s (- 0m 52s) (4950 93%) loss : 0.313  accuracy : 90.1 %\n",
      "13m 17s (- 0m 44s) (5000 94%) loss : 0.327  accuracy : 89.9 %\n",
      "13m 26s (- 0m 36s) (5050 95%) loss : 0.342  accuracy : 89.6 %\n",
      "13m 35s (- 0m 28s) (5100 96%) loss : 0.337  accuracy : 89.5 %\n",
      "13m 45s (- 0m 20s) (5150 97%) loss : 0.327  accuracy : 89.8 %\n",
      "13m 53s (- 0m 12s) (5200 98%) loss : 0.332  accuracy : 89.8 %\n",
      "14m 3s (- 0m 4s) (5250 99%) loss : 0.344  accuracy : 89.4 %\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.fit(batches, epochs = 1, lr = 0.00025, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save\n",
    "#torch.save(pos_tagger.state_dict(), path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_pos_tagger.pth')\n",
    "\n",
    "# load\n",
    "#pos_tagger.load_state_dict(torch.load(path_to_NLP + '\\\\saves\\\\models\\\\DL4NLP_I4a_pos_tagger.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ Sequence Labeller Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306541"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_labeller = SequenceLabeller(device = torch.device('cpu'), # device\n",
    "                                tokenizer = lambda s : s.split(' '),\n",
    "                                word2vec = Word2VecConnector(word2vec),\n",
    "                                hidden_dim = 100, \n",
    "                                n_layers = 2, \n",
    "                                n_class = 41,\n",
    "                                dropout = 0.1,\n",
    "                                optimizer = optim.AdamW)\n",
    "\n",
    "seq_labeller.nbParametres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5277"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = seq_labeller.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 42)\n",
    "batches+= seq_labeller.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 4242)\n",
    "batches+= seq_labeller.generatePackedSentences(corpus_trn, \n",
    "                                             batch_size = 16,\n",
    "                                             mask_ratio = 0.15,\n",
    "                                             seed = 1331)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 10s (- 18m 24s) (50 0%) loss : 2.521  accuracy : 20.8 %\n",
      "0m 18s (- 16m 14s) (100 1%) loss : 1.626  accuracy : 49.4 %\n",
      "0m 26s (- 15m 13s) (150 2%) loss : 1.011  accuracy : 69.6 %\n",
      "0m 34s (- 14m 33s) (200 3%) loss : 0.818  accuracy : 75.1 %\n",
      "0m 42s (- 14m 11s) (250 4%) loss : 0.739  accuracy : 77.4 %\n",
      "0m 50s (- 13m 50s) (300 5%) loss : 0.687  accuracy : 79.0 %\n",
      "0m 58s (- 13m 44s) (350 6%) loss : 0.651  accuracy : 79.8 %\n",
      "1m 5s (- 13m 23s) (400 7%) loss : 0.625  accuracy : 80.6 %\n",
      "1m 13s (- 13m 5s) (450 8%) loss : 0.595  accuracy : 81.0 %\n",
      "1m 19s (- 12m 37s) (500 9%) loss : 0.561  accuracy : 82.3 %\n",
      "1m 27s (- 12m 27s) (550 10%) loss : 0.572  accuracy : 82.0 %\n",
      "1m 35s (- 12m 21s) (600 11%) loss : 0.554  accuracy : 82.3 %\n",
      "1m 43s (- 12m 14s) (650 12%) loss : 0.556  accuracy : 82.1 %\n",
      "1m 50s (- 12m 0s) (700 13%) loss : 0.545  accuracy : 82.7 %\n",
      "1m 58s (- 11m 56s) (750 14%) loss : 0.546  accuracy : 82.6 %\n",
      "2m 7s (- 11m 54s) (800 15%) loss : 0.509  accuracy : 83.3 %\n",
      "2m 14s (- 11m 41s) (850 16%) loss : 0.503  accuracy : 83.6 %\n",
      "2m 22s (- 11m 34s) (900 17%) loss : 0.504  accuracy : 83.6 %\n",
      "2m 29s (- 11m 19s) (950 18%) loss : 0.490  accuracy : 83.9 %\n",
      "2m 36s (- 11m 11s) (1000 18%) loss : 0.506  accuracy : 83.8 %\n",
      "2m 46s (- 11m 8s) (1050 19%) loss : 0.510  accuracy : 83.4 %\n",
      "2m 55s (- 11m 6s) (1100 20%) loss : 0.479  accuracy : 84.0 %\n",
      "3m 4s (- 11m 3s) (1150 21%) loss : 0.476  accuracy : 84.5 %\n",
      "3m 13s (- 10m 56s) (1200 22%) loss : 0.457  accuracy : 85.1 %\n",
      "3m 20s (- 10m 46s) (1250 23%) loss : 0.452  accuracy : 85.2 %\n",
      "3m 29s (- 10m 41s) (1300 24%) loss : 0.449  accuracy : 85.2 %\n",
      "3m 37s (- 10m 33s) (1350 25%) loss : 0.455  accuracy : 85.4 %\n"
     ]
    }
   ],
   "source": [
    "seq_labeller.fit(batches, epochs = 1, lr = 0.001, masked_ratio = 0.15, print_every = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_labeller.fit(batches, epochs = 1, lr = 0.0001, masked_ratio = 0.15, print_every = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\bullet$ POSTagger Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.83161428916931"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.compute_accuracy(corpus_tst, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.18568056648309"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.compute_accuracy(corpus_add, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_labeller.compute_accuracy(corpus_tst, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_labeller.compute_accuracy(corpus_add, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mexico 's andres valencia told reporters late tuesday his talks with national liberation army ( eln ) leader francisco galan focused on ways to reduce differences between the rebels and the government in order to set up a possible meeting between the two sides in mexico . mexico 's andres valencia told reporters late tuesday his talks with national liberation army ( eln ) leader francisco galan focused on ways to reduce differences between the rebels and the government in order to set up a possible meeting between the two sides in mexico .\n",
      "\n",
      "\n",
      "[('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 10), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11), ('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 10), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11)]\n",
      "\n",
      "\n",
      "[('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11), ('mexico', 4), (\"'s\", 18), ('andres', 4), ('valencia', 4), ('told', 12), ('reporters', 0), ('late', 20), ('tuesday', 4), ('his', 23), ('talks', 0), ('with', 1), ('national', 4), ('liberation', 4), ('army', 4), ('(', 35), ('eln', 4), (')', 36), ('leader', 8), ('francisco', 4), ('galan', 4), ('focused', 12), ('on', 1), ('ways', 0), ('to', 5), ('reduce', 6), ('differences', 0), ('between', 1), ('the', 7), ('rebels', 0), ('and', 9), ('the', 7), ('government', 8), ('in', 1), ('order', 8), ('to', 5), ('set', 6), ('up', 30), ('a', 7), ('possible', 10), ('meeting', 8), ('between', 1), ('the', 7), ('two', 15), ('sides', 0), ('in', 1), ('mexico', 4), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.eval()\n",
    "sentence = ' '.join(corpus_tst[11][0]) #'what are you thinking of this'\n",
    "print(sentence)\n",
    "print('\\n')\n",
    "print(pos_tagger(sentence))\n",
    "print('\\n')\n",
    "print([(w, i) for w, i in zip(corpus_tst[11][0], corpus_tst[11][1])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
